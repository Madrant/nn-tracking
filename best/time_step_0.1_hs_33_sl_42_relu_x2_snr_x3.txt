SL: 1 HS: 8 ME: 52.000000 MSE: 0.039846
SL: 2.410550e-03 HS: Saving to file: bayesopt/bayesopt_0.039846.mat
|  335 | Accept |    0.039846 |      7.1034 |    0.032756 |    0.041032 |           43 |            2 |       2.3762 |     0.008144 |            8 |         relu |     0.030673 |           52 |         relu |      0.13499 |           34 |         none |      0.33855 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          48              3             2.6532              0.016605        95     relu    0.30491    40     none    0.13409    26     relu    0.0077411

ht_array length: 1lstm
    95    40    26

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3049    0.1341    0.0077

Type: lstm Hidden size: 95 Activation: relu Dropout: 0.304915
Type: lstm Hidden size: 40 Activation: none Dropout: 0.134087
Type: lstm Hidden size: 26 Activation: relu Dropout: 0.007741
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 95 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 40 hidden units
     6   ''   Dropout             13% dropout
     7   ''   LSTM                LSTM with 26 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             1% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 95 ME: 40.000000 MSE: 26.000000
SL: 5.217273e-02 HS: 3.966500e-03 ME: Saving to file: bayesopt/bayesopt_0.052173.mat
|  336 | Accept |    0.052173 |       13.63 |    0.032756 |    0.038312 |           48 |            3 |       2.6532 |     0.016605 |           95 |         relu |      0.30491 |           40 |         none |      0.13409 |           26 |         relu |    0.0077411 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _________    ___    ____    _______

          25              2             2.9435              0.035162        35     relu    0.28709     5     tanh    0.0098658    22     none    0.19028

ht_array length: 1lstm
    35     5

    "lstm"    "lstm"

    "relu"    "tanh"

    0.2871    0.0099

Type: lstm Hidden size: 35 Activation: relu Dropout: 0.287088
Type: lstm Hidden size: 5 Activation: tanh Dropout: 0.009866
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 35 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             29% dropout
     5   ''   LSTM                LSTM with 5 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             1% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 35 ME: 5.000000 MSE: 0.054394
SL: 4.449837e-03 HS: Saving to file: bayesopt/bayesopt_0.054394.mat
|  337 | Accept |    0.054394 |       8.339 |    0.032756 |    0.040657 |           25 |            2 |       2.9435 |     0.035162 |           35 |         relu |      0.28709 |            5 |         tanh |    0.0098658 |           22 |         none |      0.19028 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          38              3             2.3122             0.0093702        58     relu    0.12978    84     none    0.36956    30     none    0.08294

ht_array length: 1lstm
    58    84    30

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.1298    0.3696    0.0829

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.129782
Type: lstm Hidden size: 84 Activation: none Dropout: 0.369562
Type: lstm Hidden size: 30 Activation: none Dropout: 0.082940
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   Dropout             37% dropout
     7   ''   LSTM                LSTM with 30 hidden units
     8   ''   Dropout             8% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 84.000000 MSE: 30.000000
SL: 4.043542e-02 HS: 2.395453e-03 ME: Saving to file: bayesopt/bayesopt_0.040435.mat
|  338 | Accept |    0.040435 |      13.639 |    0.032756 |    0.037734 |           38 |            3 |       2.3122 |    0.0093702 |           58 |         relu |      0.12978 |           84 |         none |      0.36956 |           30 |         none |      0.08294 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          44              2             2.1983              0.014864        75     relu    0.47729    59     none    0.11736    19     none    0.25879

ht_array length: 1lstm
    75    59

    "lstm"    "lstm"

    "relu"    "none"

    0.4773    0.1174

Type: lstm Hidden size: 75 Activation: relu Dropout: 0.477289
Type: lstm Hidden size: 59 Activation: none Dropout: 0.117362
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 75 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 59 hidden units
     6   ''   Dropout             12% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 75 ME: 59.000000 MSE: 0.050454
SL: 3.581280e-03 HS: Saving to file: bayesopt/bayesopt_0.050454.mat
|  339 | Accept |    0.050454 |      9.8095 |    0.032756 |    0.038018 |           44 |            2 |       2.1983 |     0.014864 |           75 |         relu |      0.47729 |           59 |         none |      0.11736 |           19 |         none |      0.25879 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _________    ___    ____    _______

          6               2             3.6691             0.0010068        23     relu    0.14192    54     none    0.0084272    69     relu    0.16448

ht_array length: 1lstm
    23    54

    "lstm"    "lstm"

    "relu"    "none"

    0.1419    0.0084

Type: lstm Hidden size: 23 Activation: relu Dropout: 0.141915
Type: lstm Hidden size: 54 Activation: none Dropout: 0.008427
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 23 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 54 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 23 ME: 54.000000 MSE: 0.039900
SL: 3.759359e-03 HS: Saving to file: bayesopt/bayesopt_0.0399.mat
|  340 | Accept |      0.0399 |      23.399 |    0.032756 |    0.038323 |            6 |            2 |       3.6691 |    0.0010068 |           23 |         relu |      0.14192 |           54 |         none |    0.0084272 |           69 |         relu |      0.16448 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          46              2             1.3104             0.0011982         1     relu    0.43363    70     none    0.1047    51     relu    0.40279

ht_array length: 1lstm
     1    70

    "lstm"    "lstm"

    "relu"    "none"

    0.4336    0.1047

Type: lstm Hidden size: 1 Activation: relu Dropout: 0.433630
Type: lstm Hidden size: 70 Activation: none Dropout: 0.104705
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 1 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 1 ME: 70.000000 MSE: 0.184268
SL: 4.618669e-02 HS: Saving to file: bayesopt/bayesopt_0.18427.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  341 | Accept |     0.18427 |      6.7213 |    0.032756 |    0.038492 |           46 |            2 |       1.3104 |    0.0011982 |            1 |         relu |      0.43363 |           70 |         none |       0.1047 |           51 |         relu |      0.40279 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          28              2             0.19234            0.0027495        32     relu    0.016549    91     none    0.098375    69     relu    0.10566

ht_array length: 1lstm
    32    91

    "lstm"    "lstm"

    "relu"    "none"

    0.0165    0.0984

Type: lstm Hidden size: 32 Activation: relu Dropout: 0.016549
Type: lstm Hidden size: 91 Activation: none Dropout: 0.098375
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 32 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 32 ME: 91.000000 MSE: 0.044662
SL: 3.030718e-03 HS: Saving to file: bayesopt/bayesopt_0.044662.mat
|  342 | Accept |    0.044662 |      10.561 |    0.032756 |    0.037103 |           28 |            2 |      0.19234 |    0.0027495 |           32 |         relu |     0.016549 |           91 |         none |     0.098375 |           69 |         relu |      0.10566 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          39              3             0.10312             0.021338        35     relu    0.14342    75     none    0.35462    35     relu    0.016541

ht_array length: 1lstm
    35    75    35

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1434    0.3546    0.0165

Type: lstm Hidden size: 35 Activation: relu Dropout: 0.143420
Type: lstm Hidden size: 75 Activation: none Dropout: 0.354623
Type: lstm Hidden size: 35 Activation: relu Dropout: 0.016541
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 35 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 75 hidden units
     6   ''   Dropout             35% dropout
     7   ''   LSTM                LSTM with 35 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             2% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 35 ME: 75.000000 MSE: 35.000000
SL: 6.197848e-02 HS: 5.742759e-03 ME: Saving to file: bayesopt/bayesopt_0.061978.mat
|  343 | Accept |    0.061978 |      12.971 |    0.032756 |    0.036501 |           39 |            3 |      0.10312 |     0.021338 |           35 |         relu |      0.14342 |           75 |         none |      0.35462 |           35 |         relu |     0.016541 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          50              2             1.7021             0.0019276        59     relu    0.081108    42     none    0.068054     7     relu    0.37995

ht_array length: 1lstm
    59    42

    "lstm"    "lstm"

    "relu"    "none"

    0.0811    0.0681

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.081108
Type: lstm Hidden size: 42 Activation: none Dropout: 0.068054
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 42.000000 MSE: 0.053976
SL: 5.053854e-03 HS: Saving to file: bayesopt/bayesopt_0.053976.mat
|  344 | Accept |    0.053976 |      7.3849 |    0.032756 |    0.037549 |           50 |            2 |       1.7021 |    0.0019276 |           59 |         relu |     0.081108 |           42 |         none |     0.068054 |            7 |         relu |      0.37995 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          46              3             4.5192              0.017513         3     relu    0.14127    92     relu    0.43594    11     none    0.056142

ht_array length: 1lstm
     3    92    11

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "none"

    0.1413    0.4359    0.0561

Type: lstm Hidden size: 3 Activation: relu Dropout: 0.141275
Type: lstm Hidden size: 92 Activation: relu Dropout: 0.435942
Type: lstm Hidden size: 11 Activation: none Dropout: 0.056142
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 3 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 92 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             44% dropout
     8   ''   LSTM                LSTM with 11 hidden units
     9   ''   Dropout             6% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 3 ME: 92.000000 MSE: 11.000000
SL: 5.220004e-02 HS: 3.909082e-03 ME: Saving to file: bayesopt/bayesopt_0.0522.mat
|  345 | Accept |      0.0522 |      10.902 |    0.032756 |    0.037793 |           46 |            3 |       4.5192 |     0.017513 |            3 |         relu |      0.14127 |           92 |         relu |      0.43594 |           11 |         none |     0.056142 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2    drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ______    ___    ____    ________

          43              2             1.2499             0.0014574        26     relu    0.043322    24     none    0.1976    27     relu    0.032635

ht_array length: 1lstm
    26    24

    "lstm"    "lstm"

    "relu"    "none"

    0.0433    0.1976

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.043322
Type: lstm Hidden size: 24 Activation: none Dropout: 0.197603
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 24 hidden units
     6   ''   Dropout             20% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 24.000000 MSE: 0.054669
SL: 4.213097e-03 HS: Saving to file: bayesopt/bayesopt_0.054669.mat
|  346 | Accept |    0.054669 |      6.2078 |    0.032756 |    0.038126 |           43 |            2 |       1.2499 |    0.0014574 |           26 |         relu |     0.043322 |           24 |         none |       0.1976 |           27 |         relu |     0.032635 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    ________

          34              2             4.4308             0.0017784        43     relu    0.37342    94     none    0.2116    36     none    0.094397

ht_array length: 1lstm
    43    94

    "lstm"    "lstm"

    "relu"    "none"

    0.3734    0.2116

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.373419
Type: lstm Hidden size: 94 Activation: none Dropout: 0.211598
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   Dropout             21% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 94.000000 MSE: 0.057701
SL: 5.333684e-03 HS: Saving to file: bayesopt/bayesopt_0.057701.mat
|  347 | Accept |    0.057701 |      11.086 |    0.032756 |    0.038836 |           34 |            2 |       4.4308 |    0.0017784 |           43 |         relu |      0.37342 |           94 |         none |       0.2116 |           36 |         none |     0.094397 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          34              2             0.32492            0.0010443        89     relu    0.39774    44     tanh    0.27719    21     none    0.17969

ht_array length: 1lstm
    89    44

    "lstm"    "lstm"

    "relu"    "tanh"

    0.3977    0.2772

Type: lstm Hidden size: 89 Activation: relu Dropout: 0.397736
Type: lstm Hidden size: 44 Activation: tanh Dropout: 0.277189
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 89 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 44 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             28% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 89 ME: 44.000000 MSE: 0.050706
SL: 3.913085e-03 HS: Saving to file: bayesopt/bayesopt_0.050706.mat
|  348 | Accept |    0.050706 |       11.47 |    0.032756 |    0.038493 |           34 |            2 |      0.32492 |    0.0010443 |           89 |         relu |      0.39774 |           44 |         tanh |      0.27719 |           21 |         none |      0.17969 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          35              3             4.6326              0.14213         88     relu    0.30614    37     none    0.003137    71     relu    0.22114

ht_array length: 1lstm
    88    37    71

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3061    0.0031    0.2211

Type: lstm Hidden size: 88 Activation: relu Dropout: 0.306140
Type: lstm Hidden size: 37 Activation: none Dropout: 0.003137
Type: lstm Hidden size: 71 Activation: relu Dropout: 0.221143
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 88 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 37 hidden units
     6   ''   Dropout             0% dropout
     7   ''   LSTM                LSTM with 71 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             22% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 88 ME: 37.000000 MSE: 71.000000
SL: 1.827734e-01 HS: 4.535324e-02 ME: Saving to file: bayesopt/bayesopt_0.18277.mat
|  349 | Accept |     0.18277 |      14.478 |    0.032756 |    0.038229 |           35 |            3 |       4.6326 |      0.14213 |           88 |         relu |      0.30614 |           37 |         none |     0.003137 |           71 |         relu |      0.22114 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          29              2             2.2148             0.0020899        21     relu    0.24978    91     none    0.070751    97     relu    0.25401

ht_array length: 1lstm
    21    91

    "lstm"    "lstm"

    "relu"    "none"

    0.2498    0.0708

Type: lstm Hidden size: 21 Activation: relu Dropout: 0.249780
Type: lstm Hidden size: 91 Activation: none Dropout: 0.070751
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 21 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 21 ME: 91.000000 MSE: 0.054113
SL: 4.513169e-03 HS: Saving to file: bayesopt/bayesopt_0.054113.mat
|  350 | Accept |    0.054113 |      10.254 |    0.032756 |    0.036848 |           29 |            2 |       2.2148 |    0.0020899 |           21 |         relu |      0.24978 |           91 |         none |     0.070751 |           97 |         relu |      0.25401 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          22              2             4.0094             0.0013662        46     relu    0.16969    53     none    0.21946    98     relu    0.35107

ht_array length: 1lstm
    46    53

    "lstm"    "lstm"

    "relu"    "none"

    0.1697    0.2195

Type: lstm Hidden size: 46 Activation: relu Dropout: 0.169687
Type: lstm Hidden size: 53 Activation: none Dropout: 0.219463
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 46 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 53 hidden units
     6   ''   Dropout             22% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 46 ME: 53.000000 MSE: 0.049644
SL: 3.936917e-03 HS: Saving to file: bayesopt/bayesopt_0.049644.mat
|  351 | Accept |    0.049644 |      10.887 |    0.032756 |    0.037865 |           22 |            2 |       4.0094 |    0.0013662 |           46 |         relu |      0.16969 |           53 |         none |      0.21946 |           98 |         relu |      0.35107 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________

          47              2             0.45069            0.0037324        69     relu    0.052895    86     none    0.24057    50     tanh    0.043876

ht_array length: 1lstm
    69    86

    "lstm"    "lstm"

    "relu"    "none"

    0.0529    0.2406

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.052895
Type: lstm Hidden size: 86 Activation: none Dropout: 0.240569
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   Dropout             24% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 86.000000 MSE: 0.058122
SL: 5.227629e-03 HS: Saving to file: bayesopt/bayesopt_0.058122.mat
|  352 | Accept |    0.058122 |      11.528 |    0.032756 |    0.037928 |           47 |            2 |      0.45069 |    0.0037324 |           69 |         relu |     0.052895 |           86 |         none |      0.24057 |           50 |         tanh |     0.043876 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _____

          40              2             4.6277              0.013983        25     relu    0.040001    57     none    0.016206    76     relu    0.066

ht_array length: 1lstm
    25    57

    "lstm"    "lstm"

    "relu"    "none"

    0.0400    0.0162

Type: lstm Hidden size: 25 Activation: relu Dropout: 0.040001
Type: lstm Hidden size: 57 Activation: none Dropout: 0.016206
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 25 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 25 ME: 57.000000 MSE: 0.073820
SL: 7.316206e-03 HS: Saving to file: bayesopt/bayesopt_0.07382.mat
|  353 | Accept |     0.07382 |      8.4009 |    0.032756 |    0.037897 |           40 |            2 |       4.6277 |     0.013983 |           25 |         relu |     0.040001 |           57 |         none |     0.016206 |           76 |         relu |        0.066 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          35              2             0.44485             0.010514        34     relu    0.49522    33     none    0.02276    39     none    0.07137

ht_array length: 1lstm
    34    33

    "lstm"    "lstm"

    "relu"    "none"

    0.4952    0.0228

Type: lstm Hidden size: 34 Activation: relu Dropout: 0.495215
Type: lstm Hidden size: 33 Activation: none Dropout: 0.022760
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 34 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             50% dropout
     5   ''   LSTM                LSTM with 33 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 34 ME: 33.000000 MSE: 0.048265
SL: 3.318670e-03 HS: Saving to file: bayesopt/bayesopt_0.048265.mat
|  354 | Accept |    0.048265 |      7.0335 |    0.032756 |    0.040666 |           35 |            2 |      0.44485 |     0.010514 |           34 |         relu |      0.49522 |           33 |         none |      0.02276 |           39 |         none |      0.07137 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          39              3             1.7107             0.0082183        23     relu    0.10527    91     none    0.13881    72     relu    0.12265

ht_array length: 1lstm
    23    91    72

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1053    0.1388    0.1227

Type: lstm Hidden size: 23 Activation: relu Dropout: 0.105273
Type: lstm Hidden size: 91 Activation: none Dropout: 0.138808
Type: lstm Hidden size: 72 Activation: relu Dropout: 0.122650
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 23 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   Dropout             14% dropout
     7   ''   LSTM                LSTM with 72 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             12% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 23 ME: 91.000000 MSE: 72.000000
SL: 4.533146e-02 HS: 2.808067e-03 ME: Saving to file: bayesopt/bayesopt_0.045331.mat
|  355 | Accept |    0.045331 |      15.263 |    0.032756 |    0.038348 |           39 |            3 |       1.7107 |    0.0082183 |           23 |         relu |      0.10527 |           91 |         none |      0.13881 |           72 |         relu |      0.12265 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          43              2             0.66178            0.0028154        22     relu    0.10384    31     none    0.0123    20     relu    0.33737

ht_array length: 1lstm
    22    31

    "lstm"    "lstm"

    "relu"    "none"

    0.1038    0.0123

Type: lstm Hidden size: 22 Activation: relu Dropout: 0.103839
Type: lstm Hidden size: 31 Activation: none Dropout: 0.012300
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 22 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 22 ME: 31.000000 MSE: 0.052570
SL: 4.947788e-03 HS: Saving to file: bayesopt/bayesopt_0.05257.mat
|  356 | Accept |     0.05257 |      6.5434 |    0.032756 |    0.038977 |           43 |            2 |      0.66178 |    0.0028154 |           22 |         relu |      0.10384 |           31 |         none |       0.0123 |           20 |         relu |      0.33737 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ________

          44              3             1.3485             0.0039057        80     relu    0.076331    16     tanh    0.046265    33     none    0.071242

ht_array length: 1lstm
    80    16    33

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.0763    0.0463    0.0712

Type: lstm Hidden size: 80 Activation: relu Dropout: 0.076331
Type: lstm Hidden size: 16 Activation: tanh Dropout: 0.046265
Type: lstm Hidden size: 33 Activation: none Dropout: 0.071242
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 80 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 16 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             5% dropout
     8   ''   LSTM                LSTM with 33 hidden units
     9   ''   Dropout             7% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 80 ME: 16.000000 MSE: 33.000000
SL: 4.945984e-02 HS: 3.730671e-03 ME: Saving to file: bayesopt/bayesopt_0.04946.mat
|  357 | Accept |     0.04946 |      11.494 |    0.032756 |    0.037812 |           44 |            3 |       1.3485 |    0.0039057 |           80 |         relu |     0.076331 |           16 |         tanh |     0.046265 |           33 |         none |     0.071242 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1       hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    __________    ___    ____    ________    ___    ____    _______

          36              2             2.9564             0.0012952        35     relu    0.00059417     7     none    0.037201    36     relu    0.21124

ht_array length: 1lstm
    35     7

    "lstm"    "lstm"

    "relu"    "none"

    0.0006    0.0372

Type: lstm Hidden size: 35 Activation: relu Dropout: 0.000594
Type: lstm Hidden size: 7 Activation: none Dropout: 0.037201
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 35 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             0% dropout
     5   ''   LSTM                LSTM with 7 hidden units
     6   ''   Dropout             4% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 35 ME: 7.000000 MSE: 0.047399
SL: 3.815994e-03 HS: Saving to file: bayesopt/bayesopt_0.047399.mat
|  358 | Accept |    0.047399 |      6.5397 |    0.032756 |    0.038163 |           36 |            2 |       2.9564 |    0.0012952 |           35 |         relu |   0.00059417 |            7 |         none |     0.037201 |           36 |         relu |      0.21124 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          38              3             0.19934            0.0027691        99     relu    0.40726    12     tanh    0.13945     8     none    0.12916

ht_array length: 1lstm
    99    12     8

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.4073    0.1394    0.1292

Type: lstm Hidden size: 99 Activation: relu Dropout: 0.407258
Type: lstm Hidden size: 12 Activation: tanh Dropout: 0.139446
Type: lstm Hidden size: 8 Activation: none Dropout: 0.129159
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 99 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             14% dropout
     8   ''   LSTM                LSTM with 8 hidden units
     9   ''   Dropout             13% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 99 ME: 12.000000 MSE: 8.000000
SL: 8.392790e-02 HS: 9.749199e-03 ME: Saving to file: bayesopt/bayesopt_0.083928.mat
|  359 | Accept |    0.083928 |      12.161 |    0.032756 |    0.037406 |           38 |            3 |      0.19934 |    0.0027691 |           99 |         relu |      0.40726 |           12 |         tanh |      0.13945 |            8 |         none |      0.12916 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          36              2             0.11346             0.028821        20     relu    0.10712    15     none    0.045305    86     tanh    0.36728

ht_array length: 1lstm
    20    15

    "lstm"    "lstm"

    "relu"    "none"

    0.1071    0.0453

Type: lstm Hidden size: 20 Activation: relu Dropout: 0.107121
Type: lstm Hidden size: 15 Activation: none Dropout: 0.045305
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 20 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 15 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 20 ME: 15.000000 MSE: 0.056007
SL: 4.691137e-03 HS: Saving to file: bayesopt/bayesopt_0.056007.mat
|  360 | Accept |    0.056007 |       6.262 |    0.032756 |    0.037562 |           36 |            2 |      0.11346 |     0.028821 |           20 |         relu |      0.10712 |           15 |         none |     0.045305 |           86 |         tanh |      0.36728 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ______

          48              3             4.1574              0.010203        79     relu    0.23599     9     none    0.023908    52     relu    0.1767

ht_array length: 1lstm
    79     9    52

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2360    0.0239    0.1767

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.235986
Type: lstm Hidden size: 9 Activation: none Dropout: 0.023908
Type: lstm Hidden size: 52 Activation: relu Dropout: 0.176702
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             24% dropout
     5   ''   LSTM                LSTM with 9 hidden units
     6   ''   Dropout             2% dropout
     7   ''   LSTM                LSTM with 52 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             18% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 9.000000 MSE: 52.000000
SL: 4.201544e-02 HS: 2.691928e-03 ME: Saving to file: bayesopt/bayesopt_0.042015.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  361 | Accept |    0.042015 |      11.726 |    0.032756 |    0.038627 |           48 |            3 |       4.1574 |     0.010203 |           79 |         relu |      0.23599 |            9 |         none |     0.023908 |           52 |         relu |       0.1767 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _____

          48              3             0.83181             0.014815        45     relu    0.29626    94     none    0.079518     2     relu    0.217

ht_array length: 1lstm
    45    94     2

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2963    0.0795    0.2170

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.296259
Type: lstm Hidden size: 94 Activation: none Dropout: 0.079518
Type: lstm Hidden size: 2 Activation: relu Dropout: 0.217005
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   Dropout             8% dropout
     7   ''   LSTM                LSTM with 2 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             22% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 94.000000 MSE: 2.000000
SL: 1.840106e-01 HS: 4.576775e-02 ME: Saving to file: bayesopt/bayesopt_0.18401.mat
|  362 | Accept |     0.18401 |      13.063 |    0.032756 |    0.032814 |           48 |            3 |      0.83181 |     0.014815 |           45 |         relu |      0.29626 |           94 |         none |     0.079518 |            2 |         relu |        0.217 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          24              3             1.9876              0.022932        40     relu    0.21759    25     none    0.029398    80     relu    0.21711

ht_array length: 1lstm
    40    25    80

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2176    0.0294    0.2171

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.217592
Type: lstm Hidden size: 25 Activation: none Dropout: 0.029398
Type: lstm Hidden size: 80 Activation: relu Dropout: 0.217108
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             22% dropout
     5   ''   LSTM                LSTM with 25 hidden units
     6   ''   Dropout             3% dropout
     7   ''   LSTM                LSTM with 80 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             22% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 25.000000 MSE: 80.000000
SL: 5.566969e-02 HS: 4.308357e-03 ME: Saving to file: bayesopt/bayesopt_0.05567.mat
|  363 | Accept |     0.05567 |      14.168 |    0.032756 |     0.03492 |           24 |            3 |       1.9876 |     0.022932 |           40 |         relu |      0.21759 |           25 |         none |     0.029398 |           80 |         relu |      0.21711 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          46              2             4.9237              0.019312        80     relu    0.052076    76     none    0.47937     9     none    0.47233

ht_array length: 1lstm
    80    76

    "lstm"    "lstm"

    "relu"    "none"

    0.0521    0.4794

Type: lstm Hidden size: 80 Activation: relu Dropout: 0.052076
Type: lstm Hidden size: 76 Activation: none Dropout: 0.479371
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 80 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 76 hidden units
     6   ''   Dropout             48% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 80 ME: 76.000000 MSE: 0.041331
SL: 2.446349e-03 HS: Saving to file: bayesopt/bayesopt_0.041331.mat
|  364 | Accept |    0.041331 |      11.511 |    0.032756 |    0.039857 |           46 |            2 |       4.9237 |     0.019312 |           80 |         relu |     0.052076 |           76 |         none |      0.47937 |            9 |         none |      0.47233 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          16              2             0.67352             0.001807        24     relu    0.47373    93     none    0.051094    12     relu    0.058955

ht_array length: 1lstm
    24    93

    "lstm"    "lstm"

    "relu"    "none"

    0.4737    0.0511

Type: lstm Hidden size: 24 Activation: relu Dropout: 0.473735
Type: lstm Hidden size: 93 Activation: none Dropout: 0.051094
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 24 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 93 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 24 ME: 93.000000 MSE: 0.040849
SL: 2.554365e-03 HS: Saving to file: bayesopt/bayesopt_0.040849.mat
|  365 | Accept |    0.040849 |      13.782 |    0.032756 |    0.039282 |           16 |            2 |      0.67352 |     0.001807 |           24 |         relu |      0.47373 |           93 |         none |     0.051094 |           12 |         relu |     0.058955 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          35              3             1.0286             0.0014572        51     relu    0.10848    88     none    0.42465    16     none    0.25921

ht_array length: 1lstm
    51    88    16

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.1085    0.4246    0.2592

Type: lstm Hidden size: 51 Activation: relu Dropout: 0.108479
Type: lstm Hidden size: 88 Activation: none Dropout: 0.424647
Type: lstm Hidden size: 16 Activation: none Dropout: 0.259209
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 51 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 88 hidden units
     6   ''   Dropout             42% dropout
     7   ''   LSTM                LSTM with 16 hidden units
     8   ''   Dropout             26% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 51 ME: 88.000000 MSE: 16.000000
SL: 9.067748e-02 HS: 1.128354e-02 ME: Saving to file: bayesopt/bayesopt_0.090677.mat
|  366 | Accept |    0.090677 |      12.686 |    0.032756 |    0.037724 |           35 |            3 |       1.0286 |    0.0014572 |           51 |         relu |      0.10848 |           88 |         none |      0.42465 |           16 |         none |      0.25921 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          28              2             0.47047            0.0013255        99     relu    0.18354    91     none    0.18089    70     relu    0.30457

ht_array length: 1lstm
    99    91

    "lstm"    "lstm"

    "relu"    "none"

    0.1835    0.1809

Type: lstm Hidden size: 99 Activation: relu Dropout: 0.183540
Type: lstm Hidden size: 91 Activation: none Dropout: 0.180887
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 99 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   Dropout             18% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 99 ME: 91.000000 MSE: 0.044718
SL: 2.847199e-03 HS: Saving to file: bayesopt/bayesopt_0.044718.mat
|  367 | Accept |    0.044718 |      14.419 |    0.032756 |     0.03798 |           28 |            2 |      0.47047 |    0.0013255 |           99 |         relu |      0.18354 |           91 |         none |      0.18089 |           70 |         relu |      0.30457 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          36              2             1.8843             0.0055218        83     relu    0.30853    94     none    0.44536    13     relu    0.030579

ht_array length: 1lstm
    83    94

    "lstm"    "lstm"

    "relu"    "none"

    0.3085    0.4454

Type: lstm Hidden size: 83 Activation: relu Dropout: 0.308535
Type: lstm Hidden size: 94 Activation: none Dropout: 0.445361
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 83 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   Dropout             45% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 83 ME: 94.000000 MSE: 0.047694
SL: 3.385971e-03 HS: Saving to file: bayesopt/bayesopt_0.047694.mat
|  368 | Accept |    0.047694 |      12.448 |    0.032756 |    0.032787 |           36 |            2 |       1.8843 |    0.0055218 |           83 |         relu |      0.30853 |           94 |         none |      0.44536 |           13 |         relu |     0.030579 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          15              2             1.4168              0.14176         39     relu    0.33121    19     none    0.15745    99     relu    0.49423

ht_array length: 1lstm
    39    19

    "lstm"    "lstm"

    "relu"    "none"

    0.3312    0.1574

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.331207
Type: lstm Hidden size: 19 Activation: none Dropout: 0.157450
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 19.000000 MSE: 0.186190
SL: 4.839511e-02 HS: Saving to file: bayesopt/bayesopt_0.18619.mat
|  369 | Accept |     0.18619 |      12.241 |    0.032756 |    0.037035 |           15 |            2 |       1.4168 |      0.14176 |           39 |         relu |      0.33121 |           19 |         none |      0.15745 |           99 |         relu |      0.49423 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    ________

          39              2             2.8493             0.0014838        65     relu    0.18476    88     none    0.3224    28     relu    0.012336

ht_array length: 1lstm
    65    88

    "lstm"    "lstm"

    "relu"    "none"

    0.1848    0.3224

Type: lstm Hidden size: 65 Activation: relu Dropout: 0.184763
Type: lstm Hidden size: 88 Activation: none Dropout: 0.322397
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 65 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 88 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 65 ME: 88.000000 MSE: 0.038690
SL: 2.725178e-03 HS: Saving to file: bayesopt/bayesopt_0.03869.mat
|  370 | Accept |     0.03869 |      12.077 |    0.032756 |    0.038007 |           39 |            2 |       2.8493 |    0.0014838 |           65 |         relu |      0.18476 |           88 |         none |       0.3224 |           28 |         relu |     0.012336 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          30              2             4.2299             0.0010101        90     relu    0.39141    81     none    0.34821    92     relu    0.4227

ht_array length: 1lstm
    90    81

    "lstm"    "lstm"

    "relu"    "none"

    0.3914    0.3482

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.391411
Type: lstm Hidden size: 81 Activation: none Dropout: 0.348208
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   Dropout             35% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 81.000000 MSE: 0.052000
SL: 4.797977e-03 HS: Saving to file: bayesopt/bayesopt_0.052.mat
|  371 | Accept |       0.052 |      14.701 |    0.032756 |    0.038591 |           30 |            2 |       4.2299 |    0.0010101 |           90 |         relu |      0.39141 |           81 |         none |      0.34821 |           92 |         relu |       0.4227 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2    drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ______    ___    ____    ______

          44              2             0.31427            0.0027403        76     relu    0.018454    49     none    0.4084    75     relu    0.1168

ht_array length: 1lstm
    76    49

    "lstm"    "lstm"

    "relu"    "none"

    0.0185    0.4084

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.018454
Type: lstm Hidden size: 49 Activation: none Dropout: 0.408397
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   Dropout             41% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 49.000000 MSE: 0.052823
SL: 3.989776e-03 HS: Saving to file: bayesopt/bayesopt_0.052823.mat
|  372 | Accept |    0.052823 |       10.04 |    0.032756 |    0.034957 |           44 |            2 |      0.31427 |    0.0027403 |           76 |         relu |     0.018454 |           49 |         none |       0.4084 |           75 |         relu |       0.1168 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          11              2             0.10016             0.002698        76     relu    0.21844    37     none    0.034722    10     relu    0.25775

ht_array length: 1lstm
    76    37

    "lstm"    "lstm"

    "relu"    "none"

    0.2184    0.0347

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.218437
Type: lstm Hidden size: 37 Activation: none Dropout: 0.034722
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             22% dropout
     5   ''   LSTM                LSTM with 37 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 37.000000 MSE: 0.067640
SL: 6.960398e-03 HS: Saving to file: bayesopt/bayesopt_0.06764.mat
|  373 | Accept |     0.06764 |      18.163 |    0.032756 |    0.039427 |           11 |            2 |      0.10016 |     0.002698 |           76 |         relu |      0.21844 |           37 |         none |     0.034722 |           10 |         relu |      0.25775 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          32              3             0.64892            0.0062271        32     relu    0.20833    57     none    0.1078    38     relu    0.12025

ht_array length: 1lstm
    32    57    38

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2083    0.1078    0.1203

Type: lstm Hidden size: 32 Activation: relu Dropout: 0.208334
Type: lstm Hidden size: 57 Activation: none Dropout: 0.107798
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.120251
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 32 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             11% dropout
     7   ''   LSTM                LSTM with 38 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             12% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 32 ME: 57.000000 MSE: 38.000000
SL: 5.574481e-02 HS: 4.720680e-03 ME: Saving to file: bayesopt/bayesopt_0.055745.mat
|  374 | Accept |    0.055745 |      12.559 |    0.032756 |    0.039327 |           32 |            3 |      0.64892 |    0.0062271 |           32 |         relu |      0.20833 |           57 |         none |       0.1078 |           38 |         relu |      0.12025 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _________

          30              3             0.4025              0.015701        78     relu    0.035246    22     tanh    0.021909    79     none    0.0030748

ht_array length: 1lstm
    78    22    79

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.0352    0.0219    0.0031

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.035246
Type: lstm Hidden size: 22 Activation: tanh Dropout: 0.021909
Type: lstm Hidden size: 79 Activation: none Dropout: 0.003075
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 22 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             2% dropout
     8   ''   LSTM                LSTM with 79 hidden units
     9   ''   Dropout             0% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 22.000000 MSE: 79.000000
SL: 4.808606e-02 HS: 3.374617e-03 ME: Saving to file: bayesopt/bayesopt_0.048086.mat
|  375 | Accept |    0.048086 |      16.673 |    0.032756 |    0.038325 |           30 |            3 |       0.4025 |     0.015701 |           78 |         relu |     0.035246 |           22 |         tanh |     0.021909 |           79 |         none |    0.0030748 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ______

          25              2             0.17121            0.0018992        25     relu    0.085179    64     none    0.069968    62     relu    0.1422

ht_array length: 1lstm
    25    64

    "lstm"    "lstm"

    "relu"    "none"

    0.0852    0.0700

Type: lstm Hidden size: 25 Activation: relu Dropout: 0.085179
Type: lstm Hidden size: 64 Activation: none Dropout: 0.069968
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 25 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 25 ME: 64.000000 MSE: 0.043848
SL: 3.238864e-03 HS: Saving to file: bayesopt/bayesopt_0.043848.mat
|  376 | Accept |    0.043848 |      9.8323 |    0.032756 |    0.037087 |           25 |            2 |      0.17121 |    0.0018992 |           25 |         relu |     0.085179 |           64 |         none |     0.069968 |           62 |         relu |       0.1422 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          26              3             4.9661              0.006546        66     relu    0.37392    98     none    0.15827    38     none    0.35029

ht_array length: 1lstm
    66    98    38

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.3739    0.1583    0.3503

Type: lstm Hidden size: 66 Activation: relu Dropout: 0.373918
Type: lstm Hidden size: 98 Activation: none Dropout: 0.158267
Type: lstm Hidden size: 38 Activation: none Dropout: 0.350292
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 66 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   Dropout             16% dropout
     7   ''   LSTM                LSTM with 38 hidden units
     8   ''   Dropout             35% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 66 ME: 98.000000 MSE: 38.000000
SL: 6.503040e-02 HS: 6.061988e-03 ME: Saving to file: bayesopt/bayesopt_0.06503.mat
|  377 | Accept |     0.06503 |      18.628 |    0.032756 |    0.038495 |           26 |            3 |       4.9661 |     0.006546 |           66 |         relu |      0.37392 |           98 |         none |      0.15827 |           38 |         none |      0.35029 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          28              2             2.6411             0.0050387        63     relu    0.39113    41     none    0.42092    95     relu    0.45523

ht_array length: 1lstm
    63    41

    "lstm"    "lstm"

    "relu"    "none"

    0.3911    0.4209

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.391129
Type: lstm Hidden size: 41 Activation: none Dropout: 0.420917
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 41 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 41.000000 MSE: 0.041726
SL: 2.449517e-03 HS: Saving to file: bayesopt/bayesopt_0.041726.mat
|  378 | Accept |    0.041726 |       9.756 |    0.032756 |    0.032781 |           28 |            2 |       2.6411 |    0.0050387 |           63 |         relu |      0.39113 |           41 |         none |      0.42092 |           95 |         relu |      0.45523 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          45              3             4.5356              0.014403        47     relu    0.22689    48     none    0.17924    50     relu    0.13565

ht_array length: 1lstm
    47    48    50

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2269    0.1792    0.1357

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.226885
Type: lstm Hidden size: 48 Activation: none Dropout: 0.179241
Type: lstm Hidden size: 50 Activation: relu Dropout: 0.135653
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 48 hidden units
     6   ''   Dropout             18% dropout
     7   ''   LSTM                LSTM with 50 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             14% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 48.000000 MSE: 50.000000
SL: 5.833417e-02 HS: 5.046097e-03 ME: Saving to file: bayesopt/bayesopt_0.058334.mat
|  379 | Accept |    0.058334 |      12.372 |    0.032756 |    0.038772 |           45 |            3 |       4.5356 |     0.014403 |           47 |         relu |      0.22689 |           48 |         none |      0.17924 |           50 |         relu |      0.13565 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          47              3             2.8101              0.010072        70     relu    0.068987    57     none    0.39685    13     none    0.19025

ht_array length: 1lstm
    70    57    13

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.0690    0.3968    0.1902

Type: lstm Hidden size: 70 Activation: relu Dropout: 0.068987
Type: lstm Hidden size: 57 Activation: none Dropout: 0.396846
Type: lstm Hidden size: 13 Activation: none Dropout: 0.190248
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 70 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             40% dropout
     7   ''   LSTM                LSTM with 13 hidden units
     8   ''   Dropout             19% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 70 ME: 57.000000 MSE: 13.000000
SL: 4.113934e-02 HS: 2.471132e-03 ME: Saving to file: bayesopt/bayesopt_0.041139.mat
|  380 | Accept |    0.041139 |      12.534 |    0.032756 |    0.036009 |           47 |            3 |       2.8101 |     0.010072 |           70 |         relu |     0.068987 |           57 |         none |      0.39685 |           13 |         none |      0.19025 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          30              2             0.64884            0.0037907        93     relu    0.14291    32     tanh    0.010868    47     none    0.44694

ht_array length: 1lstm
    93    32

    "lstm"    "lstm"

    "relu"    "tanh"

    0.1429    0.0109

Type: lstm Hidden size: 93 Activation: relu Dropout: 0.142907
Type: lstm Hidden size: 32 Activation: tanh Dropout: 0.010868
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 93 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 32 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             1% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 93 ME: 32.000000 MSE: 0.047080
SL: 3.191948e-03 HS: Saving to file: bayesopt/bayesopt_0.04708.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  381 | Accept |     0.04708 |      12.978 |    0.032756 |    0.039223 |           30 |            2 |      0.64884 |    0.0037907 |           93 |         relu |      0.14291 |           32 |         tanh |     0.010868 |           47 |         none |      0.44694 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          47              2             4.9615              0.09456         96     relu    0.18228    63     none    0.31173    58     none    0.34052

ht_array length: 1lstm
    96    63

    "lstm"    "lstm"

    "relu"    "none"

    0.1823    0.3117

Type: lstm Hidden size: 96 Activation: relu Dropout: 0.182284
Type: lstm Hidden size: 63 Activation: none Dropout: 0.311730
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 96 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 63 hidden units
     6   ''   Dropout             31% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 63.000000 MSE: 0.191482
SL: 5.078680e-02 HS: Saving to file: bayesopt/bayesopt_0.19148.mat
|  382 | Accept |     0.19148 |      13.981 |    0.032756 |    0.032809 |           47 |            2 |       4.9615 |      0.09456 |           96 |         relu |      0.18228 |           63 |         none |      0.31173 |           58 |         none |      0.34052 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          49              2             2.0631              0.015516        69     relu    0.15498    70     none    0.48679    60     relu    0.15414

ht_array length: 1lstm
    69    70

    "lstm"    "lstm"

    "relu"    "none"

    0.1550    0.4868

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.154981
Type: lstm Hidden size: 70 Activation: none Dropout: 0.486789
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             49% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 70.000000 MSE: 0.049187
SL: 3.594327e-03 HS: Saving to file: bayesopt/bayesopt_0.049187.mat
|  383 | Accept |    0.049187 |      9.5814 |    0.032756 |    0.038217 |           49 |            2 |       2.0631 |     0.015516 |           69 |         relu |      0.15498 |           70 |         none |      0.48679 |           60 |         relu |      0.15414 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          40              3             0.10829            0.0059468        77     relu    0.32462    66     tanh    0.043662     6     none    0.47751

ht_array length: 1lstm
    77    66     6

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.3246    0.0437    0.4775

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.324625
Type: lstm Hidden size: 66 Activation: tanh Dropout: 0.043662
Type: lstm Hidden size: 6 Activation: none Dropout: 0.477510
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 66 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             4% dropout
     8   ''   LSTM                LSTM with 6 hidden units
     9   ''   Dropout             48% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 66.000000 MSE: 6.000000
SL: 6.148240e-02 HS: 5.493216e-03 ME: Saving to file: bayesopt/bayesopt_0.061482.mat
|  384 | Accept |    0.061482 |      14.906 |    0.032756 |    0.039461 |           40 |            3 |      0.10829 |    0.0059468 |           77 |         relu |      0.32462 |           66 |         tanh |     0.043662 |            6 |         none |      0.47751 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          48              2             4.5278             0.0043497        60     relu    0.41947    76     none    0.47711    50     relu    0.034547

ht_array length: 1lstm
    60    76

    "lstm"    "lstm"

    "relu"    "none"

    0.4195    0.4771

Type: lstm Hidden size: 60 Activation: relu Dropout: 0.419468
Type: lstm Hidden size: 76 Activation: none Dropout: 0.477115
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 60 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 76 hidden units
     6   ''   Dropout             48% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 60 ME: 76.000000 MSE: 0.047021
SL: 3.438756e-03 HS: Saving to file: bayesopt/bayesopt_0.047021.mat
|  385 | Accept |    0.047021 |      10.933 |    0.032756 |    0.038811 |           48 |            2 |       4.5278 |    0.0043497 |           60 |         relu |      0.41947 |           76 |         none |      0.47711 |           50 |         relu |     0.034547 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________

          20              2             0.12069             0.077545        26     relu    0.037426     9     none    0.17373    68     tanh    0.056651

ht_array length: 1lstm
    26     9

    "lstm"    "lstm"

    "relu"    "none"

    0.0374    0.1737

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.037426
Type: lstm Hidden size: 9 Activation: none Dropout: 0.173729
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 9 hidden units
     6   ''   Dropout             17% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 9.000000 MSE: 0.086189
SL: 1.069925e-02 HS: Saving to file: bayesopt/bayesopt_0.086189.mat
|  386 | Accept |    0.086189 |      9.4512 |    0.032756 |    0.032806 |           20 |            2 |      0.12069 |     0.077545 |           26 |         relu |     0.037426 |            9 |         none |      0.17373 |           68 |         tanh |     0.056651 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          43              3             0.10668             0.003443        61     relu    0.22552    89     tanh    0.017598    88     none    0.026781

ht_array length: 1lstm
    61    89    88

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.2255    0.0176    0.0268

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.225520
Type: lstm Hidden size: 89 Activation: tanh Dropout: 0.017598
Type: lstm Hidden size: 88 Activation: none Dropout: 0.026781
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 89 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             2% dropout
     8   ''   LSTM                LSTM with 88 hidden units
     9   ''   Dropout             3% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 89.000000 MSE: 88.000000
SL: 5.173261e-02 HS: 3.996210e-03 ME: Saving to file: bayesopt/bayesopt_0.051733.mat
|  387 | Accept |    0.051733 |      17.736 |    0.032756 |    0.038645 |           43 |            3 |      0.10668 |     0.003443 |           61 |         relu |      0.22552 |           89 |         tanh |     0.017598 |           88 |         none |     0.026781 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          42              2             0.44363            0.0017451        43     relu    0.23063    23     none    0.026298     6     relu    0.26354

ht_array length: 1lstm
    43    23

    "lstm"    "lstm"

    "relu"    "none"

    0.2306    0.0263

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.230629
Type: lstm Hidden size: 23 Activation: none Dropout: 0.026298
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 23 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 23.000000 MSE: 0.047524
SL: 3.435100e-03 HS: Saving to file: bayesopt/bayesopt_0.047524.mat
|  388 | Accept |    0.047524 |      6.8374 |    0.032756 |    0.039361 |           42 |            2 |      0.44363 |    0.0017451 |           43 |         relu |      0.23063 |           23 |         none |     0.026298 |            6 |         relu |      0.26354 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          50              3             0.19307            0.0023066        85     relu    0.36304    70     none    0.48119     4     tanh    0.041965

ht_array length: 1lstm
    85    70     4

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "tanh"

    0.3630    0.4812    0.0420

Type: lstm Hidden size: 85 Activation: relu Dropout: 0.363036
Type: lstm Hidden size: 70 Activation: none Dropout: 0.481195
Type: lstm Hidden size: 4 Activation: tanh Dropout: 0.041965
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 85 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             48% dropout
     7   ''   LSTM                LSTM with 4 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             4% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 85 ME: 70.000000 MSE: 4.000000
SL: 7.640364e-02 HS: 8.561002e-03 ME: Saving to file: bayesopt/bayesopt_0.076404.mat
|  389 | Accept |    0.076404 |      12.808 |    0.032756 |    0.036997 |           50 |            3 |      0.19307 |    0.0023066 |           85 |         relu |      0.36304 |           70 |         none |      0.48119 |            4 |         tanh |     0.041965 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______

          35              2             2.7474             0.0062073        37     relu    0.021858    60     none    0.30155    58     relu    0.0997

ht_array length: 1lstm
    37    60

    "lstm"    "lstm"

    "relu"    "none"

    0.0219    0.3015

Type: lstm Hidden size: 37 Activation: relu Dropout: 0.021858
Type: lstm Hidden size: 60 Activation: none Dropout: 0.301547
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 37 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 37 ME: 60.000000 MSE: 0.041970
SL: 2.638414e-03 HS: Saving to file: bayesopt/bayesopt_0.04197.mat
|  390 | Accept |     0.04197 |      8.5901 |    0.032756 |    0.037779 |           35 |            2 |       2.7474 |    0.0062073 |           37 |         relu |     0.021858 |           60 |         none |      0.30155 |           58 |         relu |       0.0997 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          50              2             0.51685            0.0025386        44     relu    0.49584    93     tanh    0.035212    91     none    0.36789

ht_array length: 1lstm
    44    93

    "lstm"    "lstm"

    "relu"    "tanh"

    0.4958    0.0352

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.495844
Type: lstm Hidden size: 93 Activation: tanh Dropout: 0.035212
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             50% dropout
     5   ''   LSTM                LSTM with 93 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             4% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 93.000000 MSE: 0.050666
SL: 4.418551e-03 HS: Saving to file: bayesopt/bayesopt_0.050666.mat
|  391 | Accept |    0.050666 |      10.042 |    0.032756 |    0.032807 |           50 |            2 |      0.51685 |    0.0025386 |           44 |         relu |      0.49584 |           93 |         tanh |     0.035212 |           91 |         none |      0.36789 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______

          17              2             1.9751             0.0011612        25     relu    0.021673    87     none    0.32423     7     relu    0.3126

ht_array length: 1lstm
    25    87

    "lstm"    "lstm"

    "relu"    "none"

    0.0217    0.3242

Type: lstm Hidden size: 25 Activation: relu Dropout: 0.021673
Type: lstm Hidden size: 87 Activation: none Dropout: 0.324230
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 25 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 87 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 25 ME: 87.000000 MSE: 0.045535
SL: 3.445783e-03 HS: Saving to file: bayesopt/bayesopt_0.045535.mat
|  392 | Accept |    0.045535 |      13.804 |    0.032756 |    0.038745 |           17 |            2 |       1.9751 |    0.0011612 |           25 |         relu |     0.021673 |           87 |         none |      0.32423 |            7 |         relu |       0.3126 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          41              3             0.73404             0.008162        82     relu    0.15323    58     none    0.4937    13     none    0.39642

ht_array length: 1lstm
    82    58    13

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.1532    0.4937    0.3964

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.153234
Type: lstm Hidden size: 58 Activation: none Dropout: 0.493703
Type: lstm Hidden size: 13 Activation: none Dropout: 0.396418
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 58 hidden units
     6   ''   Dropout             49% dropout
     7   ''   LSTM                LSTM with 13 hidden units
     8   ''   Dropout             40% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 58.000000 MSE: 13.000000
SL: 6.316061e-02 HS: 6.323126e-03 ME: Saving to file: bayesopt/bayesopt_0.063161.mat
|  393 | Accept |    0.063161 |      12.679 |    0.032756 |    0.035133 |           41 |            3 |      0.73404 |     0.008162 |           82 |         relu |      0.15323 |           58 |         none |       0.4937 |           13 |         none |      0.39642 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          22              2             4.1433              0.016522        43     relu    0.27671    81     none    0.14232     8     none    0.41476

ht_array length: 1lstm
    43    81

    "lstm"    "lstm"

    "relu"    "none"

    0.2767    0.1423

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.276705
Type: lstm Hidden size: 81 Activation: none Dropout: 0.142317
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   Dropout             14% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 81.000000 MSE: 0.064452
SL: 5.760921e-03 HS: Saving to file: bayesopt/bayesopt_0.064452.mat
|  394 | Accept |    0.064452 |      13.755 |    0.032756 |    0.032846 |           22 |            2 |       4.1433 |     0.016522 |           43 |         relu |      0.27671 |           81 |         none |      0.14232 |            8 |         none |      0.41476 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          14              2             1.5241             0.0040921        82     relu    0.4938     5     tanh    0.15902    57     none    0.49175

ht_array length: 1lstm
    82     5

    "lstm"    "lstm"

    "relu"    "tanh"

    0.4938    0.1590

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.493795
Type: lstm Hidden size: 5 Activation: tanh Dropout: 0.159017
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 5 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             16% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 5.000000 MSE: 0.073190
SL: 7.646128e-03 HS: Saving to file: bayesopt/bayesopt_0.07319.mat
|  395 | Accept |     0.07319 |      15.255 |    0.032756 |    0.038022 |           14 |            2 |       1.5241 |    0.0040921 |           82 |         relu |       0.4938 |            5 |         tanh |      0.15902 |           57 |         none |      0.49175 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          26              2             0.19163            0.0026913        98     relu    0.37808    63     none    0.26607     1     relu    0.15641

ht_array length: 1lstm
    98    63

    "lstm"    "lstm"

    "relu"    "none"

    0.3781    0.2661

Type: lstm Hidden size: 98 Activation: relu Dropout: 0.378076
Type: lstm Hidden size: 63 Activation: none Dropout: 0.266070
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 98 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 63 hidden units
     6   ''   Dropout             27% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 98 ME: 63.000000 MSE: 0.043649
SL: 2.653760e-03 HS: Saving to file: bayesopt/bayesopt_0.043649.mat
|  396 | Accept |    0.043649 |      14.661 |    0.032756 |    0.034491 |           26 |            2 |      0.19163 |    0.0026913 |           98 |         relu |      0.37808 |           63 |         none |      0.26607 |            1 |         relu |      0.15641 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ______    ___    ____    _______

          48              2             0.38751             0.020731        26     relu    0.029933    27     none    0.3096    80     relu    0.45603

ht_array length: 1lstm
    26    27

    "lstm"    "lstm"

    "relu"    "none"

    0.0299    0.3096

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.029933
Type: lstm Hidden size: 27 Activation: none Dropout: 0.309599
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 27 hidden units
     6   ''   Dropout             31% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 27.000000 MSE: 0.047293
SL: 3.341459e-03 HS: Saving to file: bayesopt/bayesopt_0.047293.mat
|  397 | Accept |    0.047293 |      6.7966 |    0.032756 |    0.034674 |           48 |            2 |      0.38751 |     0.020731 |           26 |         relu |     0.029933 |           27 |         none |       0.3096 |           80 |         relu |      0.45603 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          18              2             0.84862            0.0012396        49     relu    0.056875    94     none    0.10418     2     relu    0.19198

ht_array length: 1lstm
    49    94

    "lstm"    "lstm"

    "relu"    "none"

    0.0569    0.1042

Type: lstm Hidden size: 49 Activation: relu Dropout: 0.056875
Type: lstm Hidden size: 94 Activation: none Dropout: 0.104184
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 49 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 49 ME: 94.000000 MSE: 0.048182
SL: 3.408914e-03 HS: Saving to file: bayesopt/bayesopt_0.048182.mat
|  398 | Accept |    0.048182 |      15.401 |    0.032756 |    0.035934 |           18 |            2 |      0.84862 |    0.0012396 |           49 |         relu |     0.056875 |           94 |         none |      0.10418 |            2 |         relu |      0.19198 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    _______

          10              2             0.19108            0.0020919        29     relu    0.0075812    81     none    0.11907     5     relu    0.28988

ht_array length: 1lstm
    29    81

    "lstm"    "lstm"

    "relu"    "none"

    0.0076    0.1191

Type: lstm Hidden size: 29 Activation: relu Dropout: 0.007581
Type: lstm Hidden size: 81 Activation: none Dropout: 0.119069
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 29 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   Dropout             12% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 29 ME: 81.000000 MSE: 0.063298
SL: 6.087449e-03 HS: Saving to file: bayesopt/bayesopt_0.063298.mat
|  399 | Accept |    0.063298 |      18.833 |    0.032756 |    0.038109 |           10 |            2 |      0.19108 |    0.0020919 |           29 |         relu |    0.0075812 |           81 |         none |      0.11907 |            5 |         relu |      0.28988 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          34              2              3.37               0.014054        68     relu    0.43598    64     relu    0.017833    27     none    0.29454

ht_array length: 1lstm
    68    64

    "lstm"    "lstm"

    "relu"    "relu"

    0.4360    0.0178

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.435980
Type: lstm Hidden size: 64 Activation: relu Dropout: 0.017833
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             2% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 64.000000 MSE: 0.058326
SL: 5.044573e-03 HS: Saving to file: bayesopt/bayesopt_0.058326.mat
|  400 | Accept |    0.058326 |      11.202 |    0.032756 |    0.034048 |           34 |            2 |         3.37 |     0.014054 |           68 |         relu |      0.43598 |           64 |         relu |     0.017833 |           27 |         none |      0.29454 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          46              3             0.27279             0.027186        74     relu    0.18797    76     none    0.42568     6     relu    0.18574

ht_array length: 1lstm
    74    76     6

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1880    0.4257    0.1857

Type: lstm Hidden size: 74 Activation: relu Dropout: 0.187967
Type: lstm Hidden size: 76 Activation: none Dropout: 0.425682
Type: lstm Hidden size: 6 Activation: relu Dropout: 0.185739
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 74 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 76 hidden units
     6   ''   Dropout             43% dropout
     7   ''   LSTM                LSTM with 6 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             19% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 74 ME: 76.000000 MSE: 6.000000
SL: 1.850321e-01 HS: 4.611420e-02 ME: Saving to file: bayesopt/bayesopt_0.18503.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  401 | Accept |     0.18503 |      14.137 |    0.032756 |    0.039173 |           46 |            3 |      0.27279 |     0.027186 |           74 |         relu |      0.18797 |           76 |         none |      0.42568 |            6 |         relu |      0.18574 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          48              3             0.42658            0.0058428        82     relu    0.059543    92     relu    0.023774    31     none    0.37668

ht_array length: 1lstm
    82    92    31

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "none"

    0.0595    0.0238    0.3767

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.059543
Type: lstm Hidden size: 92 Activation: relu Dropout: 0.023774
Type: lstm Hidden size: 31 Activation: none Dropout: 0.376679
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 92 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             2% dropout
     8   ''   LSTM                LSTM with 31 hidden units
     9   ''   Dropout             38% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 92.000000 MSE: 31.000000
SL: 2.396403e-01 HS: 1.105603e-01 ME: Saving to file: bayesopt/bayesopt_0.23964.mat
|  402 | Accept |     0.23964 |      18.954 |    0.032756 |    0.034835 |           48 |            3 |      0.42658 |    0.0058428 |           82 |         relu |     0.059543 |           92 |         relu |     0.023774 |           31 |         none |      0.37668 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _____

          26              2             4.3247             0.0046281        96     relu    0.49531     7     none    0.41919    34     none    0.496

ht_array length: 1lstm
    96     7

    "lstm"    "lstm"

    "relu"    "none"

    0.4953    0.4192

Type: lstm Hidden size: 96 Activation: relu Dropout: 0.495313
Type: lstm Hidden size: 7 Activation: none Dropout: 0.419193
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 96 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             50% dropout
     5   ''   LSTM                LSTM with 7 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 7.000000 MSE: 0.049974
SL: 4.029426e-03 HS: Saving to file: bayesopt/bayesopt_0.049974.mat
|  403 | Accept |    0.049974 |      11.028 |    0.032756 |    0.033366 |           26 |            2 |       4.3247 |    0.0046281 |           96 |         relu |      0.49531 |            7 |         none |      0.41919 |           34 |         none |        0.496 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _________    ___    ____    _______

          34              3             2.1173             0.0024578        31     relu    0.20152    56     none    0.0068129    10     none    0.48068

ht_array length: 1lstm
    31    56    10

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.2015    0.0068    0.4807

Type: lstm Hidden size: 31 Activation: relu Dropout: 0.201525
Type: lstm Hidden size: 56 Activation: none Dropout: 0.006813
Type: lstm Hidden size: 10 Activation: none Dropout: 0.480678
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 31 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 56 hidden units
     6   ''   Dropout             1% dropout
     7   ''   LSTM                LSTM with 10 hidden units
     8   ''   Dropout             48% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 31 ME: 56.000000 MSE: 10.000000
SL: 1.913828e-01 HS: 5.355007e-02 ME: Saving to file: bayesopt/bayesopt_0.19138.mat
|  404 | Accept |     0.19138 |      11.447 |    0.032756 |    0.032824 |           34 |            3 |       2.1173 |    0.0024578 |           31 |         relu |      0.20152 |           56 |         none |    0.0068129 |           10 |         none |      0.48068 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          43              3             0.45087            0.0048588        28     relu    0.26068    71     none    0.043484    100    relu    0.29885

ht_array length: 1lstm
    28    71   100

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2607    0.0435    0.2988

Type: lstm Hidden size: 28 Activation: relu Dropout: 0.260680
Type: lstm Hidden size: 71 Activation: none Dropout: 0.043484
Type: lstm Hidden size: 100 Activation: relu Dropout: 0.298846
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 28 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 71 hidden units
     6   ''   Dropout             4% dropout
     7   ''   LSTM                LSTM with 100 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             30% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 28 ME: 71.000000 MSE: 100.000000
SL: 4.851994e-02 HS: 3.711309e-03 ME: Saving to file: bayesopt/bayesopt_0.04852.mat
|  405 | Accept |     0.04852 |      14.606 |    0.032756 |    0.035827 |           43 |            3 |      0.45087 |    0.0048588 |           28 |         relu |      0.26068 |           71 |         none |     0.043484 |          100 |         relu |      0.29885 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          21              2             1.6243             0.0028915        56     relu    0.1684    29     tanh    0.013972    91     none    0.16565

ht_array length: 1lstm
    56    29

    "lstm"    "lstm"

    "relu"    "tanh"

    0.1684    0.0140

Type: lstm Hidden size: 56 Activation: relu Dropout: 0.168397
Type: lstm Hidden size: 29 Activation: tanh Dropout: 0.013972
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 56 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             1% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 56 ME: 29.000000 MSE: 0.051486
SL: 3.898347e-03 HS: Saving to file: bayesopt/bayesopt_0.051486.mat
|  406 | Accept |    0.051486 |      11.107 |    0.032756 |    0.038384 |           21 |            2 |       1.6243 |    0.0028915 |           56 |         relu |       0.1684 |           29 |         tanh |     0.013972 |           91 |         none |      0.16565 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          49              3             0.12433             0.008214        17     relu    0.43367    16     none    0.15173    53     none    0.48045

ht_array length: 1lstm
    17    16    53

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.4337    0.1517    0.4804

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.433675
Type: lstm Hidden size: 16 Activation: none Dropout: 0.151726
Type: lstm Hidden size: 53 Activation: none Dropout: 0.480446
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 16 hidden units
     6   ''   Dropout             15% dropout
     7   ''   LSTM                LSTM with 53 hidden units
     8   ''   Dropout             48% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 16.000000 MSE: 53.000000
SL: 7.513183e-02 HS: 8.189009e-03 ME: Saving to file: bayesopt/bayesopt_0.075132.mat
|  407 | Accept |    0.075132 |      8.5717 |    0.032756 |    0.038129 |           49 |            3 |      0.12433 |     0.008214 |           17 |         relu |      0.43367 |           16 |         none |      0.15173 |           53 |         none |      0.48045 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          42              3             0.56343            0.0063412        71     relu    0.43192    61     none    0.2458    15     none    0.23293

ht_array length: 1lstm
    71    61    15

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.4319    0.2458    0.2329

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.431917
Type: lstm Hidden size: 61 Activation: none Dropout: 0.245797
Type: lstm Hidden size: 15 Activation: none Dropout: 0.232926
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 61 hidden units
     6   ''   Dropout             25% dropout
     7   ''   LSTM                LSTM with 15 hidden units
     8   ''   Dropout             23% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 61.000000 MSE: 15.000000
SL: 4.946099e-02 HS: 3.696002e-03 ME: Saving to file: bayesopt/bayesopt_0.049461.mat
|  408 | Accept |    0.049461 |      11.532 |    0.032756 |    0.041013 |           42 |            3 |      0.56343 |    0.0063412 |           71 |         relu |      0.43192 |           61 |         none |       0.2458 |           15 |         none |      0.23293 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2      drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _________    ___    ____    ________

          28              2             0.87711            0.0016351        30     relu    0.4907    47     none    0.0016602    46     relu    0.078489

ht_array length: 1lstm
    30    47

    "lstm"    "lstm"

    "relu"    "none"

    0.4907    0.0017

Type: lstm Hidden size: 30 Activation: relu Dropout: 0.490695
Type: lstm Hidden size: 47 Activation: none Dropout: 0.001660
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 30 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 47 hidden units
     6   ''   Dropout             0% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 30 ME: 47.000000 MSE: 0.065720
SL: 6.238881e-03 HS: Saving to file: bayesopt/bayesopt_0.06572.mat
|  409 | Accept |     0.06572 |      9.3218 |    0.032756 |    0.037441 |           28 |            2 |      0.87711 |    0.0016351 |           30 |         relu |       0.4907 |           47 |         none |    0.0016602 |           46 |         relu |     0.078489 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          36              2             2.0514             0.0085325        60     relu    0.16753    12     tanh    0.36129    52     none    0.46601

ht_array length: 1lstm
    60    12

    "lstm"    "lstm"

    "relu"    "tanh"

    0.1675    0.3613

Type: lstm Hidden size: 60 Activation: relu Dropout: 0.167535
Type: lstm Hidden size: 12 Activation: tanh Dropout: 0.361289
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 60 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             36% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 60 ME: 12.000000 MSE: 0.065836
SL: 6.466816e-03 HS: Saving to file: bayesopt/bayesopt_0.065836.mat
|  410 | Accept |    0.065836 |      8.2655 |    0.032756 |    0.036184 |           36 |            2 |       2.0514 |    0.0085325 |           60 |         relu |      0.16753 |           12 |         tanh |      0.36129 |           52 |         none |      0.46601 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          29              3             0.15409             0.010766        48     relu    0.32723    92     none    0.072172    77     relu    0.19833

ht_array length: 1lstm
    48    92    77

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3272    0.0722    0.1983

Type: lstm Hidden size: 48 Activation: relu Dropout: 0.327230
Type: lstm Hidden size: 92 Activation: none Dropout: 0.072172
Type: lstm Hidden size: 77 Activation: relu Dropout: 0.198329
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 48 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 92 hidden units
     6   ''   Dropout             7% dropout
     7   ''   LSTM                LSTM with 77 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             20% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 48 ME: 92.000000 MSE: 77.000000
SL: 8.537130e-02 HS: 1.202359e-02 ME: Saving to file: bayesopt/bayesopt_0.085371.mat
|  411 | Accept |    0.085371 |      17.879 |    0.032756 |    0.038509 |           29 |            3 |      0.15409 |     0.010766 |           48 |         relu |      0.32723 |           92 |         none |     0.072172 |           77 |         relu |      0.19833 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ______

          39              3             0.30949            0.0036309        61     relu    0.38441    22     tanh    0.014252    93     none    0.1878

ht_array length: 1lstm
    61    22    93

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.3844    0.0143    0.1878

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.384407
Type: lstm Hidden size: 22 Activation: tanh Dropout: 0.014252
Type: lstm Hidden size: 93 Activation: none Dropout: 0.187800
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 22 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             1% dropout
     8   ''   LSTM                LSTM with 93 hidden units
     9   ''   Dropout             19% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 22.000000 MSE: 93.000000
SL: 7.514958e-02 HS: 7.768129e-03 ME: Saving to file: bayesopt/bayesopt_0.07515.mat
|  412 | Accept |     0.07515 |      14.228 |    0.032756 |    0.035336 |           39 |            3 |      0.30949 |    0.0036309 |           61 |         relu |      0.38441 |           22 |         tanh |     0.014252 |           93 |         none |       0.1878 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          11              2             4.0651              0.008233        42     relu    0.20236    82     none    0.030126    82     none    0.067286

ht_array length: 1lstm
    42    82

    "lstm"    "lstm"

    "relu"    "none"

    0.2024    0.0301

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.202360
Type: lstm Hidden size: 82 Activation: none Dropout: 0.030126
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 82 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 82.000000 MSE: 0.052687
SL: 4.103332e-03 HS: Saving to file: bayesopt/bayesopt_0.052687.mat
|  413 | Accept |    0.052687 |      17.133 |    0.032756 |    0.036074 |           11 |            2 |       4.0651 |     0.008233 |           42 |         relu |      0.20236 |           82 |         none |     0.030126 |           82 |         none |     0.067286 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          46              2             2.3017             0.0012804        53     relu    0.36766     5     none    0.046056    94     relu    0.056594

ht_array length: 1lstm
    53     5

    "lstm"    "lstm"

    "relu"    "none"

    0.3677    0.0461

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.367661
Type: lstm Hidden size: 5 Activation: none Dropout: 0.046056
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 5 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 5.000000 MSE: 0.063166
SL: 6.079584e-03 HS: Saving to file: bayesopt/bayesopt_0.063166.mat
|  414 | Accept |    0.063166 |      7.1242 |    0.032756 |    0.040297 |           46 |            2 |       2.3017 |    0.0012804 |           53 |         relu |      0.36766 |            5 |         none |     0.046056 |           94 |         relu |     0.056594 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          32              2             1.2128             0.0062085        62     relu    0.074762    24     tanh    0.035144     4     none    0.29981

ht_array length: 1lstm
    62    24

    "lstm"    "lstm"

    "relu"    "tanh"

    0.0748    0.0351

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.074762
Type: lstm Hidden size: 24 Activation: tanh Dropout: 0.035144
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 24 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             4% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 24.000000 MSE: 0.049378
SL: 3.951203e-03 HS: Saving to file: bayesopt/bayesopt_0.049378.mat
|  415 | Accept |    0.049378 |      9.0932 |    0.032756 |    0.038203 |           32 |            2 |       1.2128 |    0.0062085 |           62 |         relu |     0.074762 |           24 |         tanh |     0.035144 |            4 |         none |      0.29981 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          45              3             0.23658             0.013012        29     relu    0.071378    66     none    0.17023    95     relu    0.13605

ht_array length: 1lstm
    29    66    95

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.0714    0.1702    0.1361

Type: lstm Hidden size: 29 Activation: relu Dropout: 0.071378
Type: lstm Hidden size: 66 Activation: none Dropout: 0.170231
Type: lstm Hidden size: 95 Activation: relu Dropout: 0.136052
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 29 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 66 hidden units
     6   ''   Dropout             17% dropout
     7   ''   LSTM                LSTM with 95 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             14% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 29 ME: 66.000000 MSE: 95.000000
SL: 6.802625e-02 HS: 7.135256e-03 ME: Saving to file: bayesopt/bayesopt_0.068026.mat
|  416 | Accept |    0.068026 |      14.604 |    0.032756 |    0.036569 |           45 |            3 |      0.23658 |     0.013012 |           29 |         relu |     0.071378 |           66 |         none |      0.17023 |           95 |         relu |      0.13605 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2             2.0875             0.0055676        92     relu    0.28038    63     none    0.49502    26     relu    0.48496

ht_array length: 1lstm
    92    63

    "lstm"    "lstm"

    "relu"    "none"

    0.2804    0.4950

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.280383
Type: lstm Hidden size: 63 Activation: none Dropout: 0.495018
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 63 hidden units
     6   ''   Dropout             50% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 63.000000 MSE: 0.047647
SL: 3.131763e-03 HS: Saving to file: bayesopt/bayesopt_0.047647.mat
|  417 | Accept |    0.047647 |      10.314 |    0.032756 |    0.039963 |           50 |            2 |       2.0875 |    0.0055676 |           92 |         relu |      0.28038 |           63 |         none |      0.49502 |           26 |         relu |      0.48496 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          49              3             2.8511             0.0075286        19     relu    0.2765     8     tanh    0.10749    15     none    0.36687

ht_array length: 1lstm
    19     8    15

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.2765    0.1075    0.3669

Type: lstm Hidden size: 19 Activation: relu Dropout: 0.276496
Type: lstm Hidden size: 8 Activation: tanh Dropout: 0.107490
Type: lstm Hidden size: 15 Activation: none Dropout: 0.366871
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 19 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             11% dropout
     8   ''   LSTM                LSTM with 15 hidden units
     9   ''   Dropout             37% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 19 ME: 8.000000 MSE: 15.000000
SL: 1.687503e-01 HS: 3.886780e-02 ME: Saving to file: bayesopt/bayesopt_0.16875.mat
|  418 | Accept |     0.16875 |      7.5006 |    0.032756 |    0.032791 |           49 |            3 |       2.8511 |    0.0075286 |           19 |         relu |       0.2765 |            8 |         tanh |      0.10749 |           15 |         none |      0.36687 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          49              2              3.382              0.00428         34     relu    0.43751    38     none    0.14542    18     relu    0.084574

ht_array length: 1lstm
    34    38

    "lstm"    "lstm"

    "relu"    "none"

    0.4375    0.1454

Type: lstm Hidden size: 34 Activation: relu Dropout: 0.437510
Type: lstm Hidden size: 38 Activation: none Dropout: 0.145419
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 34 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   Dropout             15% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 34 ME: 38.000000 MSE: 0.053245
SL: 4.449797e-03 HS: Saving to file: bayesopt/bayesopt_0.053245.mat
|  419 | Accept |    0.053245 |      6.1365 |    0.032756 |    0.038491 |           49 |            2 |        3.382 |      0.00428 |           34 |         relu |      0.43751 |           38 |         none |      0.14542 |           18 |         relu |     0.084574 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1    hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _____    ___    ____    ________    ___    ____    ________

          24              3             2.3082             0.0096447        92     relu    0.463    42     none    0.036789    43     relu    0.088573

ht_array length: 1lstm
    92    42    43

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4630    0.0368    0.0886

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.462998
Type: lstm Hidden size: 42 Activation: none Dropout: 0.036789
Type: lstm Hidden size: 43 Activation: relu Dropout: 0.088573
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             46% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             4% dropout
     7   ''   LSTM                LSTM with 43 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             9% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 42.000000 MSE: 43.000000
SL: 1.861928e-01 HS: 4.670839e-02 ME: Saving to file: bayesopt/bayesopt_0.18619.mat
|  420 | Accept |     0.18619 |      16.259 |    0.032756 |    0.032794 |           24 |            3 |       2.3082 |    0.0096447 |           92 |         relu |        0.463 |           42 |         none |     0.036789 |           43 |         relu |     0.088573 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          47              3             0.13304            0.0093437        59     relu    0.41338    11     none    0.028659     6     relu    0.13053

ht_array length: 1lstm
    59    11     6

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4134    0.0287    0.1305

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.413381
Type: lstm Hidden size: 11 Activation: none Dropout: 0.028659
Type: lstm Hidden size: 6 Activation: relu Dropout: 0.130526
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   Dropout             3% dropout
     7   ''   LSTM                LSTM with 6 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             13% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 11.000000 MSE: 6.000000
SL: 1.851328e-01 HS: 4.604575e-02 ME: Saving to file: bayesopt/bayesopt_0.18513.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  421 | Accept |     0.18513 |       10.08 |    0.032756 |    0.032802 |           47 |            3 |      0.13304 |    0.0093437 |           59 |         relu |      0.41338 |           11 |         none |     0.028659 |            6 |         relu |      0.13053 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2    drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ______    ___    ____    ______

          49              2             0.18275             0.057581        17     relu    0.013153     8     none    0.3179    80     tanh    0.3275

ht_array length: 1lstm
    17     8

    "lstm"    "lstm"

    "relu"    "none"

    0.0132    0.3179

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.013153
Type: lstm Hidden size: 8 Activation: none Dropout: 0.317899
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 8.000000 MSE: 0.067590
SL: 7.266477e-03 HS: Saving to file: bayesopt/bayesopt_0.06759.mat
|  422 | Accept |     0.06759 |      5.8323 |    0.032756 |    0.034902 |           49 |            2 |      0.18275 |     0.057581 |           17 |         relu |     0.013153 |            8 |         none |       0.3179 |           80 |         tanh |       0.3275 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2             1.1534             0.0090183        91     relu    0.20269    75     tanh    0.16207    14     none    0.41525

ht_array length: 1lstm
    91    75

    "lstm"    "lstm"

    "relu"    "tanh"

    0.2027    0.1621

Type: lstm Hidden size: 91 Activation: relu Dropout: 0.202694
Type: lstm Hidden size: 75 Activation: tanh Dropout: 0.162073
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 91 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 75 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             16% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 91 ME: 75.000000 MSE: 0.038988
SL: 2.323541e-03 HS: Saving to file: bayesopt/bayesopt_0.038988.mat
|  423 | Accept |    0.038988 |      11.635 |    0.032756 |    0.038009 |           50 |            2 |       1.1534 |    0.0090183 |           91 |         relu |      0.20269 |           75 |         tanh |      0.16207 |           14 |         none |      0.41525 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          43              3             3.3183              0.012861        58     relu    0.1412     7     none    0.086263     2     relu    0.39901

ht_array length: 1lstm
    58     7     2

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1412    0.0863    0.3990

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.141199
Type: lstm Hidden size: 7 Activation: none Dropout: 0.086263
Type: lstm Hidden size: 2 Activation: relu Dropout: 0.399007
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 7 hidden units
     6   ''   Dropout             9% dropout
     7   ''   LSTM                LSTM with 2 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             40% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 7.000000 MSE: 2.000000
SL: 1.822191e-01 HS: 4.528815e-02 ME: Saving to file: bayesopt/bayesopt_0.18222.mat
|  424 | Accept |     0.18222 |      9.7056 |    0.032756 |      0.0404 |           43 |            3 |       3.3183 |     0.012861 |           58 |         relu |       0.1412 |            7 |         none |     0.086263 |            2 |         relu |      0.39901 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          38              3             3.0369              0.027154        82     relu    0.13929     9     none    0.24823    42     relu    0.038287

ht_array length: 1lstm
    82     9    42

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1393    0.2482    0.0383

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.139290
Type: lstm Hidden size: 9 Activation: none Dropout: 0.248227
Type: lstm Hidden size: 42 Activation: relu Dropout: 0.038287
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 9 hidden units
     6   ''   Dropout             25% dropout
     7   ''   LSTM                LSTM with 42 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             4% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 9.000000 MSE: 42.000000
SL: 1.829991e-01 HS: 4.535560e-02 ME: Saving to file: bayesopt/bayesopt_0.183.mat
|  425 | Accept |       0.183 |       11.43 |    0.032756 |    0.042314 |           38 |            3 |       3.0369 |     0.027154 |           82 |         relu |      0.13929 |            9 |         none |      0.24823 |           42 |         relu |     0.038287 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          49              2             3.2942             0.0036942        98     relu    0.28251    60     tanh    0.21234    48     none    0.0014278

ht_array length: 1lstm
    98    60

    "lstm"    "lstm"

    "relu"    "tanh"

    0.2825    0.2123

Type: lstm Hidden size: 98 Activation: relu Dropout: 0.282505
Type: lstm Hidden size: 60 Activation: tanh Dropout: 0.212337
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 98 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             21% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 98 ME: 60.000000 MSE: 0.045290
SL: 2.893088e-03 HS: Saving to file: bayesopt/bayesopt_0.04529.mat
|  426 | Accept |     0.04529 |      10.584 |    0.032756 |     0.03279 |           49 |            2 |       3.2942 |    0.0036942 |           98 |         relu |      0.28251 |           60 |         tanh |      0.21234 |           48 |         none |    0.0014278 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          44              3             0.30629             0.028099        58     relu    0.25022     3     none    0.15758    38     relu    0.49511

ht_array length: 1lstm
    58     3    38

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2502    0.1576    0.4951

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.250221
Type: lstm Hidden size: 3 Activation: none Dropout: 0.157580
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.495110
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 3 hidden units
     6   ''   Dropout             16% dropout
     7   ''   LSTM                LSTM with 38 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             50% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 3.000000 MSE: 38.000000
SL: 7.877516e-02 HS: 8.819554e-03 ME: Saving to file: bayesopt/bayesopt_0.078775.mat
|  427 | Accept |    0.078775 |      9.8644 |    0.032756 |    0.032791 |           44 |            3 |      0.30629 |     0.028099 |           58 |         relu |      0.25022 |            3 |         none |      0.15758 |           38 |         relu |      0.49511 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _________    ___    ____    _______

          41              2             0.35502            0.0023692        75     relu    0.44433    100    none    0.0031181    61     relu    0.40732

ht_array length: 1lstm
    75   100

    "lstm"    "lstm"

    "relu"    "none"

    0.4443    0.0031

Type: lstm Hidden size: 75 Activation: relu Dropout: 0.444331
Type: lstm Hidden size: 100 Activation: none Dropout: 0.003118
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 75 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 100 hidden units
     6   ''   Dropout             0% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 75 ME: 100.000000 MSE: 0.043761
SL: 2.783459e-03 HS: Saving to file: bayesopt/bayesopt_0.043761.mat
|  428 | Accept |    0.043761 |      11.683 |    0.032756 |    0.038087 |           41 |            2 |      0.35502 |    0.0023692 |           75 |         relu |      0.44433 |          100 |         none |    0.0031181 |           61 |         relu |      0.40732 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          8               3             0.23984             0.044105        45     relu    0.18887    54     none    0.32466    17     relu    0.37615

ht_array length: 1lstm
    45    54    17

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1889    0.3247    0.3761

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.188866
Type: lstm Hidden size: 54 Activation: none Dropout: 0.324661
Type: lstm Hidden size: 17 Activation: relu Dropout: 0.376148
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 54 hidden units
     6   ''   Dropout             32% dropout
     7   ''   LSTM                LSTM with 17 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             38% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 54.000000 MSE: 17.000000
SL: 1.840803e-01 HS: 4.672955e-02 ME: Saving to file: bayesopt/bayesopt_0.18408.mat
|  429 | Accept |     0.18408 |       26.11 |    0.032756 |    0.032791 |            8 |            3 |      0.23984 |     0.044105 |           45 |         relu |      0.18887 |           54 |         none |      0.32466 |           17 |         relu |      0.37615 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          48              2             2.5859              0.012905        37     relu    0.38291    39     none    0.38705    87     none    0.24509

ht_array length: 1lstm
    37    39

    "lstm"    "lstm"

    "relu"    "none"

    0.3829    0.3871

Type: lstm Hidden size: 37 Activation: relu Dropout: 0.382908
Type: lstm Hidden size: 39 Activation: none Dropout: 0.387054
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 37 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             39% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 37 ME: 39.000000 MSE: 0.044542
SL: 2.739756e-03 HS: Saving to file: bayesopt/bayesopt_0.044542.mat
|  430 | Accept |    0.044542 |      7.3033 |    0.032756 |    0.032851 |           48 |            2 |       2.5859 |     0.012905 |           37 |         relu |      0.38291 |           39 |         none |      0.38705 |           87 |         none |      0.24509 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2             0.74685             0.019955        82     relu    0.31924    21     none    0.22108    68     tanh    0.48674

ht_array length: 1lstm
    82    21

    "lstm"    "lstm"

    "relu"    "none"

    0.3192    0.2211

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.319242
Type: lstm Hidden size: 21 Activation: none Dropout: 0.221079
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             22% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 21.000000 MSE: 0.048000
SL: 3.558165e-03 HS: Saving to file: bayesopt/bayesopt_0.048.mat
|  431 | Accept |       0.048 |      7.6788 |    0.032756 |    0.032783 |           50 |            2 |      0.74685 |     0.019955 |           82 |         relu |      0.31924 |           21 |         none |      0.22108 |           68 |         tanh |      0.48674 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _____

          47              2             1.7376              0.003058        17     relu    0.4409    95     none    0.48063     1     tanh    0.338

ht_array length: 1lstm
    17    95

    "lstm"    "lstm"

    "relu"    "none"

    0.4409    0.4806

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.440897
Type: lstm Hidden size: 95 Activation: none Dropout: 0.480633
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   Dropout             48% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 95.000000 MSE: 0.081969
SL: 9.803339e-03 HS: Saving to file: bayesopt/bayesopt_0.081969.mat
|  432 | Accept |    0.081969 |      9.2344 |    0.032756 |    0.032818 |           47 |            2 |       1.7376 |     0.003058 |           17 |         relu |       0.4409 |           95 |         none |      0.48063 |            1 |         tanh |        0.338 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          42              2             0.28682            0.0099015        84     relu    0.22091    21     none    0.39952    89     tanh    0.28355

ht_array length: 1lstm
    84    21

    "lstm"    "lstm"

    "relu"    "none"

    0.2209    0.3995

Type: lstm Hidden size: 84 Activation: relu Dropout: 0.220911
Type: lstm Hidden size: 21 Activation: none Dropout: 0.399516
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 84 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             22% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             40% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 84 ME: 21.000000 MSE: 0.054353
SL: 4.407336e-03 HS: Saving to file: bayesopt/bayesopt_0.054353.mat
|  433 | Accept |    0.054353 |      8.8604 |    0.032756 |    0.036494 |           42 |            2 |      0.28682 |    0.0099015 |           84 |         relu |      0.22091 |           21 |         none |      0.39952 |           89 |         tanh |      0.28355 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ________

          48              2             0.51734             0.027604         8     relu    0.056768    53     relu    0.022414    89     none    0.058867

ht_array length: 1lstm
     8    53

    "lstm"    "lstm"

    "relu"    "relu"

    0.0568    0.0224

Type: lstm Hidden size: 8 Activation: relu Dropout: 0.056768
Type: lstm Hidden size: 53 Activation: relu Dropout: 0.022414
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 8 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 53 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             2% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 8 ME: 53.000000 MSE: 0.139297
SL: 2.324053e-02 HS: Saving to file: bayesopt/bayesopt_0.1393.mat
|  434 | Accept |      0.1393 |      7.5098 |    0.032756 |    0.036306 |           48 |            2 |      0.51734 |     0.027604 |            8 |         relu |     0.056768 |           53 |         relu |     0.022414 |           89 |         none |     0.058867 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          48              2             3.5632              0.031141        50     relu    0.049335    19     none    0.46826    76     tanh    0.34698

ht_array length: 1lstm
    50    19

    "lstm"    "lstm"

    "relu"    "none"

    0.0493    0.4683

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.049335
Type: lstm Hidden size: 19 Activation: none Dropout: 0.468258
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   Dropout             47% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 19.000000 MSE: 0.049008
SL: 3.484524e-03 HS: Saving to file: bayesopt/bayesopt_0.049008.mat
|  435 | Accept |    0.049008 |      7.6979 |    0.032756 |    0.040307 |           48 |            2 |       3.5632 |     0.031141 |           50 |         relu |     0.049335 |           19 |         none |      0.46826 |           76 |         tanh |      0.34698 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2             4.4239             0.0094281        78     relu    0.21122    56     tanh    0.19028    67     none    0.43625

ht_array length: 1lstm
    78    56

    "lstm"    "lstm"

    "relu"    "tanh"

    0.2112    0.1903

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.211222
Type: lstm Hidden size: 56 Activation: tanh Dropout: 0.190275
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 56 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             19% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 56.000000 MSE: 0.048232
SL: 3.573742e-03 HS: Saving to file: bayesopt/bayesopt_0.048232.mat
|  436 | Accept |    0.048232 |      9.9317 |    0.032756 |    0.044827 |           50 |            2 |       4.4239 |    0.0094281 |           78 |         relu |      0.21122 |           56 |         tanh |      0.19028 |           67 |         none |      0.43625 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          28              2             1.9705             0.0012156        47     relu    0.40333    60     tanh    0.12899    10     none    0.4185

ht_array length: 1lstm
    47    60

    "lstm"    "lstm"

    "relu"    "tanh"

    0.4033    0.1290

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.403332
Type: lstm Hidden size: 60 Activation: tanh Dropout: 0.128993
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             13% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 60.000000 MSE: 0.058364
SL: 4.988626e-03 HS: Saving to file: bayesopt/bayesopt_0.058364.mat
|  437 | Accept |    0.058364 |       10.23 |    0.032756 |    0.037307 |           28 |            2 |       1.9705 |    0.0012156 |           47 |         relu |      0.40333 |           60 |         tanh |      0.12899 |           10 |         none |       0.4185 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ______

          49              3             2.6027              0.052541        39     relu    0.35427    96     relu    0.070825    81     none    0.2126

ht_array length: 1lstm
    39    96    81

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "none"

    0.3543    0.0708    0.2126

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.354275
Type: lstm Hidden size: 96 Activation: relu Dropout: 0.070825
Type: lstm Hidden size: 81 Activation: none Dropout: 0.212597
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             35% dropout
     5   ''   LSTM                LSTM with 96 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             7% dropout
     8   ''   LSTM                LSTM with 81 hidden units
     9   ''   Dropout             21% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 96.000000 MSE: 81.000000
SL: 1.823182e-01 HS: 4.608689e-02 ME: Saving to file: bayesopt/bayesopt_0.18232.mat
|  438 | Accept |     0.18232 |      14.554 |    0.032756 |      0.0383 |           49 |            3 |       2.6027 |     0.052541 |           39 |         relu |      0.35427 |           96 |         relu |     0.070825 |           81 |         none |       0.2126 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          45              2              2.243             0.0098578        91     relu    0.46982    25     tanh    0.078665    23     none    0.024321

ht_array length: 1lstm
    91    25

    "lstm"    "lstm"

    "relu"    "tanh"

    0.4698    0.0787

Type: lstm Hidden size: 91 Activation: relu Dropout: 0.469822
Type: lstm Hidden size: 25 Activation: tanh Dropout: 0.078665
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 91 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 25 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             8% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 91 ME: 25.000000 MSE: 0.053364
SL: 4.144348e-03 HS: Saving to file: bayesopt/bayesopt_0.053364.mat
|  439 | Accept |    0.053364 |      9.8268 |    0.032756 |    0.035124 |           45 |            2 |        2.243 |    0.0098578 |           91 |         relu |      0.46982 |           25 |         tanh |     0.078665 |           23 |         none |     0.024321 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          36              2             0.68453            0.0040075        28     relu    0.2335    73     tanh    0.20401     6     none    0.25481

ht_array length: 1lstm
    28    73

    "lstm"    "lstm"

    "relu"    "tanh"

    0.2335    0.2040

Type: lstm Hidden size: 28 Activation: relu Dropout: 0.233500
Type: lstm Hidden size: 73 Activation: tanh Dropout: 0.204008
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 28 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 73 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             20% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 28 ME: 73.000000 MSE: 0.046779
SL: 3.070705e-03 HS: Saving to file: bayesopt/bayesopt_0.046779.mat
|  440 | Accept |    0.046779 |      8.9914 |    0.032756 |    0.032807 |           36 |            2 |      0.68453 |    0.0040075 |           28 |         relu |       0.2335 |           73 |         tanh |      0.20401 |            6 |         none |      0.25481 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          18              2             3.5721              0.019868         2     relu    0.11571    36     relu    0.13281    34     none    0.24434

ht_array length: 1lstm
     2    36

    "lstm"    "lstm"

    "relu"    "relu"

    0.1157    0.1328

Type: lstm Hidden size: 2 Activation: relu Dropout: 0.115715
Type: lstm Hidden size: 36 Activation: relu Dropout: 0.132807
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 2 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 36 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             13% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 2 ME: 36.000000 MSE: 0.052360
SL: 3.853043e-03 HS: Saving to file: bayesopt/bayesopt_0.05236.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  441 | Accept |     0.05236 |      9.9832 |    0.032756 |    0.041223 |           18 |            2 |       3.5721 |     0.019868 |            2 |         relu |      0.11571 |           36 |         relu |      0.13281 |           34 |         none |      0.24434 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          27              2              1.994             0.0011391        31     relu    0.040859    87     tanh    0.26261    77     none    0.44601

ht_array length: 1lstm
    31    87

    "lstm"    "lstm"

    "relu"    "tanh"

    0.0409    0.2626

Type: lstm Hidden size: 31 Activation: relu Dropout: 0.040859
Type: lstm Hidden size: 87 Activation: tanh Dropout: 0.262606
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 31 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 87 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             26% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 31 ME: 87.000000 MSE: 0.046166
SL: 3.502518e-03 HS: Saving to file: bayesopt/bayesopt_0.046166.mat
|  442 | Accept |    0.046166 |      12.128 |    0.032756 |    0.043368 |           27 |            2 |        1.994 |    0.0011391 |           31 |         relu |     0.040859 |           87 |         tanh |      0.26261 |           77 |         none |      0.44601 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _________

          26              2             0.15126            0.0036709        47     relu    0.43303    33     none    0.1155    24     tanh    0.0017686

ht_array length: 1lstm
    47    33

    "lstm"    "lstm"

    "relu"    "none"

    0.4330    0.1155

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.433030
Type: lstm Hidden size: 33 Activation: none Dropout: 0.115504
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 33 hidden units
     6   ''   Dropout             12% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 33.000000 MSE: 0.049729
SL: 3.477038e-03 HS: Saving to file: bayesopt/bayesopt_0.049729.mat
|  443 | Accept |    0.049729 |      9.3484 |    0.032756 |    0.043411 |           26 |            2 |      0.15126 |    0.0036709 |           47 |         relu |      0.43303 |           33 |         none |       0.1155 |           24 |         tanh |    0.0017686 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          49              2             2.2805             0.0024496        31     relu    0.11639    86     none    0.4329    94     relu    0.14246

ht_array length: 1lstm
    31    86

    "lstm"    "lstm"

    "relu"    "none"

    0.1164    0.4329

Type: lstm Hidden size: 31 Activation: relu Dropout: 0.116385
Type: lstm Hidden size: 86 Activation: none Dropout: 0.432897
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 31 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   Dropout             43% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 31 ME: 86.000000 MSE: 0.047612
SL: 3.282610e-03 HS: Saving to file: bayesopt/bayesopt_0.047612.mat
|  444 | Accept |    0.047612 |      8.0981 |    0.032756 |    0.033212 |           49 |            2 |       2.2805 |    0.0024496 |           31 |         relu |      0.11639 |           86 |         none |       0.4329 |           94 |         relu |      0.14246 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          36              2             2.8576             0.0050243        13     relu    0.39728    84     relu    0.11359    88     none    0.094205

ht_array length: 1lstm
    13    84

    "lstm"    "lstm"

    "relu"    "relu"

    0.3973    0.1136

Type: lstm Hidden size: 13 Activation: relu Dropout: 0.397285
Type: lstm Hidden size: 84 Activation: relu Dropout: 0.113590
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 13 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             11% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 13 ME: 84.000000 MSE: 0.050846
SL: 3.898494e-03 HS: Saving to file: bayesopt/bayesopt_0.050846.mat
|  445 | Accept |    0.050846 |      8.4875 |    0.032756 |    0.032797 |           36 |            2 |       2.8576 |    0.0050243 |           13 |         relu |      0.39728 |           84 |         relu |      0.11359 |           88 |         none |     0.094205 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          25              2             0.14198            0.0063272        17     relu    0.092007    32     relu    0.17357    59     none    0.14208

ht_array length: 1lstm
    17    32

    "lstm"    "lstm"

    "relu"    "relu"

    0.0920    0.1736

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.092007
Type: lstm Hidden size: 32 Activation: relu Dropout: 0.173566
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 32 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             17% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 32.000000 MSE: 0.048924
SL: 3.648480e-03 HS: Saving to file: bayesopt/bayesopt_0.048924.mat
|  446 | Accept |    0.048924 |      8.1577 |    0.032756 |    0.033718 |           25 |            2 |      0.14198 |    0.0063272 |           17 |         relu |     0.092007 |           32 |         relu |      0.17357 |           59 |         none |      0.14208 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          31              2             3.6105             0.0099859        19     relu    0.089167    46     relu    0.46576    30     none    0.41064

ht_array length: 1lstm
    19    46

    "lstm"    "lstm"

    "relu"    "relu"

    0.0892    0.4658

Type: lstm Hidden size: 19 Activation: relu Dropout: 0.089167
Type: lstm Hidden size: 46 Activation: relu Dropout: 0.465761
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 19 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 46 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             47% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 19 ME: 46.000000 MSE: 0.045022
SL: 2.830416e-03 HS: Saving to file: bayesopt/bayesopt_0.045022.mat
|  447 | Accept |    0.045022 |       7.947 |    0.032756 |     0.03562 |           31 |            2 |       3.6105 |    0.0099859 |           19 |         relu |     0.089167 |           46 |         relu |      0.46576 |           30 |         none |      0.41064 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          22              3             1.1328              0.049378        59     relu    0.39426     8     none    0.036722    99     relu    0.11925

ht_array length: 1lstm
    59     8    99

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3943    0.0367    0.1193

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.394257
Type: lstm Hidden size: 8 Activation: none Dropout: 0.036722
Type: lstm Hidden size: 99 Activation: relu Dropout: 0.119250
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             4% dropout
     7   ''   LSTM                LSTM with 99 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             12% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 8.000000 MSE: 99.000000
SL: 1.823150e-01 HS: 4.523029e-02 ME: Saving to file: bayesopt/bayesopt_0.18232.mat
|  448 | Accept |     0.18232 |      16.105 |    0.032756 |    0.034416 |           22 |            3 |       1.1328 |     0.049378 |           59 |         relu |      0.39426 |            8 |         none |     0.036722 |           99 |         relu |      0.11925 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          48              2             0.34538            0.0026561        81     relu    0.12776    70     none    0.27675    85     tanh    0.46736

ht_array length: 1lstm
    81    70

    "lstm"    "lstm"

    "relu"    "none"

    0.1278    0.2767

Type: lstm Hidden size: 81 Activation: relu Dropout: 0.127765
Type: lstm Hidden size: 70 Activation: none Dropout: 0.276750
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 81 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 81 ME: 70.000000 MSE: 0.052010
SL: 3.959873e-03 HS: Saving to file: bayesopt/bayesopt_0.05201.mat
|  449 | Accept |     0.05201 |      11.355 |    0.032756 |    0.032793 |           48 |            2 |      0.34538 |    0.0026561 |           81 |         relu |      0.12776 |           70 |         none |      0.27675 |           85 |         tanh |      0.46736 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          28              2             0.60469            0.0019768        77     relu    0.43636    75     none    0.48116    48     relu    0.35104

ht_array length: 1lstm
    77    75

    "lstm"    "lstm"

    "relu"    "none"

    0.4364    0.4812

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.436364
Type: lstm Hidden size: 75 Activation: none Dropout: 0.481162
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 75 hidden units
     6   ''   Dropout             48% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 75.000000 MSE: 0.055333
SL: 4.434115e-03 HS: Saving to file: bayesopt/bayesopt_0.055333.mat
|  450 | Accept |    0.055333 |      12.217 |    0.032756 |    0.038191 |           28 |            2 |      0.60469 |    0.0019768 |           77 |         relu |      0.43636 |           75 |         none |      0.48116 |           48 |         relu |      0.35104 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          28              3             0.20965            0.0066875        82     relu    0.31206    85     none    0.38667     5     relu    0.080059

ht_array length: 1lstm
    82    85     5

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3121    0.3867    0.0801

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.312061
Type: lstm Hidden size: 85 Activation: none Dropout: 0.386667
Type: lstm Hidden size: 5 Activation: relu Dropout: 0.080059
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 85 hidden units
     6   ''   Dropout             39% dropout
     7   ''   LSTM                LSTM with 5 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             8% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 85.000000 MSE: 5.000000
SL: 5.991733e-02 HS: 5.465080e-03 ME: Saving to file: bayesopt/bayesopt_0.059917.mat
|  451 | Accept |    0.059917 |      15.323 |    0.032756 |    0.037355 |           28 |            3 |      0.20965 |    0.0066875 |           82 |         relu |      0.31206 |           85 |         none |      0.38667 |            5 |         relu |     0.080059 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          29              2             3.6508             0.0036758        12     relu    0.075208    11     relu    0.42689    53     none    0.05133

ht_array length: 1lstm
    12    11

    "lstm"    "lstm"

    "relu"    "relu"

    0.0752    0.4269

Type: lstm Hidden size: 12 Activation: relu Dropout: 0.075208
Type: lstm Hidden size: 11 Activation: relu Dropout: 0.426891
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 12 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             43% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 12 ME: 11.000000 MSE: 0.067220
SL: 6.743747e-03 HS: Saving to file: bayesopt/bayesopt_0.06722.mat
|  452 | Accept |     0.06722 |      7.0984 |    0.032756 |    0.032898 |           29 |            2 |       3.6508 |    0.0036758 |           12 |         relu |     0.075208 |           11 |         relu |      0.42689 |           53 |         none |      0.05133 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          34              3             0.27708            0.0013468        96     relu    0.025594    26     none    0.31652    79     relu    0.33964

ht_array length: 1lstm
    96    26    79

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.0256    0.3165    0.3396

Type: lstm Hidden size: 96 Activation: relu Dropout: 0.025594
Type: lstm Hidden size: 26 Activation: none Dropout: 0.316519
Type: lstm Hidden size: 79 Activation: relu Dropout: 0.339637
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 96 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 26 hidden units
     6   ''   Dropout             32% dropout
     7   ''   LSTM                LSTM with 79 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             34% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 26.000000 MSE: 79.000000
SL: 1.841795e-01 HS: 4.611415e-02 ME: Saving to file: bayesopt/bayesopt_0.18418.mat
|  453 | Accept |     0.18418 |       16.17 |    0.032756 |    0.032773 |           34 |            3 |      0.27708 |    0.0013468 |           96 |         relu |     0.025594 |           26 |         none |      0.31652 |           79 |         relu |      0.33964 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _____

          44              2             3.1581              0.002862        61     relu    0.41855    80     tanh    0.14791    29     none    0.138

ht_array length: 1lstm
    61    80

    "lstm"    "lstm"

    "relu"    "tanh"

    0.4185    0.1479

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.418548
Type: lstm Hidden size: 80 Activation: tanh Dropout: 0.147907
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 80 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             15% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 80.000000 MSE: 0.058060
SL: 4.691750e-03 HS: Saving to file: bayesopt/bayesopt_0.05806.mat
|  454 | Accept |     0.05806 |      10.444 |    0.032756 |    0.032792 |           44 |            2 |       3.1581 |     0.002862 |           61 |         relu |      0.41855 |           80 |         tanh |      0.14791 |           29 |         none |        0.138 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2       hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    __________    ___    ____    _______

          46              3             0.14022             0.01983         90     relu    0.45901    10     none    0.00078813    81     relu    0.44906

ht_array length: 1lstm
    90    10    81

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4590    0.0008    0.4491

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.459013
Type: lstm Hidden size: 10 Activation: none Dropout: 0.000788
Type: lstm Hidden size: 81 Activation: relu Dropout: 0.449064
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             46% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   Dropout             0% dropout
     7   ''   LSTM                LSTM with 81 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             45% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 10.000000 MSE: 81.000000
SL: 1.878280e-01 HS: 4.775937e-02 ME: Saving to file: bayesopt/bayesopt_0.18783.mat
|  455 | Accept |     0.18783 |      13.408 |    0.032756 |    0.041921 |           46 |            3 |      0.14022 |      0.01983 |           90 |         relu |      0.45901 |           10 |         none |   0.00078813 |           81 |         relu |      0.44906 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2       hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    __________    ___    ____    ________

          33              2             4.6946              0.020257        12     relu    0.41647    26     none    0.00028104    49     none    0.083412

ht_array length: 1lstm
    12    26

    "lstm"    "lstm"

    "relu"    "none"

    0.4165    0.0003

Type: lstm Hidden size: 12 Activation: relu Dropout: 0.416473
Type: lstm Hidden size: 26 Activation: none Dropout: 0.000281
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 12 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 26 hidden units
     6   ''   Dropout             0% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 12 ME: 26.000000 MSE: 0.047469
SL: 3.413156e-03 HS: Saving to file: bayesopt/bayesopt_0.047469.mat
|  456 | Accept |    0.047469 |      6.7176 |    0.032756 |    0.032793 |           33 |            2 |       4.6946 |     0.020257 |           12 |         relu |      0.41647 |           26 |         none |   0.00028104 |           49 |         none |     0.083412 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          47              3             0.63884            0.0057656        53     relu    0.023349    31     none    0.33159    16     none    0.15792

ht_array length: 1lstm
    53    31    16

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.0233    0.3316    0.1579

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.023349
Type: lstm Hidden size: 31 Activation: none Dropout: 0.331587
Type: lstm Hidden size: 16 Activation: none Dropout: 0.157919
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             33% dropout
     7   ''   LSTM                LSTM with 16 hidden units
     8   ''   Dropout             16% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 31.000000 MSE: 16.000000
SL: 5.647582e-02 HS: 4.876562e-03 ME: Saving to file: bayesopt/bayesopt_0.056476.mat
|  457 | Accept |    0.056476 |      10.157 |    0.032756 |    0.032772 |           47 |            3 |      0.63884 |    0.0057656 |           53 |         relu |     0.023349 |           31 |         none |      0.33159 |           16 |         none |      0.15792 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          29              3             0.47512             0.014287        68     relu    0.26311    68     none    0.38821    94     relu    0.046435

ht_array length: 1lstm
    68    68    94

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2631    0.3882    0.0464

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.263106
Type: lstm Hidden size: 68 Activation: none Dropout: 0.388207
Type: lstm Hidden size: 94 Activation: relu Dropout: 0.046435
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   Dropout             39% dropout
     7   ''   LSTM                LSTM with 94 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             5% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 68.000000 MSE: 94.000000
SL: 5.449413e-02 HS: 4.425856e-03 ME: Saving to file: bayesopt/bayesopt_0.054494.mat
|  458 | Accept |    0.054494 |      17.825 |    0.032756 |    0.038547 |           29 |            3 |      0.47512 |     0.014287 |           68 |         relu |      0.26311 |           68 |         none |      0.38821 |           94 |         relu |     0.046435 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          28              2             0.44369            0.0018036        89     relu    0.098769    32     none    0.34558    25     tanh    0.44748

ht_array length: 1lstm
    89    32

    "lstm"    "lstm"

    "relu"    "none"

    0.0988    0.3456

Type: lstm Hidden size: 89 Activation: relu Dropout: 0.098769
Type: lstm Hidden size: 32 Activation: none Dropout: 0.345576
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 89 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 32 hidden units
     6   ''   Dropout             35% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 89 ME: 32.000000 MSE: 0.047370
SL: 3.025565e-03 HS: Saving to file: bayesopt/bayesopt_0.04737.mat
|  459 | Accept |     0.04737 |      10.488 |    0.032756 |    0.034246 |           28 |            2 |      0.44369 |    0.0018036 |           89 |         relu |     0.098769 |           32 |         none |      0.34558 |           25 |         tanh |      0.44748 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          49              2             0.59724            0.0044366        99     relu    0.14595    74     tanh    0.1012    69     none    0.30951

ht_array length: 1lstm
    99    74

    "lstm"    "lstm"

    "relu"    "tanh"

    0.1460    0.1012

Type: lstm Hidden size: 99 Activation: relu Dropout: 0.145952
Type: lstm Hidden size: 74 Activation: tanh Dropout: 0.101197
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 99 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 74 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             10% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 99 ME: 74.000000 MSE: 0.047945
SL: 3.762706e-03 HS: Saving to file: bayesopt/bayesopt_0.047945.mat
|  460 | Accept |    0.047945 |      11.496 |    0.032756 |    0.032842 |           49 |            2 |      0.59724 |    0.0044366 |           99 |         relu |      0.14595 |           74 |         tanh |       0.1012 |           69 |         none |      0.30951 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1    hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _____    ___    ____    ________    ___    ____    ________

          22              2             0.4241             0.0014288        81     relu    0.402    78     none    0.059286    33     relu    0.072669

ht_array length: 1lstm
    81    78

    "lstm"    "lstm"

    "relu"    "none"

    0.4020    0.0593

Type: lstm Hidden size: 81 Activation: relu Dropout: 0.401997
Type: lstm Hidden size: 78 Activation: none Dropout: 0.059286
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 81 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 78 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 81 ME: 78.000000 MSE: 0.046998
SL: 3.693815e-03 HS: Saving to file: bayesopt/bayesopt_0.046998.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  461 | Accept |    0.046998 |       14.13 |    0.032756 |    0.032824 |           22 |            2 |       0.4241 |    0.0014288 |           81 |         relu |        0.402 |           78 |         none |     0.059286 |           33 |         relu |     0.072669 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          37              3             4.8478              0.010426         9     relu    0.034083    22     relu    0.46855    28     none    0.08971

ht_array length: 1lstm
     9    22    28

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "none"

    0.0341    0.4686    0.0897

Type: lstm Hidden size: 9 Activation: relu Dropout: 0.034083
Type: lstm Hidden size: 22 Activation: relu Dropout: 0.468553
Type: lstm Hidden size: 28 Activation: none Dropout: 0.089710
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 9 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 22 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             47% dropout
     8   ''   LSTM                LSTM with 28 hidden units
     9   ''   Dropout             9% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 9 ME: 22.000000 MSE: 28.000000
SL: 7.169500e-02 HS: 7.892345e-03 ME: Saving to file: bayesopt/bayesopt_0.071695.mat
|  462 | Accept |    0.071695 |      8.6277 |    0.032756 |    0.032773 |           37 |            3 |       4.8478 |     0.010426 |            9 |         relu |     0.034083 |           22 |         relu |      0.46855 |           28 |         none |      0.08971 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          9               2             3.8901              0.066378        72     relu    0.071414     5     none    0.22902    98     tanh    0.34077

ht_array length: 1lstm
    72     5

    "lstm"    "lstm"

    "relu"    "none"

    0.0714    0.2290

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.071414
Type: lstm Hidden size: 5 Activation: none Dropout: 0.229023
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 5 hidden units
     6   ''   Dropout             23% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 5.000000 MSE: 0.180883
SL: 4.512478e-02 HS: Saving to file: bayesopt/bayesopt_0.18088.mat
|  463 | Accept |     0.18088 |      17.532 |    0.032756 |    0.032801 |            9 |            2 |       3.8901 |     0.066378 |           72 |         relu |     0.071414 |            5 |         none |      0.22902 |           98 |         tanh |      0.34077 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          46              2             0.86932            0.0011281        34     relu    0.16083    42     none    0.29657    89     relu    0.1318

ht_array length: 1lstm
    34    42

    "lstm"    "lstm"

    "relu"    "none"

    0.1608    0.2966

Type: lstm Hidden size: 34 Activation: relu Dropout: 0.160825
Type: lstm Hidden size: 42 Activation: none Dropout: 0.296568
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 34 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 34 ME: 42.000000 MSE: 0.055074
SL: 4.822930e-03 HS: Saving to file: bayesopt/bayesopt_0.055074.mat
|  464 | Accept |    0.055074 |      7.1681 |    0.032756 |    0.032826 |           46 |            2 |      0.86932 |    0.0011281 |           34 |         relu |      0.16083 |           42 |         none |      0.29657 |           89 |         relu |       0.1318 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          49              3             3.9443             0.0073765        25     relu    0.2476    10     none    0.046751    68     relu    0.49753

ht_array length: 1lstm
    25    10    68

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2476    0.0468    0.4975

Type: lstm Hidden size: 25 Activation: relu Dropout: 0.247597
Type: lstm Hidden size: 10 Activation: none Dropout: 0.046751
Type: lstm Hidden size: 68 Activation: relu Dropout: 0.497532
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 25 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   Dropout             5% dropout
     7   ''   LSTM                LSTM with 68 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             50% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 25 ME: 10.000000 MSE: 68.000000
SL: 7.172737e-02 HS: 7.560105e-03 ME: Saving to file: bayesopt/bayesopt_0.071727.mat
|  465 | Accept |    0.071727 |      8.8793 |    0.032756 |    0.032808 |           49 |            3 |       3.9443 |    0.0073765 |           25 |         relu |       0.2476 |           10 |         none |     0.046751 |           68 |         relu |      0.49753 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          35              3             3.3647              0.021307        31     relu    0.46352    15     none    0.28501    12     relu    0.023228

ht_array length: 1lstm
    31    15    12

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4635    0.2850    0.0232

Type: lstm Hidden size: 31 Activation: relu Dropout: 0.463521
Type: lstm Hidden size: 15 Activation: none Dropout: 0.285009
Type: lstm Hidden size: 12 Activation: relu Dropout: 0.023228
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 31 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             46% dropout
     5   ''   LSTM                LSTM with 15 hidden units
     6   ''   Dropout             29% dropout
     7   ''   LSTM                LSTM with 12 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             2% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 31 ME: 15.000000 MSE: 12.000000
SL: 5.862717e-02 HS: 5.383569e-03 ME: Saving to file: bayesopt/bayesopt_0.058627.mat
|  466 | Accept |    0.058627 |      8.5621 |    0.032756 |    0.038376 |           35 |            3 |       3.3647 |     0.021307 |           31 |         relu |      0.46352 |           15 |         none |      0.28501 |           12 |         relu |     0.023228 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          47              2             1.4971              0.013722        69     relu    0.32147    11     tanh    0.013679    60     none    0.47038

ht_array length: 1lstm
    69    11

    "lstm"    "lstm"

    "relu"    "tanh"

    0.3215    0.0137

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.321466
Type: lstm Hidden size: 11 Activation: tanh Dropout: 0.013679
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             1% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 11.000000 MSE: 0.052183
SL: 4.457368e-03 HS: Saving to file: bayesopt/bayesopt_0.052183.mat
|  467 | Accept |    0.052183 |      8.3488 |    0.032756 |    0.033044 |           47 |            2 |       1.4971 |     0.013722 |           69 |         relu |      0.32147 |           11 |         tanh |     0.013679 |           60 |         none |      0.47038 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          27              2             0.10003            0.0010025        46     relu    0.1982    98     none    0.25344    35     relu    0.44646

ht_array length: 1lstm
    46    98

    "lstm"    "lstm"

    "relu"    "none"

    0.1982    0.2534

Type: lstm Hidden size: 46 Activation: relu Dropout: 0.198196
Type: lstm Hidden size: 98 Activation: none Dropout: 0.253440
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 46 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   Dropout             25% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 46 ME: 98.000000 MSE: 0.054143
SL: 4.363320e-03 HS: Saving to file: bayesopt/bayesopt_0.054143.mat
|  468 | Accept |    0.054143 |      13.046 |    0.032756 |    0.036065 |           27 |            2 |      0.10003 |    0.0010025 |           46 |         relu |       0.1982 |           98 |         none |      0.25344 |           35 |         relu |      0.44646 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          43              3             3.4344             0.0056462        63     relu    0.4466    27     none    0.23241    84     relu    0.46658

ht_array length: 1lstm
    63    27    84

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4466    0.2324    0.4666

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.446605
Type: lstm Hidden size: 27 Activation: none Dropout: 0.232414
Type: lstm Hidden size: 84 Activation: relu Dropout: 0.466577
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 27 hidden units
     6   ''   Dropout             23% dropout
     7   ''   LSTM                LSTM with 84 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             47% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 27.000000 MSE: 84.000000
SL: 4.760748e-02 HS: 3.115645e-03 ME: Saving to file: bayesopt/bayesopt_0.047607.mat
|  469 | Accept |    0.047607 |      11.635 |    0.032756 |    0.032798 |           43 |            3 |       3.4344 |    0.0056462 |           63 |         relu |       0.4466 |           27 |         none |      0.23241 |           84 |         relu |      0.46658 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          24              3             1.7267             0.0056793        79     relu    0.18848     9     none    0.18504    75     relu    0.054338

ht_array length: 1lstm
    79     9    75

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1885    0.1850    0.0543

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.188482
Type: lstm Hidden size: 9 Activation: none Dropout: 0.185037
Type: lstm Hidden size: 75 Activation: relu Dropout: 0.054338
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 9 hidden units
     6   ''   Dropout             19% dropout
     7   ''   LSTM                LSTM with 75 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             5% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 9.000000 MSE: 75.000000
SL: 4.206202e-02 HS: 2.434255e-03 ME: Saving to file: bayesopt/bayesopt_0.042062.mat
|  470 | Accept |    0.042062 |      14.812 |    0.032756 |    0.036232 |           24 |            3 |       1.7267 |    0.0056793 |           79 |         relu |      0.18848 |            9 |         none |      0.18504 |           75 |         relu |     0.054338 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _____

          34              2             0.26109             0.055115        10     relu    0.34497    61     none    0.39031    31     relu    0.456

ht_array length: 1lstm
    10    61

    "lstm"    "lstm"

    "relu"    "none"

    0.3450    0.3903

Type: lstm Hidden size: 10 Activation: relu Dropout: 0.344968
Type: lstm Hidden size: 61 Activation: none Dropout: 0.390315
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 10 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 61 hidden units
     6   ''   Dropout             39% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 10 ME: 61.000000 MSE: 0.052836
SL: 4.127615e-03 HS: Saving to file: bayesopt/bayesopt_0.052836.mat
|  471 | Accept |    0.052836 |      8.0522 |    0.032756 |    0.037056 |           34 |            2 |      0.26109 |     0.055115 |           10 |         relu |      0.34497 |           61 |         none |      0.39031 |           31 |         relu |        0.456 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          49              2             2.8285              0.011597        36     relu    0.11909     6     none    0.19159     7     tanh    0.13619

ht_array length: 1lstm
    36     6

    "lstm"    "lstm"

    "relu"    "none"

    0.1191    0.1916

Type: lstm Hidden size: 36 Activation: relu Dropout: 0.119089
Type: lstm Hidden size: 6 Activation: none Dropout: 0.191592
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 36 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             19% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 36 ME: 6.000000 MSE: 0.062024
SL: 5.689256e-03 HS: Saving to file: bayesopt/bayesopt_0.062024.mat
|  472 | Accept |    0.062024 |      5.2942 |    0.032756 |    0.032782 |           49 |            2 |       2.8285 |     0.011597 |           36 |         relu |      0.11909 |            6 |         none |      0.19159 |            7 |         tanh |      0.13619 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          45              2             4.7657             0.0020675        97     relu    0.21098     8     none    0.062238    27     relu    0.13567

ht_array length: 1lstm
    97     8

    "lstm"    "lstm"

    "relu"    "none"

    0.2110    0.0622

Type: lstm Hidden size: 97 Activation: relu Dropout: 0.210984
Type: lstm Hidden size: 8 Activation: none Dropout: 0.062238
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 97 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 97 ME: 8.000000 MSE: 0.060190
SL: 5.615391e-03 HS: Saving to file: bayesopt/bayesopt_0.06019.mat
|  473 | Accept |     0.06019 |      8.6754 |    0.032756 |    0.035816 |           45 |            2 |       4.7657 |    0.0020675 |           97 |         relu |      0.21098 |            8 |         none |     0.062238 |           27 |         relu |      0.13567 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          43              3             3.7753             0.0066206        37     relu    0.017322    68     none    0.23887     5     relu    0.12195

ht_array length: 1lstm
    37    68     5

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.0173    0.2389    0.1219

Type: lstm Hidden size: 37 Activation: relu Dropout: 0.017322
Type: lstm Hidden size: 68 Activation: none Dropout: 0.238867
Type: lstm Hidden size: 5 Activation: relu Dropout: 0.121946
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 37 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   Dropout             24% dropout
     7   ''   LSTM                LSTM with 5 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             12% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 37 ME: 68.000000 MSE: 5.000000
SL: 6.890123e-02 HS: 7.576551e-03 ME: Saving to file: bayesopt/bayesopt_0.068901.mat
|  474 | Accept |    0.068901 |       10.01 |    0.032756 |    0.036231 |           43 |            3 |       3.7753 |    0.0066206 |           37 |         relu |     0.017322 |           68 |         none |      0.23887 |            5 |         relu |      0.12195 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          21              2             4.5025             0.0086809        52     relu    0.11934    87     tanh    0.082688    31     none    0.066435

ht_array length: 1lstm
    52    87

    "lstm"    "lstm"

    "relu"    "tanh"

    0.1193    0.0827

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.119337
Type: lstm Hidden size: 87 Activation: tanh Dropout: 0.082688
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 87 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             8% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 87.000000 MSE: 0.053575
SL: 4.350071e-03 HS: Saving to file: bayesopt/bayesopt_0.053575.mat
|  475 | Accept |    0.053575 |      13.776 |    0.032756 |    0.032804 |           21 |            2 |       4.5025 |    0.0086809 |           52 |         relu |      0.11934 |           87 |         tanh |     0.082688 |           31 |         none |     0.066435 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          14              2             0.57087             0.012606        20     relu    0.32709     3     relu    0.07982     7     none    0.49626

ht_array length: 1lstm
    20     3

    "lstm"    "lstm"

    "relu"    "relu"

    0.3271    0.0798

Type: lstm Hidden size: 20 Activation: relu Dropout: 0.327086
Type: lstm Hidden size: 3 Activation: relu Dropout: 0.079820
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 20 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 3 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             8% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 20 ME: 3.000000 MSE: 0.069770
SL: 7.000660e-03 HS: Saving to file: bayesopt/bayesopt_0.06977.mat
|  476 | Accept |     0.06977 |      10.988 |    0.032756 |    0.032842 |           14 |            2 |      0.57087 |     0.012606 |           20 |         relu |      0.32709 |            3 |         relu |      0.07982 |            7 |         none |      0.49626 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          42              3             0.70506             0.01363         41     relu    0.014788    18     none    0.028051    53     none    0.43142

ht_array length: 1lstm
    41    18    53

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.0148    0.0281    0.4314

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.014788
Type: lstm Hidden size: 18 Activation: none Dropout: 0.028051
Type: lstm Hidden size: 53 Activation: none Dropout: 0.431419
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 18 hidden units
     6   ''   Dropout             3% dropout
     7   ''   LSTM                LSTM with 53 hidden units
     8   ''   Dropout             43% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 18.000000 MSE: 53.000000
SL: 4.632326e-02 HS: 3.194503e-03 ME: Saving to file: bayesopt/bayesopt_0.046323.mat
|  477 | Accept |    0.046323 |      9.4827 |    0.032756 |    0.036849 |           42 |            3 |      0.70506 |      0.01363 |           41 |         relu |     0.014788 |           18 |         none |     0.028051 |           53 |         none |      0.43142 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          36              2             2.1764              0.012446        57     relu    0.32522    49     relu    0.18544    20     none    0.19567

ht_array length: 1lstm
    57    49

    "lstm"    "lstm"

    "relu"    "relu"

    0.3252    0.1854

Type: lstm Hidden size: 57 Activation: relu Dropout: 0.325220
Type: lstm Hidden size: 49 Activation: relu Dropout: 0.185439
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 57 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             19% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 57 ME: 49.000000 MSE: 0.053599
SL: 4.205336e-03 HS: Saving to file: bayesopt/bayesopt_0.053599.mat
|  478 | Accept |    0.053599 |      8.6942 |    0.032756 |     0.03392 |           36 |            2 |       2.1764 |     0.012446 |           57 |         relu |      0.32522 |           49 |         relu |      0.18544 |           20 |         none |      0.19567 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          22              3             0.61162            0.0064855        65     relu    0.36319    89     none    0.48598    96     relu    0.37896

ht_array length: 1lstm
    65    89    96

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3632    0.4860    0.3790

Type: lstm Hidden size: 65 Activation: relu Dropout: 0.363189
Type: lstm Hidden size: 89 Activation: none Dropout: 0.485978
Type: lstm Hidden size: 96 Activation: relu Dropout: 0.378959
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 65 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 89 hidden units
     6   ''   Dropout             49% dropout
     7   ''   LSTM                LSTM with 96 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             38% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 65 ME: 89.000000 MSE: 96.000000
SL: 7.624952e-02 HS: 1.008196e-02 ME: Saving to file: bayesopt/bayesopt_0.07625.mat
|  479 | Accept |     0.07625 |      24.078 |    0.032756 |    0.035418 |           22 |            3 |      0.61162 |    0.0064855 |           65 |         relu |      0.36319 |           89 |         none |      0.48598 |           96 |         relu |      0.37896 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ________

          33              3             0.39959             0.012892        93     relu    0.4049    90     none    0.08817     7     none    0.033295

ht_array length: 1lstm
    93    90     7

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.4049    0.0882    0.0333

Type: lstm Hidden size: 93 Activation: relu Dropout: 0.404900
Type: lstm Hidden size: 90 Activation: none Dropout: 0.088170
Type: lstm Hidden size: 7 Activation: none Dropout: 0.033295
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 93 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 90 hidden units
     6   ''   Dropout             9% dropout
     7   ''   LSTM                LSTM with 7 hidden units
     8   ''   Dropout             3% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 93 ME: 90.000000 MSE: 7.000000
SL: 4.431842e-02 HS: 2.995634e-03 ME: Saving to file: bayesopt/bayesopt_0.044318.mat
|  480 | Accept |    0.044318 |      16.892 |    0.032756 |    0.032885 |           33 |            3 |      0.39959 |     0.012892 |           93 |         relu |       0.4049 |           90 |         none |      0.08817 |            7 |         none |     0.033295 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          35              2             0.14916            0.0013219        70     relu    0.19525    49     none    0.14479    15     relu    0.19901

ht_array length: 1lstm
    70    49

    "lstm"    "lstm"

    "relu"    "none"

    0.1953    0.1448

Type: lstm Hidden size: 70 Activation: relu Dropout: 0.195253
Type: lstm Hidden size: 49 Activation: none Dropout: 0.144792
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 70 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   Dropout             14% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 70 ME: 49.000000 MSE: 0.044911
SL: 2.957393e-03 HS: Saving to file: bayesopt/bayesopt_0.044911.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  481 | Accept |    0.044911 |      9.1561 |    0.032756 |    0.035239 |           35 |            2 |      0.14916 |    0.0013219 |           70 |         relu |      0.19525 |           49 |         none |      0.14479 |           15 |         relu |      0.19901 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          41              2             3.8453              0.014981        95     relu    0.43244    42     none    0.49088    73     tanh    0.049393

ht_array length: 1lstm
    95    42

    "lstm"    "lstm"

    "relu"    "none"

    0.4324    0.4909

Type: lstm Hidden size: 95 Activation: relu Dropout: 0.432438
Type: lstm Hidden size: 42 Activation: none Dropout: 0.490876
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 95 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             49% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 95 ME: 42.000000 MSE: 0.047246
SL: 3.212265e-03 HS: Saving to file: bayesopt/bayesopt_0.047246.mat
|  482 | Accept |    0.047246 |       10.43 |    0.032756 |    0.036715 |           41 |            2 |       3.8453 |     0.014981 |           95 |         relu |      0.43244 |           42 |         none |      0.49088 |           73 |         tanh |     0.049393 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          43              3             1.0293              0.012746        54     relu    0.45555    84     none    0.38442    48     relu    0.23173

ht_array length: 1lstm
    54    84    48

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4556    0.3844    0.2317

Type: lstm Hidden size: 54 Activation: relu Dropout: 0.455554
Type: lstm Hidden size: 84 Activation: none Dropout: 0.384417
Type: lstm Hidden size: 48 Activation: relu Dropout: 0.231726
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 54 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             46% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   Dropout             38% dropout
     7   ''   LSTM                LSTM with 48 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             23% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 54 ME: 84.000000 MSE: 48.000000
SL: 6.707642e-02 HS: 6.477567e-03 ME: Saving to file: bayesopt/bayesopt_0.067076.mat
|  483 | Accept |    0.067076 |      14.026 |    0.032756 |    0.033744 |           43 |            3 |       1.0293 |     0.012746 |           54 |         relu |      0.45555 |           84 |         none |      0.38442 |           48 |         relu |      0.23173 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          40              2             3.1684              0.014619        90     relu    0.054915    27     none    0.49537    18     tanh    0.44264

ht_array length: 1lstm
    90    27

    "lstm"    "lstm"

    "relu"    "none"

    0.0549    0.4954

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.054915
Type: lstm Hidden size: 27 Activation: none Dropout: 0.495370
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 27 hidden units
     6   ''   Dropout             50% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 27.000000 MSE: 0.046480
SL: 3.135193e-03 HS: Saving to file: bayesopt/bayesopt_0.04648.mat
|  484 | Accept |     0.04648 |      10.283 |    0.032756 |    0.038208 |           40 |            2 |       3.1684 |     0.014619 |           90 |         relu |     0.054915 |           27 |         none |      0.49537 |           18 |         tanh |      0.44264 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          36              3             2.9683              0.016565        33     relu    0.090187    86     none    0.14339    89     none    0.40704

ht_array length: 1lstm
    33    86    89

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.0902    0.1434    0.4070

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.090187
Type: lstm Hidden size: 86 Activation: none Dropout: 0.143387
Type: lstm Hidden size: 89 Activation: none Dropout: 0.407041
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   Dropout             14% dropout
     7   ''   LSTM                LSTM with 89 hidden units
     8   ''   Dropout             41% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 86.000000 MSE: 89.000000
SL: 5.141442e-02 HS: 3.949536e-03 ME: Saving to file: bayesopt/bayesopt_0.051414.mat
|  485 | Accept |    0.051414 |      15.723 |    0.032756 |    0.042541 |           36 |            3 |       2.9683 |     0.016565 |           33 |         relu |     0.090187 |           86 |         none |      0.14339 |           89 |         none |      0.40704 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          24              2             3.2094             0.0021103        53     relu    0.48501    77     none    0.099018    58     relu    0.39843

ht_array length: 1lstm
    53    77

    "lstm"    "lstm"

    "relu"    "none"

    0.4850    0.0990

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.485006
Type: lstm Hidden size: 77 Activation: none Dropout: 0.099018
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 77.000000 MSE: 0.049304
SL: 3.524657e-03 HS: Saving to file: bayesopt/bayesopt_0.049304.mat
|  486 | Accept |    0.049304 |      12.715 |    0.032756 |    0.037014 |           24 |            2 |       3.2094 |    0.0021103 |           53 |         relu |      0.48501 |           77 |         none |     0.099018 |           58 |         relu |      0.39843 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          19              3              4.638              0.013986        22     relu    0.33323    60     none    0.12032    100    relu    0.33111

ht_array length: 1lstm
    22    60   100

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.3332    0.1203    0.3311

Type: lstm Hidden size: 22 Activation: relu Dropout: 0.333230
Type: lstm Hidden size: 60 Activation: none Dropout: 0.120319
Type: lstm Hidden size: 100 Activation: relu Dropout: 0.331114
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 22 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Dropout             12% dropout
     7   ''   LSTM                LSTM with 100 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             33% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 22 ME: 60.000000 MSE: 100.000000
SL: 6.007208e-02 HS: 5.645753e-03 ME: Saving to file: bayesopt/bayesopt_0.060072.mat
|  487 | Accept |    0.060072 |      18.077 |    0.032756 |    0.032812 |           19 |            3 |        4.638 |     0.013986 |           22 |         relu |      0.33323 |           60 |         none |      0.12032 |          100 |         relu |      0.33111 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ________

          41              3             2.4602              0.028141        57     relu    0.060419     6     none    0.031142    33     relu    0.059087

ht_array length: 1lstm
    57     6    33

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.0604    0.0311    0.0591

Type: lstm Hidden size: 57 Activation: relu Dropout: 0.060419
Type: lstm Hidden size: 6 Activation: none Dropout: 0.031142
Type: lstm Hidden size: 33 Activation: relu Dropout: 0.059087
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 57 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             3% dropout
     7   ''   LSTM                LSTM with 33 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             6% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 57 ME: 6.000000 MSE: 33.000000
SL: 5.257474e-02 HS: 4.026987e-03 ME: Saving to file: bayesopt/bayesopt_0.052575.mat
|  488 | Accept |    0.052575 |      9.4277 |    0.032756 |    0.036704 |           41 |            3 |       2.4602 |     0.028141 |           57 |         relu |     0.060419 |            6 |         none |     0.031142 |           33 |         relu |     0.059087 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          29              2             0.14205            0.0024647        57     relu    0.48484    99     none    0.13664     8     relu    0.34204

ht_array length: 1lstm
    57    99

    "lstm"    "lstm"

    "relu"    "none"

    0.4848    0.1366

Type: lstm Hidden size: 57 Activation: relu Dropout: 0.484835
Type: lstm Hidden size: 99 Activation: none Dropout: 0.136637
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 57 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 99 hidden units
     6   ''   Dropout             14% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 57 ME: 99.000000 MSE: 0.054414
SL: 4.890348e-03 HS: Saving to file: bayesopt/bayesopt_0.054414.mat
|  489 | Accept |    0.054414 |      13.094 |    0.032756 |    0.032786 |           29 |            2 |      0.14205 |    0.0024647 |           57 |         relu |      0.48484 |           99 |         none |      0.13664 |            8 |         relu |      0.34204 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          39              3             1.1209              0.011647         1     relu    0.03075    77     none    0.21703    89     none    0.11643

ht_array length: 1lstm
     1    77    89

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "none"

    0.0308    0.2170    0.1164

Type: lstm Hidden size: 1 Activation: relu Dropout: 0.030750
Type: lstm Hidden size: 77 Activation: none Dropout: 0.217026
Type: lstm Hidden size: 89 Activation: none Dropout: 0.116428
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 1 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   Dropout             22% dropout
     7   ''   LSTM                LSTM with 89 hidden units
     8   ''   Dropout             12% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 1 ME: 77.000000 MSE: 89.000000
SL: 5.631778e-02 HS: 4.621026e-03 ME: Saving to file: bayesopt/bayesopt_0.056318.mat
|  490 | Accept |    0.056318 |      14.661 |    0.032756 |    0.032804 |           39 |            3 |       1.1209 |     0.011647 |            1 |         relu |      0.03075 |           77 |         none |      0.21703 |           89 |         none |      0.11643 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ________

          45              3             2.2558             0.0043071        72     relu    0.1342     2     none    0.42891    77     relu    0.049142

ht_array length: 1lstm
    72     2    77

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.1342    0.4289    0.0491

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.134199
Type: lstm Hidden size: 2 Activation: none Dropout: 0.428909
Type: lstm Hidden size: 77 Activation: relu Dropout: 0.049142
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 2 hidden units
     6   ''   Dropout             43% dropout
     7   ''   LSTM                LSTM with 77 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             5% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 2.000000 MSE: 77.000000
SL: 8.685260e-02 HS: 1.075586e-02 ME: Saving to file: bayesopt/bayesopt_0.086853.mat
|  491 | Accept |    0.086853 |      11.848 |    0.032756 |    0.037227 |           45 |            3 |       2.2558 |    0.0043071 |           72 |         relu |       0.1342 |            2 |         none |      0.42891 |           77 |         relu |     0.049142 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ________

          34              2             3.5713             0.0089104        67     relu    0.4924    65     none    0.49524    93     tanh    0.053664

ht_array length: 1lstm
    67    65

    "lstm"    "lstm"

    "relu"    "none"

    0.4924    0.4952

Type: lstm Hidden size: 67 Activation: relu Dropout: 0.492400
Type: lstm Hidden size: 65 Activation: none Dropout: 0.495238
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 67 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 65 hidden units
     6   ''   Dropout             50% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 67 ME: 65.000000 MSE: 0.058130
SL: 4.647983e-03 HS: Saving to file: bayesopt/bayesopt_0.05813.mat
|  492 | Accept |     0.05813 |      10.827 |    0.032756 |    0.040058 |           34 |            2 |       3.5713 |    0.0089104 |           67 |         relu |       0.4924 |           65 |         none |      0.49524 |           93 |         tanh |     0.053664 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          46              3              3.838              0.012648        95     relu    0.46046    56     tanh    0.18716     6     none    0.41949

ht_array length: 1lstm
    95    56     6

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.4605    0.1872    0.4195

Type: lstm Hidden size: 95 Activation: relu Dropout: 0.460456
Type: lstm Hidden size: 56 Activation: tanh Dropout: 0.187161
Type: lstm Hidden size: 6 Activation: none Dropout: 0.419491
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 95 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             46% dropout
     5   ''   LSTM                LSTM with 56 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             19% dropout
     8   ''   LSTM                LSTM with 6 hidden units
     9   ''   Dropout             42% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 95 ME: 56.000000 MSE: 6.000000
SL: 6.570053e-02 HS: 7.055993e-03 ME: Saving to file: bayesopt/bayesopt_0.065701.mat
|  493 | Accept |    0.065701 |      13.988 |    0.032756 |    0.040316 |           46 |            3 |        3.838 |     0.012648 |           95 |         relu |      0.46046 |           56 |         tanh |      0.18716 |            6 |         none |      0.41949 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          33              2             3.3518              0.007713        64     relu    0.12735     6     none    0.12428    32     relu    0.024137

ht_array length: 1lstm
    64     6

    "lstm"    "lstm"

    "relu"    "none"

    0.1273    0.1243

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.127346
Type: lstm Hidden size: 6 Activation: none Dropout: 0.124285
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             12% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 6.000000 MSE: 0.046076
SL: 3.147645e-03 HS: Saving to file: bayesopt/bayesopt_0.046076.mat
|  494 | Accept |    0.046076 |      8.2287 |    0.032756 |    0.032835 |           33 |            2 |       3.3518 |     0.007713 |           64 |         relu |      0.12735 |            6 |         none |      0.12428 |           32 |         relu |     0.024137 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          48              3             1.1266             0.0034233        78     relu    0.29747     2     none    0.14486    74     relu    0.24016

ht_array length: 1lstm
    78     2    74

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2975    0.1449    0.2402

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.297467
Type: lstm Hidden size: 2 Activation: none Dropout: 0.144858
Type: lstm Hidden size: 74 Activation: relu Dropout: 0.240161
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 2 hidden units
     6   ''   Dropout             14% dropout
     7   ''   LSTM                LSTM with 74 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             24% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 2.000000 MSE: 74.000000
SL: 1.823824e-01 HS: 4.576500e-02 ME: Saving to file: bayesopt/bayesopt_0.18238.mat
|  495 | Accept |     0.18238 |      12.885 |    0.032756 |    0.043694 |           48 |            3 |       1.1266 |    0.0034233 |           78 |         relu |      0.29747 |            2 |         none |      0.14486 |           74 |         relu |      0.24016 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          26              3             0.36913            0.0049424        97     relu    0.25223    42     tanh    0.034007    86     none    0.38267

ht_array length: 1lstm
    97    42    86

    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"

    0.2522    0.0340    0.3827

Type: lstm Hidden size: 97 Activation: relu Dropout: 0.252225
Type: lstm Hidden size: 42 Activation: tanh Dropout: 0.034007
Type: lstm Hidden size: 86 Activation: none Dropout: 0.382668
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 97 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             3% dropout
     8   ''   LSTM                LSTM with 86 hidden units
     9   ''   Dropout             38% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 97 ME: 42.000000 MSE: 86.000000
SL: 4.685583e-02 HS: 3.160255e-03 ME: Saving to file: bayesopt/bayesopt_0.046856.mat
|  496 | Accept |    0.046856 |       20.22 |    0.032756 |    0.039597 |           26 |            3 |      0.36913 |    0.0049424 |           97 |         relu |      0.25223 |           42 |         tanh |     0.034007 |           86 |         none |      0.38267 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          48              3             4.2212              0.015247        72     relu    0.44705    87     none    0.19174    86     relu    0.46459

ht_array length: 1lstm
    72    87    86

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.4471    0.1917    0.4646

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.447053
Type: lstm Hidden size: 87 Activation: none Dropout: 0.191741
Type: lstm Hidden size: 86 Activation: relu Dropout: 0.464586
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 87 hidden units
     6   ''   Dropout             19% dropout
     7   ''   LSTM                LSTM with 86 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             46% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 87.000000 MSE: 86.000000
SL: 6.533507e-02 HS: 6.525504e-03 ME: Saving to file: bayesopt/bayesopt_0.065335.mat
|  497 | Accept |    0.065335 |       18.64 |    0.032756 |    0.040607 |           48 |            3 |       4.2212 |     0.015247 |           72 |         relu |      0.44705 |           87 |         none |      0.19174 |           86 |         relu |      0.46459 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          17              2             2.1629             0.0024983        90     relu    0.37151    34     relu    0.11886    96     none    0.28395

ht_array length: 1lstm
    90    34

    "lstm"    "lstm"

    "relu"    "relu"

    0.3715    0.1189

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.371511
Type: lstm Hidden size: 34 Activation: relu Dropout: 0.118856
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 34 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             12% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 34.000000 MSE: 0.051310
SL: 3.903688e-03 HS: Saving to file: bayesopt/bayesopt_0.05131.mat
|  498 | Accept |     0.05131 |      14.787 |    0.032756 |    0.035165 |           17 |            2 |       2.1629 |    0.0024983 |           90 |         relu |      0.37151 |           34 |         relu |      0.11886 |           96 |         none |      0.28395 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          14              2             0.56348             0.002657        16     relu    0.35994    47     none    0.48401    61     tanh    0.42402

ht_array length: 1lstm
    16    47

    "lstm"    "lstm"

    "relu"    "none"

    0.3599    0.4840

Type: lstm Hidden size: 16 Activation: relu Dropout: 0.359941
Type: lstm Hidden size: 47 Activation: none Dropout: 0.484007
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 16 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 47 hidden units
     6   ''   Dropout             48% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 16 ME: 47.000000 MSE: 0.058564
SL: 4.886608e-03 HS: Saving to file: bayesopt/bayesopt_0.058564.mat
|  499 | Accept |    0.058564 |       12.46 |    0.032756 |    0.032828 |           14 |            2 |      0.56348 |     0.002657 |           16 |         relu |      0.35994 |           47 |         none |      0.48401 |           61 |         tanh |      0.42402 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          20              3             0.11733             0.010668        62     relu    0.27372    62     none    0.39717    46     relu    0.0034582

ht_array length: 1lstm
    62    62    46

    "lstm"    "lstm"    "lstm"

    "relu"    "none"    "relu"

    0.2737    0.3972    0.0035

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.273716
Type: lstm Hidden size: 62 Activation: none Dropout: 0.397165
Type: lstm Hidden size: 46 Activation: relu Dropout: 0.003458
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 62 hidden units
     6   ''   Dropout             40% dropout
     7   ''   LSTM                LSTM with 46 hidden units
     8   ''   ReLU                ReLU
     9   ''   Dropout             0% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 62.000000 MSE: 46.000000
SL: 6.880757e-02 HS: 8.321146e-03 ME: Saving to file: bayesopt/bayesopt_0.068808.mat
|  500 | Accept |    0.068808 |      17.281 |    0.032756 |    0.033072 |           20 |            3 |      0.11733 |     0.010668 |           62 |         relu |      0.27372 |           62 |         none |      0.39717 |           46 |         relu |    0.0034582 |

__________________________________________________________
Optimization completed.
MaxObjectiveEvaluations of 500 reached.
Total function evaluations: 500
Total elapsed time: 8880.7878 seconds
Total objective function evaluation time: 5319.5015

Best observed feasible point:
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          42              2             3.1843              0.013868        33     relu    0.12686    55     none    0.10398    92     none    0.11626

Observed objective function value = 0.032756
Estimated objective function value = 0.048629
Function evaluation time = 8.0202

Best estimated feasible point (according to models):
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2      drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _________    ___    ____    ________

          39              2             3.9393             0.0056316        79     relu    0.039206    52     tanh    0.0047389    25     none    0.029216

Estimated objective function value = 0.033072
Estimated function evaluation time = 10.7999

Loading file: bayesopt/bayesopt_0.032756.mat
  SeriesNetwork with properties:

         Layers: [8?1 nnet.cnn.layer.Layer]
     InputNames: {'sequenceinput'}
    OutputNames: {'regressionoutput'}

  8?1 Layer array with layers:

     1   'sequenceinput'      Sequence Input      Sequence input with 1 dimensions
     2   'lstm_1'             LSTM                LSTM with 33 hidden units
     3   'relu'               ReLU                ReLU
     4   'dropout_1'          Dropout             13% dropout
     5   'lstm_2'             LSTM                LSTM with 55 hidden units
     6   'dropout_2'          Dropout             10% dropout
     7   'fc'                 Fully Connected     1 fully connected layer
     8   'regressionoutput'   Regression Output   mean-squared-error with response 'Response'
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          42              2             3.1843              0.013868        33     relu    0.12686    55     none    0.10398    92     none    0.11626

SL: 42 HS: 33 ME: 0.032756 MSE: 0.001632
Best NN configuration
Mean error: 0.032756
Max error:  0.112509
MSE:        0.001632
RMSE:       0.040403
>> 