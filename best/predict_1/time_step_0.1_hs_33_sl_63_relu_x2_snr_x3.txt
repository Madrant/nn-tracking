     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Dropout             43% dropout
     7   ''   LSTM                LSTM with 44 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             47% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 29.000000 MSE: 44.000000
SL: 4.942193e-02 HS: 3.764383e-03 ME: Saving to file: bayesopt/bayesopt_0.049422.mat
|   48 | Best   |    0.049422 |      11.904 |    0.049422 |    0.049417 |           40 |            3 |    0.0017766 |    0.0031406 |           96 |         relu |      0.26143 |           29 |         none |      0.43439 |           44 |         tanh |      0.47329 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          9               2             0.20747            0.0031056        64     relu    0.13824    30     none    0.22326    85     tanh    0.054806

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.138244
Type: lstm Hidden size: 30 Activation: none Dropout: 0.223263
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 30 hidden units
     6   ''   Dropout             22% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 30.000000 MSE: 0.070109
SL: 7.239146e-03 HS: Saving to file: bayesopt/bayesopt_0.070109.mat
|   49 | Accept |    0.070109 |      16.302 |    0.049422 |    0.049428 |            9 |            2 |      0.20747 |    0.0031056 |           64 |         relu |      0.13824 |           30 |         none |      0.22326 |           85 |         tanh |     0.054806 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          33              2             0.21065             0.010496        81     relu    0.12609    68     none    0.39294    44     tanh    0.45169

Type: lstm Hidden size: 81 Activation: relu Dropout: 0.126088
Type: lstm Hidden size: 68 Activation: none Dropout: 0.392944
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 81 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   Dropout             39% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 81 ME: 68.000000 MSE: 0.064504
SL: 6.108973e-03 HS: Saving to file: bayesopt/bayesopt_0.064504.mat
|   50 | Accept |    0.064504 |      11.748 |    0.049422 |    0.049428 |           33 |            2 |      0.21065 |     0.010496 |           81 |         relu |      0.12609 |           68 |         none |      0.39294 |           44 |         tanh |      0.45169 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          50              2             6.6288             0.0022996         7     relu    0.47899    75     none    0.042193    43     tanh    0.45152

Type: lstm Hidden size: 7 Activation: relu Dropout: 0.478992
Type: lstm Hidden size: 75 Activation: none Dropout: 0.042193
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 7 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 75 hidden units
     6   ''   Dropout             4% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 7 ME: 75.000000 MSE: 0.085588
SL: 1.014162e-02 HS: Saving to file: bayesopt/bayesopt_0.085588.mat
|   51 | Accept |    0.085588 |      6.8872 |    0.049422 |    0.049427 |           50 |            2 |       6.6288 |    0.0022996 |            7 |         relu |      0.47899 |           75 |         none |     0.042193 |           43 |         tanh |      0.45152 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          38              2            0.026954             0.011888        46     relu    0.49564    95     none    0.12049    70     tanh    0.17455

Type: lstm Hidden size: 46 Activation: relu Dropout: 0.495644
Type: lstm Hidden size: 95 Activation: none Dropout: 0.120492
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 46 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             50% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   Dropout             12% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 46 ME: 95.000000 MSE: 0.083600
SL: 1.014581e-02 HS: Saving to file: bayesopt/bayesopt_0.0836.mat
|   52 | Accept |      0.0836 |      10.793 |    0.049422 |    0.049427 |           38 |            2 |     0.026954 |     0.011888 |           46 |         relu |      0.49564 |           95 |         none |      0.12049 |           70 |         tanh |      0.17455 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          49              3            0.0023942            0.014509        50     relu    0.41495    14     none    0.23176    20     tanh    0.28631

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.414945
Type: lstm Hidden size: 14 Activation: none Dropout: 0.231762
Type: lstm Hidden size: 20 Activation: tanh Dropout: 0.286312
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             23% dropout
     7   ''   LSTM                LSTM with 20 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             29% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 14.000000 MSE: 20.000000
SL: 6.213836e-02 HS: 5.422154e-03 ME: Saving to file: bayesopt/bayesopt_0.062138.mat
|   53 | Accept |    0.062138 |      8.3351 |    0.049422 |    0.049425 |           49 |            3 |    0.0023942 |     0.014509 |           50 |         relu |      0.41495 |           14 |         none |      0.23176 |           20 |         tanh |      0.28631 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          48              2             0.97773            0.0066803         8     relu    0.2673    49     none    0.086242    84     tanh    0.17371

Type: lstm Hidden size: 8 Activation: relu Dropout: 0.267302
Type: lstm Hidden size: 49 Activation: none Dropout: 0.086242
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 8 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   Dropout             9% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 8 ME: 49.000000 MSE: 0.085593
SL: 1.074403e-02 HS: Saving to file: bayesopt/bayesopt_0.085593.mat
|   54 | Accept |    0.085593 |      5.8737 |    0.049422 |    0.050143 |           48 |            2 |      0.97773 |    0.0066803 |            8 |         relu |       0.2673 |           49 |         none |     0.086242 |           84 |         tanh |      0.17371 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          56              3            0.013005            0.0023218        92     relu    0.40468    46     none    0.4094    92     tanh    0.20685

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.404677
Type: lstm Hidden size: 46 Activation: none Dropout: 0.409404
Type: lstm Hidden size: 92 Activation: tanh Dropout: 0.206849
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 46 hidden units
     6   ''   Dropout             41% dropout
     7   ''   LSTM                LSTM with 92 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             21% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 46.000000 MSE: 92.000000
SL: 8.864731e-02 HS: 1.042406e-02 ME: Saving to file: bayesopt/bayesopt_0.088647.mat
|   55 | Accept |    0.088647 |      15.855 |    0.049422 |    0.049433 |           56 |            3 |     0.013005 |    0.0023218 |           92 |         relu |      0.40468 |           46 |         none |       0.4094 |           92 |         tanh |      0.20685 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          15              2            0.083833             0.010691        41     relu    0.033598    70     none    0.29619    55     tanh    0.35313

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.033598
Type: lstm Hidden size: 70 Activation: none Dropout: 0.296190
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 70.000000 MSE: 0.067365
SL: 6.391658e-03 HS: Saving to file: bayesopt/bayesopt_0.067365.mat
|   56 | Accept |    0.067365 |      12.398 |    0.049422 |    0.050204 |           15 |            2 |     0.083833 |     0.010691 |           41 |         relu |     0.033598 |           70 |         none |      0.29619 |           55 |         tanh |      0.35313 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          51              3             0.72121            0.0081879        24     relu    0.14537    23     none    0.25457    70     tanh    0.33443

Type: lstm Hidden size: 24 Activation: relu Dropout: 0.145368
Type: lstm Hidden size: 23 Activation: none Dropout: 0.254573
Type: lstm Hidden size: 70 Activation: tanh Dropout: 0.334435
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 24 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 23 hidden units
     6   ''   Dropout             25% dropout
     7   ''   LSTM                LSTM with 70 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             33% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 24 ME: 23.000000 MSE: 70.000000
SL: 8.168327e-02 HS: 1.038478e-02 ME: Saving to file: bayesopt/bayesopt_0.081683.mat
|   57 | Accept |    0.081683 |      9.4988 |    0.049422 |    0.049429 |           51 |            3 |      0.72121 |    0.0081879 |           24 |         relu |      0.14537 |           23 |         none |      0.25457 |           70 |         tanh |      0.33443 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _____

          76              2            0.001216             0.017477         4     relu    0.31104    75     none    0.17731     2     tanh    0.056

Type: lstm Hidden size: 4 Activation: relu Dropout: 0.311042
Type: lstm Hidden size: 75 Activation: none Dropout: 0.177309
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 4 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 75 hidden units
     6   ''   Dropout             18% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 4 ME: 75.000000 MSE: 0.075361
SL: 8.352724e-03 HS: Saving to file: bayesopt/bayesopt_0.075361.mat
|   58 | Accept |    0.075361 |      7.3398 |    0.049422 |    0.058585 |           76 |            2 |     0.001216 |     0.017477 |            4 |         relu |      0.31104 |           75 |         none |      0.17731 |            2 |         tanh |        0.056 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          96              2             0.62679             0.032357        92     relu    0.37518    66     none    0.30343    77     tanh    0.060169

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.375179
Type: lstm Hidden size: 66 Activation: none Dropout: 0.303425
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 66 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 66.000000 MSE: 0.183240
SL: 4.589363e-02 HS: Saving to file: bayesopt/bayesopt_0.18324.mat
|   59 | Accept |     0.18324 |      10.599 |    0.049422 |    0.049438 |           96 |            2 |      0.62679 |     0.032357 |           92 |         relu |      0.37518 |           66 |         none |      0.30343 |           77 |         tanh |     0.060169 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          18              3            0.017243            0.0067129        95     relu    0.39241    42     none    0.097091     6     tanh    0.13672

Type: lstm Hidden size: 95 Activation: relu Dropout: 0.392408
Type: lstm Hidden size: 42 Activation: none Dropout: 0.097091
Type: lstm Hidden size: 6 Activation: tanh Dropout: 0.136721
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 95 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             10% dropout
     7   ''   LSTM                LSTM with 6 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             14% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 95 ME: 42.000000 MSE: 6.000000
SL: 6.273816e-02 HS: 6.258313e-03 ME: Saving to file: bayesopt/bayesopt_0.062738.mat
|   60 | Accept |    0.062738 |      17.405 |    0.049422 |    0.049431 |           18 |            3 |     0.017243 |    0.0067129 |           95 |         relu |      0.39241 |           42 |         none |     0.097091 |            6 |         tanh |      0.13672 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          94              3             1.3247              0.089761         8     relu    0.47736    30     none    0.053429     5     tanh    0.46646

Type: lstm Hidden size: 8 Activation: relu Dropout: 0.477357
Type: lstm Hidden size: 30 Activation: none Dropout: 0.053429
Type: lstm Hidden size: 5 Activation: tanh Dropout: 0.466463
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 8 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 30 hidden units
     6   ''   Dropout             5% dropout
     7   ''   LSTM                LSTM with 5 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             47% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 8 ME: 30.000000 MSE: 5.000000
SL: 1.806036e-01 HS: 4.439258e-02 ME: Saving to file: bayesopt/bayesopt_0.1806.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|   61 | Accept |      0.1806 |      6.7247 |    0.049422 |    0.049441 |           94 |            3 |       1.3247 |     0.089761 |            8 |         relu |      0.47736 |           30 |         none |     0.053429 |            5 |         tanh |      0.46646 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ______

          55              3            0.008054             0.003807        40     relu    0.1547    93     none    0.17972    78     tanh    0.3627

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.154701
Type: lstm Hidden size: 93 Activation: none Dropout: 0.179717
Type: lstm Hidden size: 78 Activation: tanh Dropout: 0.362702
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 93 hidden units
     6   ''   Dropout             18% dropout
     7   ''   LSTM                LSTM with 78 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             36% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 93.000000 MSE: 78.000000
SL: 1.349465e-01 HS: 2.913008e-02 ME: Saving to file: bayesopt/bayesopt_0.13495.mat
|   62 | Accept |     0.13495 |      15.353 |    0.049422 |    0.049432 |           55 |            3 |     0.008054 |     0.003807 |           40 |         relu |       0.1547 |           93 |         none |      0.17972 |           78 |         tanh |       0.3627 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _________    ___    ____    _______

          50              2            0.0011178           0.0081072        28     relu    0.27855    46     none    0.0017734     7     tanh    0.35869

Type: lstm Hidden size: 28 Activation: relu Dropout: 0.278554
Type: lstm Hidden size: 46 Activation: none Dropout: 0.001773
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 28 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 46 hidden units
     6   ''   Dropout             0% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 28 ME: 46.000000 MSE: 0.074936
SL: 7.795512e-03 HS: Saving to file: bayesopt/bayesopt_0.074936.mat
|   63 | Accept |    0.074936 |      6.4647 |    0.049422 |    0.049437 |           50 |            2 |    0.0011178 |    0.0081072 |           28 |         relu |      0.27855 |           46 |         none |    0.0017734 |            7 |         tanh |      0.35869 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          37              2             1.8139              0.002344        65     relu    0.11102    69     none    0.078433    70     tanh    0.040961

Type: lstm Hidden size: 65 Activation: relu Dropout: 0.111016
Type: lstm Hidden size: 69 Activation: none Dropout: 0.078433
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 65 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 69 hidden units
     6   ''   Dropout             8% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 65 ME: 69.000000 MSE: 0.049886
SL: 3.525273e-03 HS: Saving to file: bayesopt/bayesopt_0.049886.mat
|   64 | Accept |    0.049886 |      9.6843 |    0.049422 |    0.049433 |           37 |            2 |       1.8139 |     0.002344 |           65 |         relu |      0.11102 |           69 |         none |     0.078433 |           70 |         tanh |     0.040961 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    ______

          49              2            0.001707            0.0076968        100    relu    0.1511     3     none    0.096687     4     tanh    0.4429

Type: lstm Hidden size: 100 Activation: relu Dropout: 0.151099
Type: lstm Hidden size: 3 Activation: none Dropout: 0.096687
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 100 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 3 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 100 ME: 3.000000 MSE: 0.063588
SL: 5.943992e-03 HS: Saving to file: bayesopt/bayesopt_0.063588.mat
|   65 | Accept |    0.063588 |      7.9745 |    0.049422 |     0.04944 |           49 |            2 |     0.001707 |    0.0076968 |          100 |         relu |       0.1511 |            3 |         none |     0.096687 |            4 |         tanh |       0.4429 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          33              3             1.4589              0.004072         2     relu    0.22367    28     none    0.40471    13     tanh    0.30143

Type: lstm Hidden size: 2 Activation: relu Dropout: 0.223670
Type: lstm Hidden size: 28 Activation: none Dropout: 0.404706
Type: lstm Hidden size: 13 Activation: tanh Dropout: 0.301426
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 2 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             22% dropout
     5   ''   LSTM                LSTM with 28 hidden units
     6   ''   Dropout             40% dropout
     7   ''   LSTM                LSTM with 13 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             30% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 2 ME: 28.000000 MSE: 13.000000
SL: 1.885200e-01 HS: 4.920853e-02 ME: Saving to file: bayesopt/bayesopt_0.18852.mat
|   66 | Accept |     0.18852 |      9.0879 |    0.049422 |    0.049433 |           33 |            3 |       1.4589 |     0.004072 |            2 |         relu |      0.22367 |           28 |         none |      0.40471 |           13 |         tanh |      0.30143 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          62              2              3.408             0.0033075        41     relu    0.27046    98     none    0.38645    98     tanh    0.39661

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.270456
Type: lstm Hidden size: 98 Activation: none Dropout: 0.386449
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   Dropout             39% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 98.000000 MSE: 0.055636
SL: 4.677397e-03 HS: Saving to file: bayesopt/bayesopt_0.055636.mat
|   67 | Accept |    0.055636 |      8.6845 |    0.049422 |    0.049434 |           62 |            2 |        3.408 |    0.0033075 |           41 |         relu |      0.27046 |           98 |         none |      0.38645 |           98 |         tanh |      0.39661 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          98              3            0.015284             0.01237         26     relu    0.30033    79     none    0.19668    40     tanh    0.26387

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.300328
Type: lstm Hidden size: 79 Activation: none Dropout: 0.196675
Type: lstm Hidden size: 40 Activation: tanh Dropout: 0.263867
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 79 hidden units
     6   ''   Dropout             20% dropout
     7   ''   LSTM                LSTM with 40 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             26% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 79.000000 MSE: 40.000000
SL: 8.180703e-02 HS: 9.999353e-03 ME: Saving to file: bayesopt/bayesopt_0.081807.mat
|   68 | Accept |    0.081807 |      10.938 |    0.049422 |    0.049433 |           98 |            3 |     0.015284 |      0.01237 |           26 |         relu |      0.30033 |           79 |         none |      0.19668 |           40 |         tanh |      0.26387 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          72              2             1.1836              0.043328        21     relu    0.058225    100    none    0.37619    56     tanh    0.43118

Type: lstm Hidden size: 21 Activation: relu Dropout: 0.058225
Type: lstm Hidden size: 100 Activation: none Dropout: 0.376195
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 21 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 100 hidden units
     6   ''   Dropout             38% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 21 ME: 100.000000 MSE: 0.107312
SL: 1.534798e-02 HS: Saving to file: bayesopt/bayesopt_0.10731.mat
|   69 | Accept |     0.10731 |      8.9425 |    0.049422 |    0.051212 |           72 |            2 |       1.1836 |     0.043328 |           21 |         relu |     0.058225 |          100 |         none |      0.37619 |           56 |         tanh |      0.43118 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          31              3            0.0048873           0.0033229        69     relu    0.48187     2     none    0.22322    65     tanh    0.30921

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.481871
Type: lstm Hidden size: 2 Activation: none Dropout: 0.223215
Type: lstm Hidden size: 65 Activation: tanh Dropout: 0.309206
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 2 hidden units
     6   ''   Dropout             22% dropout
     7   ''   LSTM                LSTM with 65 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             31% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 2.000000 MSE: 65.000000
SL: 8.393887e-02 HS: 9.886342e-03 ME: Saving to file: bayesopt/bayesopt_0.083939.mat
|   70 | Accept |    0.083939 |      11.426 |    0.049422 |    0.049439 |           31 |            3 |    0.0048873 |    0.0033229 |           69 |         relu |      0.48187 |            2 |         none |      0.22322 |           65 |         tanh |      0.30921 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          98              3            0.0019119           0.0092097        78     relu    0.10402    35     none    0.066511     7     tanh    0.16685

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.104017
Type: lstm Hidden size: 35 Activation: none Dropout: 0.066511
Type: lstm Hidden size: 7 Activation: tanh Dropout: 0.166846
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 35 hidden units
     6   ''   Dropout             7% dropout
     7   ''   LSTM                LSTM with 7 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             17% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 35.000000 MSE: 7.000000
SL: 6.879496e-02 HS: 6.456191e-03 ME: Saving to file: bayesopt/bayesopt_0.068795.mat
|   71 | Accept |    0.068795 |       9.766 |    0.049422 |    0.051285 |           98 |            3 |    0.0019119 |    0.0092097 |           78 |         relu |      0.10402 |           35 |         none |     0.066511 |            7 |         tanh |      0.16685 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          20              2             0.45431            0.0030511        62     relu    0.34289    80     none    0.29102    20     tanh    0.26837

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.342894
Type: lstm Hidden size: 80 Activation: none Dropout: 0.291015
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 80 hidden units
     6   ''   Dropout             29% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 80.000000 MSE: 0.083384
SL: 9.378142e-03 HS: Saving to file: bayesopt/bayesopt_0.083384.mat
|   72 | Accept |    0.083384 |      12.579 |    0.049422 |    0.049432 |           20 |            2 |      0.45431 |    0.0030511 |           62 |         relu |      0.34289 |           80 |         none |      0.29102 |           20 |         tanh |      0.26837 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          84              2              1.656              0.019754        11     relu    0.3786    11     none    0.46347    35     tanh    0.21167

Type: lstm Hidden size: 11 Activation: relu Dropout: 0.378603
Type: lstm Hidden size: 11 Activation: none Dropout: 0.463466
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 11 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   Dropout             46% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 11 ME: 11.000000 MSE: 0.057908
SL: 4.844957e-03 HS: Saving to file: bayesopt/bayesopt_0.057908.mat
|   73 | Accept |    0.057908 |      4.2611 |    0.049422 |    0.056622 |           84 |            2 |        1.656 |     0.019754 |           11 |         relu |       0.3786 |           11 |         none |      0.46347 |           35 |         tanh |      0.21167 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          31              2             1.6291             0.0017994        42     relu    0.18758    85     none    0.016915    85     tanh    0.048462

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.187580
Type: lstm Hidden size: 85 Activation: none Dropout: 0.016915
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 85 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 85.000000 MSE: 0.058804
SL: 4.932636e-03 HS: Saving to file: bayesopt/bayesopt_0.058804.mat
|   74 | Accept |    0.058804 |      9.6865 |    0.049422 |    0.049439 |           31 |            2 |       1.6291 |    0.0017994 |           42 |         relu |      0.18758 |           85 |         none |     0.016915 |           85 |         tanh |     0.048462 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          38              2            0.015662            0.0015509        60     relu    0.27731    58     none    0.13349    27     tanh    0.32149

Type: lstm Hidden size: 60 Activation: relu Dropout: 0.277307
Type: lstm Hidden size: 58 Activation: none Dropout: 0.133488
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 60 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 58 hidden units
     6   ''   Dropout             13% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 60 ME: 58.000000 MSE: 0.070312
SL: 6.798613e-03 HS: Saving to file: bayesopt/bayesopt_0.070312.mat
|   75 | Accept |    0.070312 |      8.9327 |    0.049422 |    0.056568 |           38 |            2 |     0.015662 |    0.0015509 |           60 |         relu |      0.27731 |           58 |         none |      0.13349 |           27 |         tanh |      0.32149 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          72              2             5.9365             0.0036866        22     relu    0.16938    95     none    0.066114    53     tanh    0.023785

Type: lstm Hidden size: 22 Activation: relu Dropout: 0.169382
Type: lstm Hidden size: 95 Activation: none Dropout: 0.066114
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 22 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 22 ME: 95.000000 MSE: 0.066427
SL: 6.494490e-03 HS: Saving to file: bayesopt/bayesopt_0.066427.mat
|   76 | Accept |    0.066427 |      8.5411 |    0.049422 |    0.049432 |           72 |            2 |       5.9365 |    0.0036866 |           22 |         relu |      0.16938 |           95 |         none |     0.066114 |           53 |         tanh |     0.023785 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          59              2            0.080771            0.0042706        42     relu    0.47231    63     none    0.046639    63     tanh    0.38248

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.472306
Type: lstm Hidden size: 63 Activation: none Dropout: 0.046639
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 63 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 63.000000 MSE: 0.085493
SL: 9.921995e-03 HS: Saving to file: bayesopt/bayesopt_0.085493.mat
|   77 | Accept |    0.085493 |      8.2015 |    0.049422 |    0.056129 |           59 |            2 |     0.080771 |    0.0042706 |           42 |         relu |      0.47231 |           63 |         none |     0.046639 |           63 |         tanh |      0.38248 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

         100              2             0.45127             0.01322         17     relu    0.3675    40     none    0.11068    44     tanh    0.39881

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.367501
Type: lstm Hidden size: 40 Activation: none Dropout: 0.110682
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 40 hidden units
     6   ''   Dropout             11% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 40.000000 MSE: 0.061201
SL: 5.629986e-03 HS: Saving to file: bayesopt/bayesopt_0.061201.mat
|   78 | Accept |    0.061201 |      5.4209 |    0.049422 |    0.056085 |          100 |            2 |      0.45127 |      0.01322 |           17 |         relu |       0.3675 |           40 |         none |      0.11068 |           44 |         tanh |      0.39881 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          89              2             0.58327             0.010083        75     relu    0.060191     9     none    0.21742    96     tanh    0.35216

Type: lstm Hidden size: 75 Activation: relu Dropout: 0.060191
Type: lstm Hidden size: 9 Activation: none Dropout: 0.217423
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 75 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 9 hidden units
     6   ''   Dropout             22% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 75 ME: 9.000000 MSE: 0.064665
SL: 6.463654e-03 HS: Saving to file: bayesopt/bayesopt_0.064665.mat
|   79 | Accept |    0.064665 |      6.3744 |    0.049422 |    0.056148 |           89 |            2 |      0.58327 |     0.010083 |           75 |         relu |     0.060191 |            9 |         none |      0.21742 |           96 |         tanh |      0.35216 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          97              2              1.605             0.0036415        97     relu    0.17607    38     none    0.25159    29     tanh    0.18154

Type: lstm Hidden size: 97 Activation: relu Dropout: 0.176071
Type: lstm Hidden size: 38 Activation: none Dropout: 0.251594
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 97 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   Dropout             25% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 97 ME: 38.000000 MSE: 0.060765
SL: 5.492997e-03 HS: Saving to file: bayesopt/bayesopt_0.060765.mat
|   80 | Accept |    0.060765 |      9.3385 |    0.049422 |    0.056131 |           97 |            2 |        1.605 |    0.0036415 |           97 |         relu |      0.17607 |           38 |         none |      0.25159 |           29 |         tanh |      0.18154 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          11              2            0.077705            0.0029457        98     relu    0.11007    49     none    0.053636    64     tanh    0.068247

Type: lstm Hidden size: 98 Activation: relu Dropout: 0.110069
Type: lstm Hidden size: 49 Activation: none Dropout: 0.053636
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 98 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 98 ME: 49.000000 MSE: 0.069674
SL: 7.349916e-03 HS: Saving to file: bayesopt/bayesopt_0.069674.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|   81 | Accept |    0.069674 |      17.553 |    0.049422 |    0.056311 |           11 |            2 |     0.077705 |    0.0029457 |           98 |         relu |      0.11007 |           49 |         none |     0.053636 |           64 |         tanh |     0.068247 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          16              2            0.034308             0.014874        71     relu    0.18571    13     none    0.27364    69     tanh    0.19691

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.185708
Type: lstm Hidden size: 13 Activation: none Dropout: 0.273640
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   Dropout             27% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 13.000000 MSE: 0.074131
SL: 8.152427e-03 HS: Saving to file: bayesopt/bayesopt_0.074131.mat
|   82 | Accept |    0.074131 |      11.142 |    0.049422 |    0.056518 |           16 |            2 |     0.034308 |     0.014874 |           71 |         relu |      0.18571 |           13 |         none |      0.27364 |           69 |         tanh |      0.19691 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          10              2            0.005613             0.006422        86     relu    0.22923    46     none    0.14293    93     tanh    0.28065

Type: lstm Hidden size: 86 Activation: relu Dropout: 0.229232
Type: lstm Hidden size: 46 Activation: none Dropout: 0.142929
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 86 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 46 hidden units
     6   ''   Dropout             14% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 86 ME: 46.000000 MSE: 0.077302
SL: 9.864367e-03 HS: Saving to file: bayesopt/bayesopt_0.077302.mat
|   83 | Accept |    0.077302 |      17.175 |    0.049422 |    0.049434 |           10 |            2 |     0.005613 |     0.006422 |           86 |         relu |      0.22923 |           46 |         none |      0.14293 |           93 |         tanh |      0.28065 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          67              2             5.0827             0.0057365        25     relu    0.067609    91     none    0.36067    86     tanh    0.39175

Type: lstm Hidden size: 25 Activation: relu Dropout: 0.067609
Type: lstm Hidden size: 91 Activation: none Dropout: 0.360665
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 25 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   Dropout             36% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 25 ME: 91.000000 MSE: 0.067389
SL: 6.796059e-03 HS: Saving to file: bayesopt/bayesopt_0.067389.mat
|   84 | Accept |    0.067389 |       7.839 |    0.049422 |    0.049433 |           67 |            2 |       5.0827 |    0.0057365 |           25 |         relu |     0.067609 |           91 |         none |      0.36067 |           86 |         tanh |      0.39175 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          54              2             0.59071            0.0019529        62     relu    0.12756     1     none    0.23268    85     tanh    0.35921

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.127557
Type: lstm Hidden size: 1 Activation: none Dropout: 0.232676
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 1 hidden units
     6   ''   Dropout             23% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 1.000000 MSE: 0.095328
SL: 1.333614e-02 HS: Saving to file: bayesopt/bayesopt_0.095328.mat
|   85 | Accept |    0.095328 |      5.9831 |    0.049422 |    0.057497 |           54 |            2 |      0.59071 |    0.0019529 |           62 |         relu |      0.12756 |            1 |         none |      0.23268 |           85 |         tanh |      0.35921 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              2             4.4468              0.030132        38     relu    0.10301    37     none    0.10333    49     tanh    0.14622

Type: lstm Hidden size: 38 Activation: relu Dropout: 0.103009
Type: lstm Hidden size: 37 Activation: none Dropout: 0.103332
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 38 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 37 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 38 ME: 37.000000 MSE: 0.050694
SL: 4.199182e-03 HS: Saving to file: bayesopt/bayesopt_0.050694.mat
|   86 | Accept |    0.050694 |      5.6021 |    0.049422 |     0.05622 |           90 |            2 |       4.4468 |     0.030132 |           38 |         relu |      0.10301 |           37 |         none |      0.10333 |           49 |         tanh |      0.14622 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          97              2            0.0011271            0.030619        17     relu    0.10403     6     none    0.33244     6     tanh    0.21756

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.104033
Type: lstm Hidden size: 6 Activation: none Dropout: 0.332443
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             33% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 6.000000 MSE: 0.062248
SL: 5.515916e-03 HS: Saving to file: bayesopt/bayesopt_0.062248.mat
|   87 | Accept |    0.062248 |      4.6018 |    0.049422 |    0.056246 |           97 |            2 |    0.0011271 |     0.030619 |           17 |         relu |      0.10403 |            6 |         none |      0.33244 |            6 |         tanh |      0.21756 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          97              2            0.0065928            0.015346         8     relu    0.15031     4     none    0.41637    32     tanh    0.16859

Type: lstm Hidden size: 8 Activation: relu Dropout: 0.150308
Type: lstm Hidden size: 4 Activation: none Dropout: 0.416372
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 8 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 4 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 8 ME: 4.000000 MSE: 0.095675
SL: 1.431024e-02 HS: Saving to file: bayesopt/bayesopt_0.095675.mat
|   88 | Accept |    0.095675 |      4.3617 |    0.049422 |    0.055412 |           97 |            2 |    0.0065928 |     0.015346 |            8 |         relu |      0.15031 |            4 |         none |      0.41637 |           32 |         tanh |      0.16859 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          60              2            0.049451             0.030779        30     relu    0.41669    29     none    0.18763    81     tanh    0.11467

Type: lstm Hidden size: 30 Activation: relu Dropout: 0.416687
Type: lstm Hidden size: 29 Activation: none Dropout: 0.187635
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 30 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Dropout             19% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 30 ME: 29.000000 MSE: 0.070799
SL: 7.306382e-03 HS: Saving to file: bayesopt/bayesopt_0.070799.mat
|   89 | Accept |    0.070799 |      5.1952 |    0.049422 |    0.058493 |           60 |            2 |     0.049451 |     0.030779 |           30 |         relu |      0.41669 |           29 |         none |      0.18763 |           81 |         tanh |      0.11467 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          73              3             1.4308              0.015776        48     relu    0.045174    17     none    0.48728    72     tanh    0.44385

Type: lstm Hidden size: 48 Activation: relu Dropout: 0.045174
Type: lstm Hidden size: 17 Activation: none Dropout: 0.487277
Type: lstm Hidden size: 72 Activation: tanh Dropout: 0.443851
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 48 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 17 hidden units
     6   ''   Dropout             49% dropout
     7   ''   LSTM                LSTM with 72 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             44% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 48 ME: 17.000000 MSE: 72.000000
SL: 5.988627e-02 HS: 5.230074e-03 ME: Saving to file: bayesopt/bayesopt_0.059886.mat
|   90 | Accept |    0.059886 |      10.291 |    0.049422 |    0.058558 |           73 |            3 |       1.4308 |     0.015776 |           48 |         relu |     0.045174 |           17 |         none |      0.48728 |           72 |         tanh |      0.44385 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          94              3            0.0066808           0.0023151        33     relu    0.39538    57     none    0.14392    66     none    0.37832

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.395384
Type: lstm Hidden size: 57 Activation: none Dropout: 0.143921
Type: lstm Hidden size: 66 Activation: none Dropout: 0.378320
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             14% dropout
     7   ''   LSTM                LSTM with 66 hidden units
     8   ''   Dropout             38% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 57.000000 MSE: 66.000000
SL: 9.439410e-02 HS: 1.312563e-02 ME: Saving to file: bayesopt/bayesopt_0.094394.mat
|   91 | Accept |    0.094394 |      10.083 |    0.049422 |    0.058519 |           94 |            3 |    0.0066808 |    0.0023151 |           33 |         relu |      0.39538 |           57 |         none |      0.14392 |           66 |         none |      0.37832 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          57              2            0.051426            0.0049319        94     relu    0.13885    50     none    0.068487    57     tanh    0.49438

Type: lstm Hidden size: 94 Activation: relu Dropout: 0.138851
Type: lstm Hidden size: 50 Activation: none Dropout: 0.068487
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 94 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 50 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 94 ME: 50.000000 MSE: 0.044791
SL: 3.306133e-03 HS: Saving to file: bayesopt/bayesopt_0.044791.mat
|   92 | Best   |    0.044791 |      10.611 |    0.044791 |    0.050216 |           57 |            2 |     0.051426 |    0.0049319 |           94 |         relu |      0.13885 |           50 |         none |     0.068487 |           57 |         tanh |      0.49438 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          20              2             0.45177            0.0018576        36     relu    0.24118    97     none    0.42261    21     tanh    0.43954

Type: lstm Hidden size: 36 Activation: relu Dropout: 0.241176
Type: lstm Hidden size: 97 Activation: none Dropout: 0.422608
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 36 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             24% dropout
     5   ''   LSTM                LSTM with 97 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 36 ME: 97.000000 MSE: 0.070039
SL: 7.469902e-03 HS: Saving to file: bayesopt/bayesopt_0.070039.mat
|   93 | Accept |    0.070039 |      12.179 |    0.044791 |    0.049755 |           20 |            2 |      0.45177 |    0.0018576 |           36 |         relu |      0.24118 |           97 |         none |      0.42261 |           21 |         tanh |      0.43954 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          63              2            0.010733             0.026647        33     relu    0.27291    36     none    0.17883     7     tanh    0.46316

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.272908
Type: lstm Hidden size: 36 Activation: none Dropout: 0.178832
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 36 hidden units
     6   ''   Dropout             18% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 36.000000 MSE: 0.040542
SL: 2.417759e-03 HS: Saving to file: bayesopt/bayesopt_0.040542.mat
|   94 | Best   |    0.040542 |       5.766 |    0.040542 |    0.045939 |           63 |            2 |     0.010733 |     0.026647 |           33 |         relu |      0.27291 |           36 |         none |      0.17883 |            7 |         tanh |      0.46316 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          76              2             0.1524              0.02882         31     relu    0.062227    27     none    0.36345     4     tanh    0.35385

Type: lstm Hidden size: 31 Activation: relu Dropout: 0.062227
Type: lstm Hidden size: 27 Activation: none Dropout: 0.363454
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 31 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 27 hidden units
     6   ''   Dropout             36% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 31 ME: 27.000000 MSE: 0.050843
SL: 4.338445e-03 HS: Saving to file: bayesopt/bayesopt_0.050843.mat
|   95 | Accept |    0.050843 |      5.9156 |    0.040542 |    0.046645 |           76 |            2 |       0.1524 |      0.02882 |           31 |         relu |     0.062227 |           27 |         none |      0.36345 |            4 |         tanh |      0.35385 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          63              2             0.86595            0.0064831        86     relu    0.045526    25     none    0.13185    42     tanh    0.48348

Type: lstm Hidden size: 86 Activation: relu Dropout: 0.045526
Type: lstm Hidden size: 25 Activation: none Dropout: 0.131854
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 86 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 25 hidden units
     6   ''   Dropout             13% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 86 ME: 25.000000 MSE: 0.056228
SL: 5.389465e-03 HS: Saving to file: bayesopt/bayesopt_0.056228.mat
|   96 | Accept |    0.056228 |      7.4973 |    0.040542 |    0.047068 |           63 |            2 |      0.86595 |    0.0064831 |           86 |         relu |     0.045526 |           25 |         none |      0.13185 |           42 |         tanh |      0.48348 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ______

          90              2            0.024122             0.02353         23     relu    0.1442    35     none    0.26848    25     tanh    0.3408

Type: lstm Hidden size: 23 Activation: relu Dropout: 0.144201
Type: lstm Hidden size: 35 Activation: none Dropout: 0.268484
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 23 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 35 hidden units
     6   ''   Dropout             27% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 23 ME: 35.000000 MSE: 0.073490
SL: 7.665728e-03 HS: Saving to file: bayesopt/bayesopt_0.07349.mat
|   97 | Accept |     0.07349 |      5.1547 |    0.040542 |    0.051432 |           90 |            2 |     0.024122 |      0.02353 |           23 |         relu |       0.1442 |           35 |         none |      0.26848 |           25 |         tanh |       0.3408 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          61              2             0.11756             0.031745        39     relu    0.037198    38     none    0.013332    25     tanh    0.37025

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.037198
Type: lstm Hidden size: 38 Activation: none Dropout: 0.013332
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 38.000000 MSE: 0.073823
SL: 7.776650e-03 HS: Saving to file: bayesopt/bayesopt_0.073823.mat
|   98 | Accept |    0.073823 |      5.7879 |    0.040542 |     0.05441 |           61 |            2 |      0.11756 |     0.031745 |           39 |         relu |     0.037198 |           38 |         none |     0.013332 |           25 |         tanh |      0.37025 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          96              2             4.4328             0.0048901        88     relu    0.076643    68     none    0.35435    58     tanh    0.14192

Type: lstm Hidden size: 88 Activation: relu Dropout: 0.076643
Type: lstm Hidden size: 68 Activation: none Dropout: 0.354346
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 88 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   Dropout             35% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 88 ME: 68.000000 MSE: 0.056484
SL: 4.514169e-03 HS: Saving to file: bayesopt/bayesopt_0.056484.mat
|   99 | Accept |    0.056484 |      10.149 |    0.040542 |    0.054322 |           96 |            2 |       4.4328 |    0.0048901 |           88 |         relu |     0.076643 |           68 |         none |      0.35435 |           58 |         tanh |      0.14192 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          54              2             2.4527             0.0070068        93     relu    0.36382    44     none    0.38425    13     tanh    0.48712

Type: lstm Hidden size: 93 Activation: relu Dropout: 0.363825
Type: lstm Hidden size: 44 Activation: none Dropout: 0.384250
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 93 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 44 hidden units
     6   ''   Dropout             38% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 93 ME: 44.000000 MSE: 0.043392
SL: 2.880152e-03 HS: Saving to file: bayesopt/bayesopt_0.043392.mat
|  100 | Accept |    0.043392 |      10.239 |    0.040542 |     0.04935 |           54 |            2 |       2.4527 |    0.0070068 |           93 |         relu |      0.36382 |           44 |         none |      0.38425 |           13 |         tanh |      0.48712 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _____

          52              2            0.0023556           0.0046445        98     relu    0.17299    55     none    0.2792    66     tanh    0.137

Type: lstm Hidden size: 98 Activation: relu Dropout: 0.172986
Type: lstm Hidden size: 55 Activation: none Dropout: 0.279201
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 98 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 55 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 98 ME: 55.000000 MSE: 0.071407
SL: 7.563246e-03 HS: Saving to file: bayesopt/bayesopt_0.071407.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  101 | Accept |    0.071407 |      10.633 |    0.040542 |    0.049445 |           52 |            2 |    0.0023556 |    0.0046445 |           98 |         relu |      0.17299 |           55 |         none |       0.2792 |           66 |         tanh |        0.137 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          86              3             0.66002             0.016882        40     relu    0.12062    22     none    0.066882    18     tanh    0.18565

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.120617
Type: lstm Hidden size: 22 Activation: none Dropout: 0.066882
Type: lstm Hidden size: 18 Activation: tanh Dropout: 0.185655
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 22 hidden units
     6   ''   Dropout             7% dropout
     7   ''   LSTM                LSTM with 18 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             19% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 22.000000 MSE: 18.000000
SL: 5.888477e-02 HS: 4.787172e-03 ME: Saving to file: bayesopt/bayesopt_0.058885.mat
|  102 | Accept |    0.058885 |      7.0858 |    0.040542 |    0.049884 |           86 |            3 |      0.66002 |     0.016882 |           40 |         relu |      0.12062 |           22 |         none |     0.066882 |           18 |         tanh |      0.18565 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              3             3.0249              0.010457        45     relu    0.47986    28     none    0.25543    20     tanh    0.32688

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.479864
Type: lstm Hidden size: 28 Activation: none Dropout: 0.255432
Type: lstm Hidden size: 20 Activation: tanh Dropout: 0.326884
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 28 hidden units
     6   ''   Dropout             26% dropout
     7   ''   LSTM                LSTM with 20 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             33% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 28.000000 MSE: 20.000000
SL: 5.505119e-02 HS: 4.274474e-03 ME: Saving to file: bayesopt/bayesopt_0.055051.mat
|  103 | Accept |    0.055051 |      7.6486 |    0.040542 |    0.049343 |           90 |            3 |       3.0249 |     0.010457 |           45 |         relu |      0.47986 |           28 |         none |      0.25543 |           20 |         tanh |      0.32688 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          90              3            0.019365             0.012717        69     relu    0.059117    62     none    0.19489    83     none    0.30772

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.059117
Type: lstm Hidden size: 62 Activation: none Dropout: 0.194891
Type: lstm Hidden size: 83 Activation: none Dropout: 0.307717
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 62 hidden units
     6   ''   Dropout             19% dropout
     7   ''   LSTM                LSTM with 83 hidden units
     8   ''   Dropout             31% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 62.000000 MSE: 83.000000
SL: 5.498194e-02 HS: 4.160209e-03 ME: Saving to file: bayesopt/bayesopt_0.054982.mat
|  104 | Accept |    0.054982 |      12.848 |    0.040542 |    0.049405 |           90 |            3 |     0.019365 |     0.012717 |           69 |         relu |     0.059117 |           62 |         none |      0.19489 |           83 |         none |      0.30772 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          61              2             2.4176              0.027202        26     relu    0.12657    13     none    0.37342     7     tanh    0.032715

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.126572
Type: lstm Hidden size: 13 Activation: none Dropout: 0.373416
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   Dropout             37% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 13.000000 MSE: 0.055479
SL: 4.363207e-03 HS: Saving to file: bayesopt/bayesopt_0.055479.mat
|  105 | Accept |    0.055479 |      4.9083 |    0.040542 |    0.049448 |           61 |            2 |       2.4176 |     0.027202 |           26 |         relu |      0.12657 |           13 |         none |      0.37342 |            7 |         tanh |     0.032715 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          96              2            0.018772             0.010386        59     relu    0.30897    78     none    0.33121    21     none    0.31153

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.308966
Type: lstm Hidden size: 78 Activation: none Dropout: 0.331208
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 78 hidden units
     6   ''   Dropout             33% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 78.000000 MSE: 0.071467
SL: 8.948975e-03 HS: Saving to file: bayesopt/bayesopt_0.071467.mat
|  106 | Accept |    0.071467 |      8.8724 |    0.040542 |     0.04958 |           96 |            2 |     0.018772 |     0.010386 |           59 |         relu |      0.30897 |           78 |         none |      0.33121 |           21 |         none |      0.31153 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          74              2             5.2368             0.0072247        86     relu    0.47746    30     none    0.33071     2     tanh    0.037715

Type: lstm Hidden size: 86 Activation: relu Dropout: 0.477457
Type: lstm Hidden size: 30 Activation: none Dropout: 0.330708
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 86 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 30 hidden units
     6   ''   Dropout             33% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 86 ME: 30.000000 MSE: 0.048304
SL: 3.433778e-03 HS: Saving to file: bayesopt/bayesopt_0.048304.mat
|  107 | Accept |    0.048304 |      8.8832 |    0.040542 |    0.049661 |           74 |            2 |       5.2368 |    0.0072247 |           86 |         relu |      0.47746 |           30 |         none |      0.33071 |            2 |         tanh |     0.037715 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    ______

          99              2            0.050146             0.033319        79     relu    0.25076    44     none    0.3315    17     none    0.2107

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.250761
Type: lstm Hidden size: 44 Activation: none Dropout: 0.331495
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 44 hidden units
     6   ''   Dropout             33% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 44.000000 MSE: 0.052411
SL: 4.277095e-03 HS: Saving to file: bayesopt/bayesopt_0.052411.mat
|  108 | Accept |    0.052411 |      8.7754 |    0.040542 |    0.049628 |           99 |            2 |     0.050146 |     0.033319 |           79 |         relu |      0.25076 |           44 |         none |       0.3315 |           17 |         none |       0.2107 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          96              2             1.5611              0.037118        52     relu    0.34495    42     none    0.028831     3     none    0.065386

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.344946
Type: lstm Hidden size: 42 Activation: none Dropout: 0.028831
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 42.000000 MSE: 0.077756
SL: 9.332589e-03 HS: Saving to file: bayesopt/bayesopt_0.077756.mat
|  109 | Accept |    0.077756 |      6.7275 |    0.040542 |    0.049689 |           96 |            2 |       1.5611 |     0.037118 |           52 |         relu |      0.34495 |           42 |         none |     0.028831 |            3 |         none |     0.065386 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          82              2             0.34561             0.012593        86     relu    0.16158    46     none    0.22734    53     none    0.11616

Type: lstm Hidden size: 86 Activation: relu Dropout: 0.161580
Type: lstm Hidden size: 46 Activation: none Dropout: 0.227341
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 86 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 46 hidden units
     6   ''   Dropout             23% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 86 ME: 46.000000 MSE: 0.066429
SL: 6.256340e-03 HS: Saving to file: bayesopt/bayesopt_0.066429.mat
|  110 | Accept |    0.066429 |      7.8559 |    0.040542 |    0.049719 |           82 |            2 |      0.34561 |     0.012593 |           86 |         relu |      0.16158 |           46 |         none |      0.22734 |           53 |         none |      0.11616 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          79              2            0.013963             0.029157        92     relu    0.14891    81     none    0.29639    78     none    0.0048828

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.148909
Type: lstm Hidden size: 81 Activation: none Dropout: 0.296389
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 81.000000 MSE: 0.088011
SL: 1.080996e-02 HS: Saving to file: bayesopt/bayesopt_0.088011.mat
|  111 | Accept |    0.088011 |      13.067 |    0.040542 |    0.049705 |           79 |            2 |     0.013963 |     0.029157 |           92 |         relu |      0.14891 |           81 |         none |      0.29639 |           78 |         none |    0.0048828 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          71              3             2.0088              0.017543        60     relu    0.1027    21     none    0.094306    46     none    0.12607

Type: lstm Hidden size: 60 Activation: relu Dropout: 0.102704
Type: lstm Hidden size: 21 Activation: none Dropout: 0.094306
Type: lstm Hidden size: 46 Activation: none Dropout: 0.126069
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 60 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             9% dropout
     7   ''   LSTM                LSTM with 46 hidden units
     8   ''   Dropout             13% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 60 ME: 21.000000 MSE: 46.000000
SL: 4.962664e-02 HS: 3.455523e-03 ME: Saving to file: bayesopt/bayesopt_0.049627.mat
|  112 | Accept |    0.049627 |       9.312 |    0.040542 |    0.049703 |           71 |            3 |       2.0088 |     0.017543 |           60 |         relu |       0.1027 |           21 |         none |     0.094306 |           46 |         none |      0.12607 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          89              2             0.10299             0.022711        76     relu    0.31073    14     none    0.045129    40     none    0.019879

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.310731
Type: lstm Hidden size: 14 Activation: none Dropout: 0.045129
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 14.000000 MSE: 0.064698
SL: 6.294481e-03 HS: Saving to file: bayesopt/bayesopt_0.064698.mat
|  113 | Accept |    0.064698 |      6.6521 |    0.040542 |    0.049687 |           89 |            2 |      0.10299 |     0.022711 |           76 |         relu |      0.31073 |           14 |         none |     0.045129 |           40 |         none |     0.019879 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          43              3            0.078137             0.011607        69     relu    0.4256    38     none    0.052142     5     none    0.38161

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.425603
Type: lstm Hidden size: 38 Activation: none Dropout: 0.052142
Type: lstm Hidden size: 5 Activation: none Dropout: 0.381614
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   Dropout             5% dropout
     7   ''   LSTM                LSTM with 5 hidden units
     8   ''   Dropout             38% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 38.000000 MSE: 5.000000
SL: 9.765308e-02 HS: 1.382864e-02 ME: Saving to file: bayesopt/bayesopt_0.097653.mat
|  114 | Accept |    0.097653 |      9.9661 |    0.040542 |    0.049861 |           43 |            3 |     0.078137 |     0.011607 |           69 |         relu |       0.4256 |           38 |         none |     0.052142 |            5 |         none |      0.38161 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          97              3            0.010345             0.025351        63     relu    0.32129    26     none    0.10959     9     none    0.48933

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.321286
Type: lstm Hidden size: 26 Activation: none Dropout: 0.109588
Type: lstm Hidden size: 9 Activation: none Dropout: 0.489335
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 26 hidden units
     6   ''   Dropout             11% dropout
     7   ''   LSTM                LSTM with 9 hidden units
     8   ''   Dropout             49% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 26.000000 MSE: 9.000000
SL: 1.106530e-01 HS: 2.829901e-02 ME: Saving to file: bayesopt/bayesopt_0.11065.mat
|  115 | Accept |     0.11065 |      8.4614 |    0.040542 |    0.048056 |           97 |            3 |     0.010345 |     0.025351 |           63 |         relu |      0.32129 |           26 |         none |      0.10959 |            9 |         none |      0.48933 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2      drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _________    ___    ____    _______

          83              2            0.087366             0.012658        67     relu    0.21884    12     none    0.0023493    99     none    0.16062

Type: lstm Hidden size: 67 Activation: relu Dropout: 0.218837
Type: lstm Hidden size: 12 Activation: none Dropout: 0.002349
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 67 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             22% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Dropout             0% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 67 ME: 12.000000 MSE: 0.053256
SL: 4.391296e-03 HS: Saving to file: bayesopt/bayesopt_0.053256.mat
|  116 | Accept |    0.053256 |      5.7189 |    0.040542 |    0.048406 |           83 |            2 |     0.087366 |     0.012658 |           67 |         relu |      0.21884 |           12 |         none |    0.0023493 |           99 |         none |      0.16062 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          88              2            0.0012043            0.011223        41     relu    0.48158    12     none    0.14088    95     none    0.41762

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.481578
Type: lstm Hidden size: 12 Activation: none Dropout: 0.140883
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Dropout             14% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 12.000000 MSE: 0.070527
SL: 7.408118e-03 HS: Saving to file: bayesopt/bayesopt_0.070527.mat
|  117 | Accept |    0.070527 |      5.0684 |    0.040542 |    0.048474 |           88 |            2 |    0.0012043 |     0.011223 |           41 |         relu |      0.48158 |           12 |         none |      0.14088 |           95 |         none |      0.41762 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          97              3             0.47471            0.0085288        43     relu    0.43104    47     none    0.33623    99     none    0.40847

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.431044
Type: lstm Hidden size: 47 Activation: none Dropout: 0.336227
Type: lstm Hidden size: 99 Activation: none Dropout: 0.408473
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 47 hidden units
     6   ''   Dropout             34% dropout
     7   ''   LSTM                LSTM with 99 hidden units
     8   ''   Dropout             41% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 47.000000 MSE: 99.000000
SL: 4.812644e-02 HS: 3.265706e-03 ME: Saving to file: bayesopt/bayesopt_0.048126.mat
|  118 | Accept |    0.048126 |      12.517 |    0.040542 |    0.048295 |           97 |            3 |      0.47471 |    0.0085288 |           43 |         relu |      0.43104 |           47 |         none |      0.33623 |           99 |         none |      0.40847 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          48              2            0.038184             0.025554        64     relu    0.23048    13     none    0.087933    87     none    0.064026

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.230477
Type: lstm Hidden size: 13 Activation: none Dropout: 0.087933
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   Dropout             9% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 13.000000 MSE: 0.075355
SL: 8.326231e-03 HS: Saving to file: bayesopt/bayesopt_0.075355.mat
|  119 | Accept |    0.075355 |      6.6095 |    0.040542 |    0.048402 |           48 |            2 |     0.038184 |     0.025554 |           64 |         relu |      0.23048 |           13 |         none |     0.087933 |           87 |         none |     0.064026 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          70              2             6.7386             0.0053715        58     relu    0.42259    23     none    0.098227    88     none    0.19511

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.422591
Type: lstm Hidden size: 23 Activation: none Dropout: 0.098227
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 23 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 23.000000 MSE: 0.055120
SL: 4.348139e-03 HS: Saving to file: bayesopt/bayesopt_0.05512.mat
|  120 | Accept |     0.05512 |       6.673 |    0.040542 |    0.048159 |           70 |            2 |       6.7386 |    0.0053715 |           58 |         relu |      0.42259 |           23 |         none |     0.098227 |           88 |         none |      0.19511 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          97              2            0.0015923            0.014345        64     relu    0.079106    49     none    0.37058    99     none    0.12849

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.079106
Type: lstm Hidden size: 49 Activation: none Dropout: 0.370583
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   Dropout             37% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 49.000000 MSE: 0.070292
SL: 7.557407e-03 HS: Saving to file: bayesopt/bayesopt_0.070292.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  121 | Accept |    0.070292 |      7.5575 |    0.040542 |    0.048049 |           97 |            2 |    0.0015923 |     0.014345 |           64 |         relu |     0.079106 |           49 |         none |      0.37058 |           99 |         none |      0.12849 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ______    ___    ____    _______

          58              3             0.13403             0.008415        59     relu    0.055434    15     none    0.3363    88     none    0.09776

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.055434
Type: lstm Hidden size: 15 Activation: none Dropout: 0.336305
Type: lstm Hidden size: 88 Activation: none Dropout: 0.097760
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 15 hidden units
     6   ''   Dropout             34% dropout
     7   ''   LSTM                LSTM with 88 hidden units
     8   ''   Dropout             10% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 15.000000 MSE: 88.000000
SL: 8.684660e-02 HS: 1.220017e-02 ME: Saving to file: bayesopt/bayesopt_0.086847.mat
|  122 | Accept |    0.086847 |        12.1 |    0.040542 |     0.04873 |           58 |            3 |      0.13403 |     0.008415 |           59 |         relu |     0.055434 |           15 |         none |       0.3363 |           88 |         none |      0.09776 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          96              2             0.52768             0.00569         91     relu    0.48341     7     none    0.27549    14     none    0.063664

Type: lstm Hidden size: 91 Activation: relu Dropout: 0.483408
Type: lstm Hidden size: 7 Activation: none Dropout: 0.275491
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 91 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 7 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 91 ME: 7.000000 MSE: 0.060097
SL: 5.659906e-03 HS: Saving to file: bayesopt/bayesopt_0.060097.mat
|  123 | Accept |    0.060097 |      7.5779 |    0.040542 |     0.04911 |           96 |            2 |      0.52768 |      0.00569 |           91 |         relu |      0.48341 |            7 |         none |      0.27549 |           14 |         none |     0.063664 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          81              2             2.6979             0.0075947        50     relu    0.20786    23     none    0.034614     7     none    0.19454

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.207860
Type: lstm Hidden size: 23 Activation: none Dropout: 0.034614
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 23 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 23.000000 MSE: 0.058069
SL: 5.077014e-03 HS: Saving to file: bayesopt/bayesopt_0.058069.mat
|  124 | Accept |    0.058069 |      5.4844 |    0.040542 |    0.051638 |           81 |            2 |       2.6979 |    0.0075947 |           50 |         relu |      0.20786 |           23 |         none |     0.034614 |            7 |         none |      0.19454 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ______

          97              2             3.5769             0.0033203        77     relu    0.14643    42     none    0.024374    76     none    0.3992

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.146430
Type: lstm Hidden size: 42 Activation: none Dropout: 0.024374
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 42.000000 MSE: 0.062630
SL: 5.897528e-03 HS: Saving to file: bayesopt/bayesopt_0.06263.mat
|  125 | Accept |     0.06263 |      8.0293 |    0.040542 |    0.049349 |           97 |            2 |       3.5769 |    0.0033203 |           77 |         relu |      0.14643 |           42 |         none |     0.024374 |           76 |         none |       0.3992 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              2             5.1347              0.00914         50     relu    0.15386    42     none    0.40354    80     none    0.34052

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.153856
Type: lstm Hidden size: 42 Activation: none Dropout: 0.403539
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             40% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 42.000000 MSE: 0.066592
SL: 6.668656e-03 HS: Saving to file: bayesopt/bayesopt_0.066592.mat
|  126 | Accept |    0.066592 |      6.3765 |    0.040542 |    0.049974 |           90 |            2 |       5.1347 |      0.00914 |           50 |         relu |      0.15386 |           42 |         none |      0.40354 |           80 |         none |      0.34052 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          91              2             6.2224             0.0033131        44     relu    0.14913     2     none    0.49908    24     none    0.13945

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.149133
Type: lstm Hidden size: 2 Activation: none Dropout: 0.499084
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 2 hidden units
     6   ''   Dropout             50% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 2.000000 MSE: 0.108602
SL: 1.653362e-02 HS: Saving to file: bayesopt/bayesopt_0.1086.mat
|  127 | Accept |      0.1086 |       5.001 |    0.040542 |    0.050006 |           91 |            2 |       6.2224 |    0.0033131 |           44 |         relu |      0.14913 |            2 |         none |      0.49908 |           24 |         none |      0.13945 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          36              3             2.3256             0.0055893        95     relu    0.37994    19     none    0.24676     2     tanh    0.42486

Type: lstm Hidden size: 95 Activation: relu Dropout: 0.379937
Type: lstm Hidden size: 19 Activation: none Dropout: 0.246756
Type: lstm Hidden size: 2 Activation: tanh Dropout: 0.424857
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 95 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   Dropout             25% dropout
     7   ''   LSTM                LSTM with 2 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             42% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 95 ME: 19.000000 MSE: 2.000000
SL: 1.878435e-01 HS: 4.796600e-02 ME: Saving to file: bayesopt/bayesopt_0.18784.mat
|  128 | Accept |     0.18784 |      11.422 |    0.040542 |    0.040575 |           36 |            3 |       2.3256 |    0.0055893 |           95 |         relu |      0.37994 |           19 |         none |      0.24676 |            2 |         tanh |      0.42486 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          97              2             0.95181             0.006084        64     relu    0.090388    13     none    0.28909    31     none    0.15017

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.090388
Type: lstm Hidden size: 13 Activation: none Dropout: 0.289091
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   Dropout             29% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 13.000000 MSE: 0.057295
SL: 4.817016e-03 HS: Saving to file: bayesopt/bayesopt_0.057295.mat
|  129 | Accept |    0.057295 |       6.412 |    0.040542 |    0.048502 |           97 |            2 |      0.95181 |     0.006084 |           64 |         relu |     0.090388 |           13 |         none |      0.28909 |           31 |         none |      0.15017 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          67              3             1.4962              0.010924        47     relu    0.23231    91     none    0.19405    82     none    0.4356

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.232306
Type: lstm Hidden size: 91 Activation: none Dropout: 0.194051
Type: lstm Hidden size: 82 Activation: none Dropout: 0.435596
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   Dropout             19% dropout
     7   ''   LSTM                LSTM with 82 hidden units
     8   ''   Dropout             44% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 91.000000 MSE: 82.000000
SL: 6.456997e-02 HS: 6.146605e-03 ME: Saving to file: bayesopt/bayesopt_0.06457.mat
|  130 | Accept |     0.06457 |      14.411 |    0.040542 |     0.04845 |           67 |            3 |       1.4962 |     0.010924 |           47 |         relu |      0.23231 |           91 |         none |      0.19405 |           82 |         none |       0.4356 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          85              2             5.6768              0.001523        58     relu    0.42375    55     none    0.27232    75     tanh    0.20869

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.423754
Type: lstm Hidden size: 55 Activation: none Dropout: 0.272319
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 55 hidden units
     6   ''   Dropout             27% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 55.000000 MSE: 0.070820
SL: 7.409606e-03 HS: Saving to file: bayesopt/bayesopt_0.07082.mat
|  131 | Accept |     0.07082 |       6.842 |    0.040542 |    0.048668 |           85 |            2 |       5.6768 |     0.001523 |           58 |         relu |      0.42375 |           55 |         none |      0.27232 |           75 |         tanh |      0.20869 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          29              2             5.4549              0.012027        70     relu    0.21388    33     none    0.45831    42     none    0.18283

Type: lstm Hidden size: 70 Activation: relu Dropout: 0.213878
Type: lstm Hidden size: 33 Activation: none Dropout: 0.458309
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 70 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 33 hidden units
     6   ''   Dropout             46% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 70 ME: 33.000000 MSE: 0.078505
SL: 1.050201e-02 HS: Saving to file: bayesopt/bayesopt_0.078505.mat
|  132 | Accept |    0.078505 |      9.4533 |    0.040542 |    0.048619 |           29 |            2 |       5.4549 |     0.012027 |           70 |         relu |      0.21388 |           33 |         none |      0.45831 |           42 |         none |      0.18283 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______

          39              3             4.4004              0.031454        44     relu    0.087861    48     none    0.11319    66     none    0.3389

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.087861
Type: lstm Hidden size: 48 Activation: none Dropout: 0.113192
Type: lstm Hidden size: 66 Activation: none Dropout: 0.338902
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 48 hidden units
     6   ''   Dropout             11% dropout
     7   ''   LSTM                LSTM with 66 hidden units
     8   ''   Dropout             34% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 48.000000 MSE: 66.000000
SL: 6.126612e-02 HS: 5.565086e-03 ME: Saving to file: bayesopt/bayesopt_0.061266.mat
|  133 | Accept |    0.061266 |      12.334 |    0.040542 |    0.048619 |           39 |            3 |       4.4004 |     0.031454 |           44 |         relu |     0.087861 |           48 |         none |      0.11319 |           66 |         none |       0.3389 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          96              2             0.07239            0.0077157        72     relu    0.27638    19     none    0.41822    68     none    0.17039

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.276377
Type: lstm Hidden size: 19 Activation: none Dropout: 0.418221
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 19.000000 MSE: 0.052779
SL: 4.209819e-03 HS: Saving to file: bayesopt/bayesopt_0.052779.mat
|  134 | Accept |    0.052779 |      6.9095 |    0.040542 |    0.048593 |           96 |            2 |      0.07239 |    0.0077157 |           72 |         relu |      0.27638 |           19 |         none |      0.41822 |           68 |         none |      0.17039 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              3            0.023512            0.0097404        40     relu    0.36933    14     none    0.28092     7     tanh    0.35684

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.369332
Type: lstm Hidden size: 14 Activation: none Dropout: 0.280923
Type: lstm Hidden size: 7 Activation: tanh Dropout: 0.356837
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             28% dropout
     7   ''   LSTM                LSTM with 7 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             36% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 14.000000 MSE: 7.000000
SL: 7.406155e-02 HS: 8.112022e-03 ME: Saving to file: bayesopt/bayesopt_0.074062.mat
|  135 | Accept |    0.074062 |      6.9528 |    0.040542 |    0.048563 |           90 |            3 |     0.023512 |    0.0097404 |           40 |         relu |      0.36933 |           14 |         none |      0.28092 |            7 |         tanh |      0.35684 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          76              2            0.0052955           0.0039177        91     relu    0.29289    16     none    0.1979     9     none    0.11114

Type: lstm Hidden size: 91 Activation: relu Dropout: 0.292887
Type: lstm Hidden size: 16 Activation: none Dropout: 0.197903
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 91 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             29% dropout
     5   ''   LSTM                LSTM with 16 hidden units
     6   ''   Dropout             20% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 91 ME: 16.000000 MSE: 0.099340
SL: 1.845939e-02 HS: Saving to file: bayesopt/bayesopt_0.09934.mat
|  136 | Accept |     0.09934 |      8.3931 |    0.040542 |    0.048492 |           76 |            2 |    0.0052955 |    0.0039177 |           91 |         relu |      0.29289 |           16 |         none |       0.1979 |            9 |         none |      0.11114 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          92              2             7.1267              0.015939        29     relu    0.26392    21     none    0.16646    17     tanh    0.48323

Type: lstm Hidden size: 29 Activation: relu Dropout: 0.263923
Type: lstm Hidden size: 21 Activation: none Dropout: 0.166460
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 29 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             17% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 29 ME: 21.000000 MSE: 0.071416
SL: 8.389006e-03 HS: Saving to file: bayesopt/bayesopt_0.071416.mat
|  137 | Accept |    0.071416 |      4.9802 |    0.040542 |    0.048531 |           92 |            2 |       7.1267 |     0.015939 |           29 |         relu |      0.26392 |           21 |         none |      0.16646 |           17 |         tanh |      0.48323 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          38              2             0.27134            0.0066295        84     relu    0.42931    39     none    0.081959     9     tanh    0.19187

Type: lstm Hidden size: 84 Activation: relu Dropout: 0.429306
Type: lstm Hidden size: 39 Activation: none Dropout: 0.081959
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 84 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             8% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 84 ME: 39.000000 MSE: 0.073895
SL: 8.291516e-03 HS: Saving to file: bayesopt/bayesopt_0.073895.mat
|  138 | Accept |    0.073895 |      9.7204 |    0.040542 |    0.049881 |           38 |            2 |      0.27134 |    0.0066295 |           84 |         relu |      0.42931 |           39 |         none |     0.081959 |            9 |         tanh |      0.19187 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________

          71              3             3.9877              0.010319        34     relu    0.081038     8     none    0.39073    44     none    0.001033

Type: lstm Hidden size: 34 Activation: relu Dropout: 0.081038
Type: lstm Hidden size: 8 Activation: none Dropout: 0.390735
Type: lstm Hidden size: 44 Activation: none Dropout: 0.001033
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 34 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             39% dropout
     7   ''   LSTM                LSTM with 44 hidden units
     8   ''   Dropout             0% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 34 ME: 8.000000 MSE: 44.000000
SL: 6.139665e-02 HS: 5.543404e-03 ME: Saving to file: bayesopt/bayesopt_0.061397.mat
|  139 | Accept |    0.061397 |      7.7979 |    0.040542 |     0.04992 |           71 |            3 |       3.9877 |     0.010319 |           34 |         relu |     0.081038 |            8 |         none |      0.39073 |           44 |         none |     0.001033 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    _______

          79              2             3.0554              0.021091        10     relu    0.0050338    29     none    0.42179    88     none    0.09465

Type: lstm Hidden size: 10 Activation: relu Dropout: 0.005034
Type: lstm Hidden size: 29 Activation: none Dropout: 0.421795
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 10 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 10 ME: 29.000000 MSE: 0.253588
SL: 1.074253e-01 HS: Saving to file: bayesopt/bayesopt_0.25359.mat
|  140 | Accept |     0.25359 |      5.4494 |    0.040542 |     0.04992 |           79 |            2 |       3.0554 |     0.021091 |           10 |         relu |    0.0050338 |           29 |         none |      0.42179 |           88 |         none |      0.09465 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          87              2            0.076729            0.0063337        56     relu    0.16938    37     none    0.34884    26     none    0.27911

Type: lstm Hidden size: 56 Activation: relu Dropout: 0.169383
Type: lstm Hidden size: 37 Activation: none Dropout: 0.348843
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 56 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 37 hidden units
     6   ''   Dropout             35% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 56 ME: 37.000000 MSE: 0.056296
SL: 4.600426e-03 HS: Saving to file: bayesopt/bayesopt_0.056296.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  141 | Accept |    0.056296 |      6.2456 |    0.040542 |    0.049977 |           87 |            2 |     0.076729 |    0.0063337 |           56 |         relu |      0.16938 |           37 |         none |      0.34884 |           26 |         none |      0.27911 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          94              2            0.091111             0.081656        71     relu    0.18594    48     none    0.24123    24     none    0.051655

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.185942
Type: lstm Hidden size: 48 Activation: none Dropout: 0.241232
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 48 hidden units
     6   ''   Dropout             24% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 48.000000 MSE: 0.055535
SL: 4.342083e-03 HS: Saving to file: bayesopt/bayesopt_0.055535.mat
|  142 | Accept |    0.055535 |      7.8966 |    0.040542 |    0.050098 |           94 |            2 |     0.091111 |     0.081656 |           71 |         relu |      0.18594 |           48 |         none |      0.24123 |           24 |         none |     0.051655 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          50              2             2.6163              0.099021        79     relu    0.097764    12     none    0.39533    35     none    0.11141

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.097764
Type: lstm Hidden size: 12 Activation: none Dropout: 0.395335
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Dropout             40% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 12.000000 MSE: 0.095238
SL: 1.332400e-02 HS: Saving to file: bayesopt/bayesopt_0.095238.mat
|  143 | Accept |    0.095238 |      7.3136 |    0.040542 |    0.050056 |           50 |            2 |       2.6163 |     0.099021 |           79 |         relu |     0.097764 |           12 |         none |      0.39533 |           35 |         none |      0.11141 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          94              2            0.0071861           0.0064159        10     relu    0.49032    98     none    0.17277    66     tanh    0.29967

Type: lstm Hidden size: 10 Activation: relu Dropout: 0.490324
Type: lstm Hidden size: 98 Activation: none Dropout: 0.172771
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 10 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   Dropout             17% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 10 ME: 98.000000 MSE: 0.078420
SL: 8.615694e-03 HS: Saving to file: bayesopt/bayesopt_0.07842.mat
|  144 | Accept |     0.07842 |      7.7089 |    0.040542 |    0.050161 |           94 |            2 |    0.0071861 |    0.0064159 |           10 |         relu |      0.49032 |           98 |         none |      0.17277 |           66 |         tanh |      0.29967 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          80              3              7.74              0.0095035        52     relu    0.051598    31     none    0.016943    67     none    0.47571

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.051598
Type: lstm Hidden size: 31 Activation: none Dropout: 0.016943
Type: lstm Hidden size: 67 Activation: none Dropout: 0.475713
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             2% dropout
     7   ''   LSTM                LSTM with 67 hidden units
     8   ''   Dropout             48% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 31.000000 MSE: 67.000000
SL: 6.241500e-02 HS: 5.908062e-03 ME: Saving to file: bayesopt/bayesopt_0.062415.mat
|  145 | Accept |    0.062415 |      8.6016 |    0.040542 |    0.050074 |           80 |            3 |         7.74 |    0.0095035 |           52 |         relu |     0.051598 |           31 |         none |     0.016943 |           67 |         none |      0.47571 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          81              2             1.1828              0.05313         68     relu    0.46644    69     none    0.24708    56     none    0.40457

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.466444
Type: lstm Hidden size: 69 Activation: none Dropout: 0.247076
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 69 hidden units
     6   ''   Dropout             25% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 69.000000 MSE: 0.186446
SL: 4.701082e-02 HS: Saving to file: bayesopt/bayesopt_0.18645.mat
|  146 | Accept |     0.18645 |      7.6952 |    0.040542 |    0.048481 |           81 |            2 |       1.1828 |      0.05313 |           68 |         relu |      0.46644 |           69 |         none |      0.24708 |           56 |         none |      0.40457 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          73              2            0.0024239            0.074612        90     relu    0.033661    25     none    0.21085    44     none    0.34017

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.033661
Type: lstm Hidden size: 25 Activation: none Dropout: 0.210849
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 25 hidden units
     6   ''   Dropout             21% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 25.000000 MSE: 0.065406
SL: 6.039676e-03 HS: Saving to file: bayesopt/bayesopt_0.065406.mat
|  147 | Accept |    0.065406 |      8.7292 |    0.040542 |    0.048462 |           73 |            2 |    0.0024239 |     0.074612 |           90 |         relu |     0.033661 |           25 |         none |      0.21085 |           44 |         none |      0.34017 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          20              2            0.0076125            0.03779         79     relu    0.06144    70     none    0.36752     5     none    0.059706

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.061440
Type: lstm Hidden size: 70 Activation: none Dropout: 0.367522
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             37% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 70.000000 MSE: 0.084359
SL: 1.096979e-02 HS: Saving to file: bayesopt/bayesopt_0.084359.mat
|  148 | Accept |    0.084359 |      15.181 |    0.040542 |    0.049482 |           20 |            2 |    0.0076125 |      0.03779 |           79 |         relu |      0.06144 |           70 |         none |      0.36752 |            5 |         none |     0.059706 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2    hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _____    ___    ____    _______

          71              2            0.0042758            0.14196         79     relu    0.37987    31     none    0.281    11     none    0.13476

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.379865
Type: lstm Hidden size: 31 Activation: none Dropout: 0.281002
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 31.000000 MSE: 0.098541
SL: 1.470316e-02 HS: Saving to file: bayesopt/bayesopt_0.098541.mat
|  149 | Accept |    0.098541 |      8.1197 |    0.040542 |     0.04912 |           71 |            2 |    0.0042758 |      0.14196 |           79 |         relu |      0.37987 |           31 |         none |        0.281 |           11 |         none |      0.13476 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _________

          86              2            0.0017949            0.019399        41     relu    0.48932    34     none    0.016901    30     tanh    0.0056181

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.489323
Type: lstm Hidden size: 34 Activation: none Dropout: 0.016901
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 34 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 34.000000 MSE: 0.078680
SL: 8.373526e-03 HS: Saving to file: bayesopt/bayesopt_0.07868.mat
|  150 | Accept |     0.07868 |      5.4963 |    0.040542 |    0.050303 |           86 |            2 |    0.0017949 |     0.019399 |           41 |         relu |      0.48932 |           34 |         none |     0.016901 |           30 |         tanh |    0.0056181 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1       hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    __________    ___    ____    ________    ___    ____    _______

          98              2             0.26399             0.05078         57     relu    0.00076298     6     none    0.033821    51     none    0.38402

Type: lstm Hidden size: 57 Activation: relu Dropout: 0.000763
Type: lstm Hidden size: 6 Activation: none Dropout: 0.033821
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 57 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             0% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 57 ME: 6.000000 MSE: 0.059377
SL: 5.295042e-03 HS: Saving to file: bayesopt/bayesopt_0.059377.mat
|  151 | Accept |    0.059377 |      6.0541 |    0.040542 |    0.050442 |           98 |            2 |      0.26399 |      0.05078 |           57 |         relu |   0.00076298 |            6 |         none |     0.033821 |           51 |         none |      0.38402 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              3            0.0011903           0.0067763        92     relu    0.41068    79     none    0.15086    45     tanh    0.45082

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.410677
Type: lstm Hidden size: 79 Activation: none Dropout: 0.150861
Type: lstm Hidden size: 45 Activation: tanh Dropout: 0.450820
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 79 hidden units
     6   ''   Dropout             15% dropout
     7   ''   LSTM                LSTM with 45 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             45% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 79.000000 MSE: 45.000000
SL: 4.672304e-02 HS: 3.306259e-03 ME: Saving to file: bayesopt/bayesopt_0.046723.mat
|  152 | Accept |    0.046723 |      14.126 |    0.040542 |    0.050419 |           90 |            3 |    0.0011903 |    0.0067763 |           92 |         relu |      0.41068 |           79 |         none |      0.15086 |           45 |         tanh |      0.45082 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          98              2            0.002965             0.050249        68     relu    0.17897    42     none    0.071222    17     none    0.052349

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.178969
Type: lstm Hidden size: 42 Activation: none Dropout: 0.071222
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 42.000000 MSE: 0.063415
SL: 5.761575e-03 HS: Saving to file: bayesopt/bayesopt_0.063415.mat
|  153 | Accept |    0.063415 |      7.6891 |    0.040542 |    0.050461 |           98 |            2 |     0.002965 |     0.050249 |           68 |         relu |      0.17897 |           42 |         none |     0.071222 |           17 |         none |     0.052349 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          75              3             1.2605              0.013645        45     relu    0.18152    34     none    0.32942    63     none    0.42984

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.181519
Type: lstm Hidden size: 34 Activation: none Dropout: 0.329417
Type: lstm Hidden size: 63 Activation: none Dropout: 0.429844
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 34 hidden units
     6   ''   Dropout             33% dropout
     7   ''   LSTM                LSTM with 63 hidden units
     8   ''   Dropout             43% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 34.000000 MSE: 63.000000
SL: 6.083682e-02 HS: 5.619092e-03 ME: Saving to file: bayesopt/bayesopt_0.060837.mat
|  154 | Accept |    0.060837 |      10.611 |    0.040542 |    0.050613 |           75 |            3 |       1.2605 |     0.013645 |           45 |         relu |      0.18152 |           34 |         none |      0.32942 |           63 |         none |      0.42984 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    ________

          80              2            0.095073             0.028111        74     relu    0.0021452    15     none    0.17158    76     none    0.085225

Type: lstm Hidden size: 74 Activation: relu Dropout: 0.002145
Type: lstm Hidden size: 15 Activation: none Dropout: 0.171583
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 74 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             0% dropout
     5   ''   LSTM                LSTM with 15 hidden units
     6   ''   Dropout             17% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 74 ME: 15.000000 MSE: 0.078885
SL: 8.692630e-03 HS: Saving to file: bayesopt/bayesopt_0.078885.mat
|  155 | Accept |    0.078885 |      6.6214 |    0.040542 |    0.050823 |           80 |            2 |     0.095073 |     0.028111 |           74 |         relu |    0.0021452 |           15 |         none |      0.17158 |           76 |         none |     0.085225 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          56              2            0.002374             0.012153        55     relu    0.12366    11     none    0.26227    15     none    0.4042

Type: lstm Hidden size: 55 Activation: relu Dropout: 0.123664
Type: lstm Hidden size: 11 Activation: none Dropout: 0.262272
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 55 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   Dropout             26% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 55 ME: 11.000000 MSE: 0.093623
SL: 1.250417e-02 HS: Saving to file: bayesopt/bayesopt_0.093623.mat
|  156 | Accept |    0.093623 |      6.7289 |    0.040542 |    0.050133 |           56 |            2 |     0.002374 |     0.012153 |           55 |         relu |      0.12366 |           11 |         none |      0.26227 |           15 |         none |       0.4042 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    ________

          22              3            0.0016557           0.0058609        89     relu    0.18048    39     none    0.2523    18     tanh    0.097584

Type: lstm Hidden size: 89 Activation: relu Dropout: 0.180479
Type: lstm Hidden size: 39 Activation: none Dropout: 0.252298
Type: lstm Hidden size: 18 Activation: tanh Dropout: 0.097584
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 89 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             25% dropout
     7   ''   LSTM                LSTM with 18 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             10% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 89 ME: 39.000000 MSE: 18.000000
SL: 7.752923e-02 HS: 9.074162e-03 ME: Saving to file: bayesopt/bayesopt_0.077529.mat
|  157 | Accept |    0.077529 |       14.66 |    0.040542 |     0.05073 |           22 |            3 |    0.0016557 |    0.0058609 |           89 |         relu |      0.18048 |           39 |         none |       0.2523 |           18 |         tanh |     0.097584 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          47              2             0.22443             0.008461        54     relu    0.43516     2     none    0.30459    48     none    0.16577

Type: lstm Hidden size: 54 Activation: relu Dropout: 0.435161
Type: lstm Hidden size: 2 Activation: none Dropout: 0.304591
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 54 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 2 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 54 ME: 2.000000 MSE: 0.070866
SL: 7.841639e-03 HS: Saving to file: bayesopt/bayesopt_0.070866.mat
|  158 | Accept |    0.070866 |      6.9186 |    0.040542 |    0.051068 |           47 |            2 |      0.22443 |     0.008461 |           54 |         relu |      0.43516 |            2 |         none |      0.30459 |           48 |         none |      0.16577 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          49              2             0.24622            0.0048659        62     relu    0.41017    52     none    0.088141    33     none    0.21009

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.410174
Type: lstm Hidden size: 52 Activation: none Dropout: 0.088141
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 52 hidden units
     6   ''   Dropout             9% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 52.000000 MSE: 0.059141
SL: 4.725361e-03 HS: Saving to file: bayesopt/bayesopt_0.059141.mat
|  159 | Accept |    0.059141 |      7.6947 |    0.040542 |    0.048747 |           49 |            2 |      0.24622 |    0.0048659 |           62 |         relu |      0.41017 |           52 |         none |     0.088141 |           33 |         none |      0.21009 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              3            0.0020062            0.016509        42     relu    0.38735    81     none    0.34806    36     none    0.44148

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.387347
Type: lstm Hidden size: 81 Activation: none Dropout: 0.348057
Type: lstm Hidden size: 36 Activation: none Dropout: 0.441482
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   Dropout             35% dropout
     7   ''   LSTM                LSTM with 36 hidden units
     8   ''   Dropout             44% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 81.000000 MSE: 36.000000
SL: 1.095682e-01 HS: 1.560935e-02 ME: Saving to file: bayesopt/bayesopt_0.10957.mat
|  160 | Accept |     0.10957 |      10.724 |    0.040542 |    0.050935 |           90 |            3 |    0.0020062 |     0.016509 |           42 |         relu |      0.38735 |           81 |         none |      0.34806 |           36 |         none |      0.44148 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          43              3            0.001347            0.0052462        98     relu    0.34133    61     none    0.40005    63     tanh    0.21681

Type: lstm Hidden size: 98 Activation: relu Dropout: 0.341335
Type: lstm Hidden size: 61 Activation: none Dropout: 0.400053
Type: lstm Hidden size: 63 Activation: tanh Dropout: 0.216806
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 98 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 61 hidden units
     6   ''   Dropout             40% dropout
     7   ''   LSTM                LSTM with 63 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             22% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 98 ME: 61.000000 MSE: 63.000000
SL: 7.088217e-02 HS: 7.273221e-03 ME: Saving to file: bayesopt/bayesopt_0.070882.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  161 | Accept |    0.070882 |       16.68 |    0.040542 |    0.048898 |           43 |            3 |     0.001347 |    0.0052462 |           98 |         relu |      0.34133 |           61 |         none |      0.40005 |           63 |         tanh |      0.21681 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          70              3             3.2916              0.011393        46     relu    0.05781    68     none    0.34106    25     none    0.0063815

Type: lstm Hidden size: 46 Activation: relu Dropout: 0.057810
Type: lstm Hidden size: 68 Activation: none Dropout: 0.341063
Type: lstm Hidden size: 25 Activation: none Dropout: 0.006382
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 46 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   Dropout             34% dropout
     7   ''   LSTM                LSTM with 25 hidden units
     8   ''   Dropout             1% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 46 ME: 68.000000 MSE: 25.000000
SL: 6.486975e-02 HS: 6.264665e-03 ME: Saving to file: bayesopt/bayesopt_0.06487.mat
|  162 | Accept |     0.06487 |       10.32 |    0.040542 |    0.051457 |           70 |            3 |       3.2916 |     0.011393 |           46 |         relu |      0.05781 |           68 |         none |      0.34106 |           25 |         none |    0.0063815 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          76              2             0.25943            0.0069504        72     relu    0.46825    32     none    0.46094    64     none    0.44736

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.468249
Type: lstm Hidden size: 32 Activation: none Dropout: 0.460939
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 32 hidden units
     6   ''   Dropout             46% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 32.000000 MSE: 0.087313
SL: 1.093268e-02 HS: Saving to file: bayesopt/bayesopt_0.087313.mat
|  163 | Accept |    0.087313 |      8.1349 |    0.040542 |    0.051788 |           76 |            2 |      0.25943 |    0.0069504 |           72 |         relu |      0.46825 |           32 |         none |      0.46094 |           64 |         none |      0.44736 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          50              2            0.035386             0.23973         59     relu    0.089315    13     none    0.44233    48     none    0.47716

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.089315
Type: lstm Hidden size: 13 Activation: none Dropout: 0.442325
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   Dropout             44% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 13.000000 MSE: 0.112859
SL: 1.853928e-02 HS: Saving to file: bayesopt/bayesopt_0.11286.mat
|  164 | Accept |     0.11286 |      6.5169 |    0.040542 |    0.051909 |           50 |            2 |     0.035386 |      0.23973 |           59 |         relu |     0.089315 |           13 |         none |      0.44233 |           48 |         none |      0.47716 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          61              3             2.2632             0.0040019        32     relu    0.47758    37     none    0.08849    11     none    0.20709

Type: lstm Hidden size: 32 Activation: relu Dropout: 0.477577
Type: lstm Hidden size: 37 Activation: none Dropout: 0.088490
Type: lstm Hidden size: 11 Activation: none Dropout: 0.207092
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 32 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 37 hidden units
     6   ''   Dropout             9% dropout
     7   ''   LSTM                LSTM with 11 hidden units
     8   ''   Dropout             21% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 32 ME: 37.000000 MSE: 11.000000
SL: 8.729793e-02 HS: 1.054494e-02 ME: Saving to file: bayesopt/bayesopt_0.087298.mat
|  165 | Accept |    0.087298 |      7.1751 |    0.040542 |    0.051984 |           61 |            3 |       2.2632 |    0.0040019 |           32 |         relu |      0.47758 |           37 |         none |      0.08849 |           11 |         none |      0.20709 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    _______

          89              3             0.95455             0.041419        95     relu    0.0069003    31     none    0.49471    10     none    0.10526

Type: lstm Hidden size: 95 Activation: relu Dropout: 0.006900
Type: lstm Hidden size: 31 Activation: none Dropout: 0.494708
Type: lstm Hidden size: 10 Activation: none Dropout: 0.105264
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 95 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             49% dropout
     7   ''   LSTM                LSTM with 10 hidden units
     8   ''   Dropout             11% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 95 ME: 31.000000 MSE: 10.000000
SL: 1.848358e-01 HS: 4.658149e-02 ME: Saving to file: bayesopt/bayesopt_0.18484.mat
|  166 | Accept |     0.18484 |      10.095 |    0.040542 |    0.052956 |           89 |            3 |      0.95455 |     0.041419 |           95 |         relu |    0.0069003 |           31 |         none |      0.49471 |           10 |         none |      0.10526 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          85              2             0.39049            0.0081596        59     relu    0.44948    41     none    0.31584    58     none    0.089166

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.449479
Type: lstm Hidden size: 41 Activation: none Dropout: 0.315837
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 41 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 41.000000 MSE: 0.056138
SL: 4.445282e-03 HS: Saving to file: bayesopt/bayesopt_0.056138.mat
|  167 | Accept |    0.056138 |      6.4359 |    0.040542 |    0.052803 |           85 |            2 |      0.39049 |    0.0081596 |           59 |         relu |      0.44948 |           41 |         none |      0.31584 |           58 |         none |     0.089166 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          88              2             4.5859              0.010615        64     relu    0.12599    45     none    0.34321     2     tanh    0.45275

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.125989
Type: lstm Hidden size: 45 Activation: none Dropout: 0.343213
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 45 hidden units
     6   ''   Dropout             34% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 45.000000 MSE: 0.053371
SL: 4.133178e-03 HS: Saving to file: bayesopt/bayesopt_0.053371.mat
|  168 | Accept |    0.053371 |      6.9568 |    0.040542 |    0.052601 |           88 |            2 |       4.5859 |     0.010615 |           64 |         relu |      0.12599 |           45 |         none |      0.34321 |            2 |         tanh |      0.45275 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          43              2            0.076808            0.0065923        90     relu    0.1498    15     none    0.28119    15     tanh    0.49502

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.149795
Type: lstm Hidden size: 15 Activation: none Dropout: 0.281192
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 15 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 15.000000 MSE: 0.048335
SL: 3.453564e-03 HS: Saving to file: bayesopt/bayesopt_0.048335.mat
|  169 | Accept |    0.048335 |      8.3881 |    0.040542 |    0.052478 |           43 |            2 |     0.076808 |    0.0065923 |           90 |         relu |       0.1498 |           15 |         none |      0.28119 |           15 |         tanh |      0.49502 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          94              2             1.0083              0.029833        23     relu    0.32349     6     none    0.25648     7     tanh    0.093075

Type: lstm Hidden size: 23 Activation: relu Dropout: 0.323485
Type: lstm Hidden size: 6 Activation: none Dropout: 0.256484
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 23 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             26% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 23 ME: 6.000000 MSE: 0.066708
SL: 6.359884e-03 HS: Saving to file: bayesopt/bayesopt_0.066708.mat
|  170 | Accept |    0.066708 |      4.7233 |    0.040542 |    0.053132 |           94 |            2 |       1.0083 |     0.029833 |           23 |         relu |      0.32349 |            6 |         none |      0.25648 |            7 |         tanh |     0.093075 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          64              2             0.15335             0.032684        36     relu    0.39067    14     none    0.39555    29     tanh    0.48504

Type: lstm Hidden size: 36 Activation: relu Dropout: 0.390666
Type: lstm Hidden size: 14 Activation: none Dropout: 0.395547
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 36 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             40% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 36 ME: 14.000000 MSE: 0.070138
SL: 7.215497e-03 HS: Saving to file: bayesopt/bayesopt_0.070138.mat
|  171 | Accept |    0.070138 |      5.3858 |    0.040542 |    0.051168 |           64 |            2 |      0.15335 |     0.032684 |           36 |         relu |      0.39067 |           14 |         none |      0.39555 |           29 |         tanh |      0.48504 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          91              2             9.2193             0.0043972        75     relu    0.19749    10     none    0.44954    38     none    0.22971

Type: lstm Hidden size: 75 Activation: relu Dropout: 0.197489
Type: lstm Hidden size: 10 Activation: none Dropout: 0.449545
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 75 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   Dropout             45% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 75 ME: 10.000000 MSE: 0.072813
SL: 7.558122e-03 HS: Saving to file: bayesopt/bayesopt_0.072813.mat
|  172 | Accept |    0.072813 |      6.4874 |    0.040542 |    0.051204 |           91 |            2 |       9.2193 |    0.0043972 |           75 |         relu |      0.19749 |           10 |         none |      0.44954 |           38 |         none |      0.22971 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          56              2             0.25559            0.0083323        84     relu    0.12379    24     none    0.44998    16     tanh    0.1499

Type: lstm Hidden size: 84 Activation: relu Dropout: 0.123795
Type: lstm Hidden size: 24 Activation: none Dropout: 0.449984
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 84 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 24 hidden units
     6   ''   Dropout             45% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 84 ME: 24.000000 MSE: 0.069444
SL: 7.762325e-03 HS: Saving to file: bayesopt/bayesopt_0.069444.mat
|  173 | Accept |    0.069444 |      8.1573 |    0.040542 |    0.048553 |           56 |            2 |      0.25559 |    0.0083323 |           84 |         relu |      0.12379 |           24 |         none |      0.44998 |           16 |         tanh |       0.1499 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          59              3             1.2539              0.027519        43     relu    0.11878    13     none    0.15956    19     none    0.17964

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.118781
Type: lstm Hidden size: 13 Activation: none Dropout: 0.159556
Type: lstm Hidden size: 19 Activation: none Dropout: 0.179638
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   Dropout             16% dropout
     7   ''   LSTM                LSTM with 19 hidden units
     8   ''   Dropout             18% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 13.000000 MSE: 19.000000
SL: 4.773057e-02 HS: 3.335563e-03 ME: Saving to file: bayesopt/bayesopt_0.047731.mat
|  174 | Accept |    0.047731 |      8.7248 |    0.040542 |    0.049048 |           59 |            3 |       1.2539 |     0.027519 |           43 |         relu |      0.11878 |           13 |         none |      0.15956 |           19 |         none |      0.17964 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          58              3            0.0012891           0.0078858        87     relu    0.44168    12     none    0.20789    78     tanh    0.2241

Type: lstm Hidden size: 87 Activation: relu Dropout: 0.441679
Type: lstm Hidden size: 12 Activation: none Dropout: 0.207885
Type: lstm Hidden size: 78 Activation: tanh Dropout: 0.224101
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 87 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Dropout             21% dropout
     7   ''   LSTM                LSTM with 78 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             22% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 87 ME: 12.000000 MSE: 78.000000
SL: 1.017993e-01 HS: 1.573064e-02 ME: Saving to file: bayesopt/bayesopt_0.1018.mat
|  175 | Accept |      0.1018 |      12.702 |    0.040542 |    0.048602 |           58 |            3 |    0.0012891 |    0.0078858 |           87 |         relu |      0.44168 |           12 |         none |      0.20789 |           78 |         tanh |       0.2241 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          63              2             0.77201            0.0025602        60     relu    0.26531    77     none    0.14586    49     none    0.24138

Type: lstm Hidden size: 60 Activation: relu Dropout: 0.265313
Type: lstm Hidden size: 77 Activation: none Dropout: 0.145856
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 60 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   Dropout             15% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 60 ME: 77.000000 MSE: 0.067225
SL: 6.473313e-03 HS: Saving to file: bayesopt/bayesopt_0.067225.mat
|  176 | Accept |    0.067225 |      8.3816 |    0.040542 |    0.048562 |           63 |            2 |      0.77201 |    0.0025602 |           60 |         relu |      0.26531 |           77 |         none |      0.14586 |           49 |         none |      0.24138 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          73              2            0.004161             0.029218        96     relu    0.40171    28     none    0.037945     6     none    0.35129

Type: lstm Hidden size: 96 Activation: relu Dropout: 0.401713
Type: lstm Hidden size: 28 Activation: none Dropout: 0.037945
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 96 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 28 hidden units
     6   ''   Dropout             4% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 28.000000 MSE: 0.076613
SL: 8.365401e-03 HS: Saving to file: bayesopt/bayesopt_0.076613.mat
|  177 | Accept |    0.076613 |      10.015 |    0.040542 |    0.048715 |           73 |            2 |     0.004161 |     0.029218 |           96 |         relu |      0.40171 |           28 |         none |     0.037945 |            6 |         none |      0.35129 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          91              2            0.002393             0.059932        63     relu    0.41295     1     none    0.37282    46     none    0.024418

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.412953
Type: lstm Hidden size: 1 Activation: none Dropout: 0.372822
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 1 hidden units
     6   ''   Dropout             37% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 1.000000 MSE: 0.110922
SL: 1.822877e-02 HS: Saving to file: bayesopt/bayesopt_0.11092.mat
|  178 | Accept |     0.11092 |      5.1892 |    0.040542 |    0.047857 |           91 |            2 |     0.002393 |     0.059932 |           63 |         relu |      0.41295 |            1 |         none |      0.37282 |           46 |         none |     0.024418 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          71              2             2.8744             0.0040605        68     relu    0.03077    55     none    0.20044    30     none    0.028752

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.030770
Type: lstm Hidden size: 55 Activation: none Dropout: 0.200438
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 55 hidden units
     6   ''   Dropout             20% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 55.000000 MSE: 0.062890
SL: 5.716822e-03 HS: Saving to file: bayesopt/bayesopt_0.06289.mat
|  179 | Accept |     0.06289 |      8.3691 |    0.040542 |     0.04786 |           71 |            2 |       2.8744 |    0.0040605 |           68 |         relu |      0.03077 |           55 |         none |      0.20044 |           30 |         none |     0.028752 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          84              3             4.5898              0.015042        41     relu    0.28222     6     none    0.09947     8     tanh    0.32752

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.282217
Type: lstm Hidden size: 6 Activation: none Dropout: 0.099470
Type: lstm Hidden size: 8 Activation: tanh Dropout: 0.327517
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             10% dropout
     7   ''   LSTM                LSTM with 8 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             33% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 6.000000 MSE: 8.000000
SL: 6.628643e-02 HS: 6.462023e-03 ME: Saving to file: bayesopt/bayesopt_0.066286.mat
|  180 | Accept |    0.066286 |      6.3931 |    0.040542 |    0.047821 |           84 |            3 |       4.5898 |     0.015042 |           41 |         relu |      0.28222 |            6 |         none |      0.09947 |            8 |         tanh |      0.32752 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    _______

          63              3             5.2009              0.017551        37     relu    0.0025915    22     none    0.36112    10     none    0.17717

Type: lstm Hidden size: 37 Activation: relu Dropout: 0.002592
Type: lstm Hidden size: 22 Activation: none Dropout: 0.361119
Type: lstm Hidden size: 10 Activation: none Dropout: 0.177168
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 37 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             0% dropout
     5   ''   LSTM                LSTM with 22 hidden units
     6   ''   Dropout             36% dropout
     7   ''   LSTM                LSTM with 10 hidden units
     8   ''   Dropout             18% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 37 ME: 22.000000 MSE: 10.000000
SL: 1.833245e-01 HS: 4.563568e-02 ME: Saving to file: bayesopt/bayesopt_0.18332.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  181 | Accept |     0.18332 |      7.2138 |    0.040542 |    0.048356 |           63 |            3 |       5.2009 |     0.017551 |           37 |         relu |    0.0025915 |           22 |         none |      0.36112 |           10 |         none |      0.17717 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          61              2             0.07415             0.007082        32     relu    0.40568    84     none    0.058959    27     none    0.04415

Type: lstm Hidden size: 32 Activation: relu Dropout: 0.405683
Type: lstm Hidden size: 84 Activation: none Dropout: 0.058959
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 32 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 32 ME: 84.000000 MSE: 0.067323
SL: 6.603756e-03 HS: Saving to file: bayesopt/bayesopt_0.067323.mat
|  182 | Accept |    0.067323 |      7.4017 |    0.040542 |    0.048504 |           61 |            2 |      0.07415 |     0.007082 |           32 |         relu |      0.40568 |           84 |         none |     0.058959 |           27 |         none |      0.04415 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          67              3             0.64825             0.015634        75     relu    0.062404    64     none    0.23574     2     none    0.28315

Type: lstm Hidden size: 75 Activation: relu Dropout: 0.062404
Type: lstm Hidden size: 64 Activation: none Dropout: 0.235744
Type: lstm Hidden size: 2 Activation: none Dropout: 0.283148
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 75 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   Dropout             24% dropout
     7   ''   LSTM                LSTM with 2 hidden units
     8   ''   Dropout             28% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 75 ME: 64.000000 MSE: 2.000000
SL: 6.281428e-02 HS: 6.052181e-03 ME: Saving to file: bayesopt/bayesopt_0.062814.mat
|  183 | Accept |    0.062814 |      10.671 |    0.040542 |    0.048935 |           67 |            3 |      0.64825 |     0.015634 |           75 |         relu |     0.062404 |           64 |         none |      0.23574 |            2 |         none |      0.28315 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          41              3             1.8989             0.0092824        42     relu    0.23207    11     none    0.49884     4     none    0.32119

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.232073
Type: lstm Hidden size: 11 Activation: none Dropout: 0.498838
Type: lstm Hidden size: 4 Activation: none Dropout: 0.321185
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   Dropout             50% dropout
     7   ''   LSTM                LSTM with 4 hidden units
     8   ''   Dropout             32% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 11.000000 MSE: 4.000000
SL: 1.803290e-01 HS: 4.421792e-02 ME: Saving to file: bayesopt/bayesopt_0.18033.mat
|  184 | Accept |     0.18033 |      7.6511 |    0.040542 |    0.050488 |           41 |            3 |       1.8989 |    0.0092824 |           42 |         relu |      0.23207 |           11 |         none |      0.49884 |            4 |         none |      0.32119 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          18              3            0.012412             0.029133        44     relu    0.48957     1     none    0.10434    47     none    0.4912

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.489574
Type: lstm Hidden size: 1 Activation: none Dropout: 0.104338
Type: lstm Hidden size: 47 Activation: none Dropout: 0.491201
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 1 hidden units
     6   ''   Dropout             10% dropout
     7   ''   LSTM                LSTM with 47 hidden units
     8   ''   Dropout             49% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 1.000000 MSE: 47.000000
SL: 1.906957e-01 HS: 4.920546e-02 ME: Saving to file: bayesopt/bayesopt_0.1907.mat
|  185 | Accept |      0.1907 |      12.533 |    0.040542 |     0.05161 |           18 |            3 |     0.012412 |     0.029133 |           44 |         relu |      0.48957 |            1 |         none |      0.10434 |           47 |         none |       0.4912 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          95              2             0.05154            0.0052617        50     relu    0.26233    17     none    0.16219    92     none    0.26788

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.262326
Type: lstm Hidden size: 17 Activation: none Dropout: 0.162190
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 17 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 17.000000 MSE: 0.062853
SL: 5.933747e-03 HS: Saving to file: bayesopt/bayesopt_0.062853.mat
|  186 | Accept |    0.062853 |        6.02 |    0.040542 |    0.051692 |           95 |            2 |      0.05154 |    0.0052617 |           50 |         relu |      0.26233 |           17 |         none |      0.16219 |           92 |         none |      0.26788 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          96              2             9.1941             0.0098146        77     relu    0.16416    69     none    0.20559    12     none    0.45227

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.164157
Type: lstm Hidden size: 69 Activation: none Dropout: 0.205590
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 69 hidden units
     6   ''   Dropout             21% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 69.000000 MSE: 0.065582
SL: 6.354338e-03 HS: Saving to file: bayesopt/bayesopt_0.065582.mat
|  187 | Accept |    0.065582 |      9.4964 |    0.040542 |    0.040556 |           96 |            2 |       9.1941 |    0.0098146 |           77 |         relu |      0.16416 |           69 |         none |      0.20559 |           12 |         none |      0.45227 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          26              2             1.6691              0.001875        66     relu    0.12971    90     none    0.4623    61     tanh    0.27795

Type: lstm Hidden size: 66 Activation: relu Dropout: 0.129709
Type: lstm Hidden size: 90 Activation: none Dropout: 0.462302
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 66 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 90 hidden units
     6   ''   Dropout             46% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 66 ME: 90.000000 MSE: 0.072757
SL: 7.379188e-03 HS: Saving to file: bayesopt/bayesopt_0.072757.mat
|  188 | Accept |    0.072757 |      12.348 |    0.040542 |    0.051321 |           26 |            2 |       1.6691 |     0.001875 |           66 |         relu |      0.12971 |           90 |         none |       0.4623 |           61 |         tanh |      0.27795 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          78              2             0.18586             0.011719        64     relu    0.26927     8     none    0.075611    46     none    0.16946

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.269270
Type: lstm Hidden size: 8 Activation: none Dropout: 0.075611
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             8% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 8.000000 MSE: 0.049989
SL: 3.557237e-03 HS: Saving to file: bayesopt/bayesopt_0.049989.mat
|  189 | Accept |    0.049989 |      6.9122 |    0.040542 |    0.050007 |           78 |            2 |      0.18586 |     0.011719 |           64 |         relu |      0.26927 |            8 |         none |     0.075611 |           46 |         none |      0.16946 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          87              2            0.047596            0.0069591        67     relu    0.26609    82     none    0.095146    76     none    0.10536

Type: lstm Hidden size: 67 Activation: relu Dropout: 0.266090
Type: lstm Hidden size: 82 Activation: none Dropout: 0.095146
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 67 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 82 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 67 ME: 82.000000 MSE: 0.052785
SL: 4.589560e-03 HS: Saving to file: bayesopt/bayesopt_0.052785.mat
|  190 | Accept |    0.052785 |      8.5169 |    0.040542 |    0.049928 |           87 |            2 |     0.047596 |    0.0069591 |           67 |         relu |      0.26609 |           82 |         none |     0.095146 |           76 |         none |      0.10536 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _________

          65              3            0.016357            0.0096861        33     relu    0.32315    57     none    0.036526    82     none    0.0003705

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.323151
Type: lstm Hidden size: 57 Activation: none Dropout: 0.036526
Type: lstm Hidden size: 82 Activation: none Dropout: 0.000371
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             4% dropout
     7   ''   LSTM                LSTM with 82 hidden units
     8   ''   Dropout             0% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 57.000000 MSE: 82.000000
SL: 4.752865e-02 HS: 3.276332e-03 ME: Saving to file: bayesopt/bayesopt_0.047529.mat
|  191 | Accept |    0.047529 |       10.83 |    0.040542 |    0.049915 |           65 |            3 |     0.016357 |    0.0096861 |           33 |         relu |      0.32315 |           57 |         none |     0.036526 |           82 |         none |    0.0003705 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          90              3            0.043683             0.012666        72     relu    0.055461    99     none    0.10928    21     none    0.23931

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.055461
Type: lstm Hidden size: 99 Activation: none Dropout: 0.109278
Type: lstm Hidden size: 21 Activation: none Dropout: 0.239312
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 99 hidden units
     6   ''   Dropout             11% dropout
     7   ''   LSTM                LSTM with 21 hidden units
     8   ''   Dropout             24% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 99.000000 MSE: 21.000000
SL: 9.301512e-02 HS: 1.151554e-02 ME: Saving to file: bayesopt/bayesopt_0.093015.mat
|  192 | Accept |    0.093015 |      12.835 |    0.040542 |     0.05079 |           90 |            3 |     0.043683 |     0.012666 |           72 |         relu |     0.055461 |           99 |         none |      0.10928 |           21 |         none |      0.23931 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          56              2             1.1806             0.0061347        47     relu    0.47738    74     none    0.035372    89     none    0.088443

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.477384
Type: lstm Hidden size: 74 Activation: none Dropout: 0.035372
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 74 hidden units
     6   ''   Dropout             4% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 74.000000 MSE: 0.074654
SL: 8.046034e-03 HS: Saving to file: bayesopt/bayesopt_0.074654.mat
|  193 | Accept |    0.074654 |      8.7438 |    0.040542 |    0.050468 |           56 |            2 |       1.1806 |    0.0061347 |           47 |         relu |      0.47738 |           74 |         none |     0.035372 |           89 |         none |     0.088443 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          99              2              2.982             0.0054852        52     relu    0.45092    56     none    0.16033    58     none    0.1265

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.450923
Type: lstm Hidden size: 56 Activation: none Dropout: 0.160332
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 56 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 56.000000 MSE: 0.077598
SL: 9.869038e-03 HS: Saving to file: bayesopt/bayesopt_0.077598.mat
|  194 | Accept |    0.077598 |      7.2747 |    0.040542 |     0.05044 |           99 |            2 |        2.982 |    0.0054852 |           52 |         relu |      0.45092 |           56 |         none |      0.16033 |           58 |         none |       0.1265 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          94              2            0.0034614            0.011488        71     relu    0.12312    41     none    0.062968    84     none    0.17456

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.123117
Type: lstm Hidden size: 41 Activation: none Dropout: 0.062968
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 41 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 41.000000 MSE: 0.079062
SL: 8.679553e-03 HS: Saving to file: bayesopt/bayesopt_0.079062.mat
|  195 | Accept |    0.079062 |      7.3833 |    0.040542 |    0.050923 |           94 |            2 |    0.0034614 |     0.011488 |           71 |         relu |      0.12312 |           41 |         none |     0.062968 |           84 |         none |      0.17456 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          77              2            0.011686            0.0028658        92     relu    0.24056    65     none    0.013781     6     tanh    0.31641

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.240556
Type: lstm Hidden size: 65 Activation: none Dropout: 0.013781
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             24% dropout
     5   ''   LSTM                LSTM with 65 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 65.000000 MSE: 0.079072
SL: 9.773712e-03 HS: Saving to file: bayesopt/bayesopt_0.079072.mat
|  196 | Accept |    0.079072 |      11.086 |    0.040542 |     0.05122 |           77 |            2 |     0.011686 |    0.0028658 |           92 |         relu |      0.24056 |           65 |         none |     0.013781 |            6 |         tanh |      0.31641 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          61              2             7.5261              0.014336        42     relu    0.15298    86     none    0.10532    10     tanh    0.38296

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.152977
Type: lstm Hidden size: 86 Activation: none Dropout: 0.105322
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   Dropout             11% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 86.000000 MSE: 0.051074
SL: 3.783474e-03 HS: Saving to file: bayesopt/bayesopt_0.051074.mat
|  197 | Accept |    0.051074 |      7.5815 |    0.040542 |    0.051215 |           61 |            2 |       7.5261 |     0.014336 |           42 |         relu |      0.15298 |           86 |         none |      0.10532 |           10 |         tanh |      0.38296 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          95              3             0.0238             0.0082398        54     relu    0.13082    46     none    0.093811    76     none    0.19046

Type: lstm Hidden size: 54 Activation: relu Dropout: 0.130824
Type: lstm Hidden size: 46 Activation: none Dropout: 0.093811
Type: lstm Hidden size: 76 Activation: none Dropout: 0.190456
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 54 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 46 hidden units
     6   ''   Dropout             9% dropout
     7   ''   LSTM                LSTM with 76 hidden units
     8   ''   Dropout             19% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 54 ME: 46.000000 MSE: 76.000000
SL: 1.038158e-01 HS: 1.610770e-02 ME: Saving to file: bayesopt/bayesopt_0.10382.mat
|  198 | Accept |     0.10382 |      11.302 |    0.040542 |    0.051875 |           95 |            3 |       0.0238 |    0.0082398 |           54 |         relu |      0.13082 |           46 |         none |     0.093811 |           76 |         none |      0.19046 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          58              2             0.60892            0.0097826        54     relu    0.08508    44     none    0.078419    65     none    0.028882

Type: lstm Hidden size: 54 Activation: relu Dropout: 0.085080
Type: lstm Hidden size: 44 Activation: none Dropout: 0.078419
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 54 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 44 hidden units
     6   ''   Dropout             8% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 54 ME: 44.000000 MSE: 0.049766
SL: 3.645540e-03 HS: Saving to file: bayesopt/bayesopt_0.049766.mat
|  199 | Accept |    0.049766 |      7.7132 |    0.040542 |    0.042341 |           58 |            2 |      0.60892 |    0.0097826 |           54 |         relu |      0.08508 |           44 |         none |     0.078419 |           65 |         none |     0.028882 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2    hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _____    ___    ____    _________

          91              2             0.86527            0.0059867        52     relu    0.11968    58     none    0.208    88     tanh    0.0038201

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.119678
Type: lstm Hidden size: 58 Activation: none Dropout: 0.207995
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 58 hidden units
     6   ''   Dropout             21% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 58.000000 MSE: 0.047811
SL: 3.089645e-03 HS: Saving to file: bayesopt/bayesopt_0.047811.mat
|  200 | Accept |    0.047811 |      7.1018 |    0.040542 |    0.052302 |           91 |            2 |      0.86527 |    0.0059867 |           52 |         relu |      0.11968 |           58 |         none |        0.208 |           88 |         tanh |    0.0038201 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          67              2             0.43196             0.018137        45     relu    0.026772    12     none    0.22253    70     tanh    0.36959

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.026772
Type: lstm Hidden size: 12 Activation: none Dropout: 0.222532
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Dropout             22% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 12.000000 MSE: 0.079814
SL: 8.550718e-03 HS: Saving to file: bayesopt/bayesopt_0.079814.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  201 | Accept |    0.079814 |      5.6454 |    0.040542 |    0.040554 |           67 |            2 |      0.43196 |     0.018137 |           45 |         relu |     0.026772 |           12 |         none |      0.22253 |           70 |         tanh |      0.36959 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          74              3            0.017029            0.0052068        69     relu    0.048983     3     none    0.46883    49     tanh    0.26636

Type: lstm Hidden size: 69 Activation: relu Dropout: 0.048983
Type: lstm Hidden size: 3 Activation: none Dropout: 0.468835
Type: lstm Hidden size: 49 Activation: tanh Dropout: 0.266361
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 69 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 3 hidden units
     6   ''   Dropout             47% dropout
     7   ''   LSTM                LSTM with 49 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             27% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 69 ME: 3.000000 MSE: 49.000000
SL: 6.897246e-02 HS: 6.228052e-03 ME: Saving to file: bayesopt/bayesopt_0.068972.mat
|  202 | Accept |    0.068972 |      9.6431 |    0.040542 |    0.051597 |           74 |            3 |     0.017029 |    0.0052068 |           69 |         relu |     0.048983 |            3 |         none |      0.46883 |           49 |         tanh |      0.26636 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          93              2             2.1537              0.014365        76     relu    0.1281     6     none    0.14812     6     none    0.22195

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.128104
Type: lstm Hidden size: 6 Activation: none Dropout: 0.148118
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             15% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 6.000000 MSE: 0.189946
SL: 4.844650e-02 HS: Saving to file: bayesopt/bayesopt_0.18995.mat
|  203 | Accept |     0.18995 |      6.5229 |    0.040542 |    0.042196 |           93 |            2 |       2.1537 |     0.014365 |           76 |         relu |       0.1281 |            6 |         none |      0.14812 |            6 |         none |      0.22195 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          95              3            0.029745             0.001491        78     relu    0.27259    17     none    0.42549    65     tanh    0.31216

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.272589
Type: lstm Hidden size: 17 Activation: none Dropout: 0.425494
Type: lstm Hidden size: 65 Activation: tanh Dropout: 0.312164
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 17 hidden units
     6   ''   Dropout             43% dropout
     7   ''   LSTM                LSTM with 65 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             31% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 17.000000 MSE: 65.000000
SL: 9.903101e-02 HS: 1.367028e-02 ME: Saving to file: bayesopt/bayesopt_0.099031.mat
|  204 | Accept |    0.099031 |      10.422 |    0.040542 |    0.042036 |           95 |            3 |     0.029745 |     0.001491 |           78 |         relu |      0.27259 |           17 |         none |      0.42549 |           65 |         tanh |      0.31216 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          86              2             0.24827            0.0072394        42     relu    0.046104    72     none    0.32206    63     none    0.33575

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.046104
Type: lstm Hidden size: 72 Activation: none Dropout: 0.322064
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 72 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 72.000000 MSE: 0.083853
SL: 1.024967e-02 HS: Saving to file: bayesopt/bayesopt_0.083853.mat
|  205 | Accept |    0.083853 |      6.8164 |    0.040542 |    0.041332 |           86 |            2 |      0.24827 |    0.0072394 |           42 |         relu |     0.046104 |           72 |         none |      0.32206 |           63 |         none |      0.33575 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          41              3             2.5972              0.032483        58     relu    0.1563    42     none    0.077243     6     none    0.15398

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.156299
Type: lstm Hidden size: 42 Activation: none Dropout: 0.077243
Type: lstm Hidden size: 6 Activation: none Dropout: 0.153976
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             8% dropout
     7   ''   LSTM                LSTM with 6 hidden units
     8   ''   Dropout             15% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 42.000000 MSE: 6.000000
SL: 6.705343e-02 HS: 6.361220e-03 ME: Saving to file: bayesopt/bayesopt_0.067053.mat
|  206 | Accept |    0.067053 |      9.4346 |    0.040542 |    0.041936 |           41 |            3 |       2.5972 |     0.032483 |           58 |         relu |       0.1563 |           42 |         none |     0.077243 |            6 |         none |      0.15398 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          30              3            0.042062             0.013752        63     relu    0.27583    39     none    0.063742    77     none    0.079134

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.275827
Type: lstm Hidden size: 39 Activation: none Dropout: 0.063742
Type: lstm Hidden size: 77 Activation: none Dropout: 0.079134
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             6% dropout
     7   ''   LSTM                LSTM with 77 hidden units
     8   ''   Dropout             8% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 39.000000 MSE: 77.000000
SL: 7.298586e-02 HS: 7.781669e-03 ME: Saving to file: bayesopt/bayesopt_0.072986.mat
|  207 | Accept |    0.072986 |      12.971 |    0.040542 |    0.042597 |           30 |            3 |     0.042062 |     0.013752 |           63 |         relu |      0.27583 |           39 |         none |     0.063742 |           77 |         none |     0.079134 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          86              3            0.018105             0.028279        33     relu    0.17616    20     none    0.13704    17     none    0.11764

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.176157
Type: lstm Hidden size: 20 Activation: none Dropout: 0.137039
Type: lstm Hidden size: 17 Activation: none Dropout: 0.117636
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 20 hidden units
     6   ''   Dropout             14% dropout
     7   ''   LSTM                LSTM with 17 hidden units
     8   ''   Dropout             12% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 20.000000 MSE: 17.000000
SL: 6.150379e-02 HS: 5.411024e-03 ME: Saving to file: bayesopt/bayesopt_0.061504.mat
|  208 | Accept |    0.061504 |      6.4017 |    0.040542 |    0.042729 |           86 |            3 |     0.018105 |     0.028279 |           33 |         relu |      0.17616 |           20 |         none |      0.13704 |           17 |         none |      0.11764 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          38              2            0.059846            0.0088179        31     relu    0.47446    24     none    0.38054    25     tanh    0.37341

Type: lstm Hidden size: 31 Activation: relu Dropout: 0.474465
Type: lstm Hidden size: 24 Activation: none Dropout: 0.380538
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 31 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 24 hidden units
     6   ''   Dropout             38% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 31 ME: 24.000000 MSE: 0.069279
SL: 6.827640e-03 HS: Saving to file: bayesopt/bayesopt_0.069279.mat
|  209 | Accept |    0.069279 |       6.358 |    0.040542 |    0.049911 |           38 |            2 |     0.059846 |    0.0088179 |           31 |         relu |      0.47446 |           24 |         none |      0.38054 |           25 |         tanh |      0.37341 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ______

          30              2              6.857             0.0073832        39     relu    0.2087    57     none    0.15913    46     none    0.2177

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.208703
Type: lstm Hidden size: 57 Activation: none Dropout: 0.159126
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 57.000000 MSE: 0.060147
SL: 4.794479e-03 HS: Saving to file: bayesopt/bayesopt_0.060147.mat
|  210 | Accept |    0.060147 |      8.1215 |    0.040542 |    0.050371 |           30 |            2 |        6.857 |    0.0073832 |           39 |         relu |       0.2087 |           57 |         none |      0.15913 |           46 |         none |       0.2177 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          87              2            0.0096515           0.0080361        40     relu    0.24756    52     none    0.23661    63     tanh    0.22117

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.247560
Type: lstm Hidden size: 52 Activation: none Dropout: 0.236612
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 52 hidden units
     6   ''   Dropout             24% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 52.000000 MSE: 0.062846
SL: 5.889293e-03 HS: Saving to file: bayesopt/bayesopt_0.062846.mat
|  211 | Accept |    0.062846 |      6.1476 |    0.040542 |    0.042426 |           87 |            2 |    0.0096515 |    0.0080361 |           40 |         relu |      0.24756 |           52 |         none |      0.23661 |           63 |         tanh |      0.22117 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          92              2            0.0074717            0.041609        33     relu    0.21067    24     none    0.46072     5     tanh    0.23479

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.210668
Type: lstm Hidden size: 24 Activation: none Dropout: 0.460724
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 24 hidden units
     6   ''   Dropout             46% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 24.000000 MSE: 0.049151
SL: 3.829037e-03 HS: Saving to file: bayesopt/bayesopt_0.049151.mat
|  212 | Accept |    0.049151 |      4.9991 |    0.040542 |    0.042312 |           92 |            2 |    0.0074717 |     0.041609 |           33 |         relu |      0.21067 |           24 |         none |      0.46072 |            5 |         tanh |      0.23479 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          66              2             0.62313            0.0089552        62     relu    0.25686    47     none    0.21655    72     none    0.3706

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.256856
Type: lstm Hidden size: 47 Activation: none Dropout: 0.216548
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 47 hidden units
     6   ''   Dropout             22% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 47.000000 MSE: 0.055440
SL: 4.673775e-03 HS: Saving to file: bayesopt/bayesopt_0.05544.mat
|  213 | Accept |     0.05544 |      7.1827 |    0.040542 |    0.041758 |           66 |            2 |      0.62313 |    0.0089552 |           62 |         relu |      0.25686 |           47 |         none |      0.21655 |           72 |         none |       0.3706 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          96              3            0.090957             0.017106        62     relu    0.10791    60     none    0.45396    87     none    0.20714

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.107914
Type: lstm Hidden size: 60 Activation: none Dropout: 0.453961
Type: lstm Hidden size: 87 Activation: none Dropout: 0.207137
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Dropout             45% dropout
     7   ''   LSTM                LSTM with 87 hidden units
     8   ''   Dropout             21% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 60.000000 MSE: 87.000000
SL: 1.203733e-01 HS: 2.128511e-02 ME: Saving to file: bayesopt/bayesopt_0.12037.mat
|  214 | Accept |     0.12037 |      13.345 |    0.040542 |    0.050379 |           96 |            3 |     0.090957 |     0.017106 |           62 |         relu |      0.10791 |           60 |         none |      0.45396 |           87 |         none |      0.20714 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          67              2             0.64641             0.010224        26     relu    0.25283    29     none    0.42513    90     tanh    0.01568

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.252832
Type: lstm Hidden size: 29 Activation: none Dropout: 0.425133
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Dropout             43% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 29.000000 MSE: 0.067481
SL: 7.037768e-03 HS: Saving to file: bayesopt/bayesopt_0.067481.mat
|  215 | Accept |    0.067481 |      5.3339 |    0.040542 |    0.050405 |           67 |            2 |      0.64641 |     0.010224 |           26 |         relu |      0.25283 |           29 |         none |      0.42513 |           90 |         tanh |      0.01568 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ______

          84              2             6.5622             0.0041445        57     relu    0.053714    14     none    0.035673    56     none    0.2472

Type: lstm Hidden size: 57 Activation: relu Dropout: 0.053714
Type: lstm Hidden size: 14 Activation: none Dropout: 0.035673
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 57 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             4% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 57 ME: 14.000000 MSE: 0.072479
SL: 6.859087e-03 HS: Saving to file: bayesopt/bayesopt_0.072479.mat
|  216 | Accept |    0.072479 |      5.5057 |    0.040542 |    0.049887 |           84 |            2 |       6.5622 |    0.0041445 |           57 |         relu |     0.053714 |           14 |         none |     0.035673 |           56 |         none |       0.2472 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          35              3             0.13361            0.0064794        68     relu    0.28452    60     none    0.14275    40     tanh    0.096555

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.284520
Type: lstm Hidden size: 60 Activation: none Dropout: 0.142747
Type: lstm Hidden size: 40 Activation: tanh Dropout: 0.096555
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Dropout             14% dropout
     7   ''   LSTM                LSTM with 40 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             10% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 60.000000 MSE: 40.000000
SL: 8.177176e-02 HS: 1.021220e-02 ME: Saving to file: bayesopt/bayesopt_0.081772.mat
|  217 | Accept |    0.081772 |      12.445 |    0.040542 |    0.042959 |           35 |            3 |      0.13361 |    0.0064794 |           68 |         relu |      0.28452 |           60 |         none |      0.14275 |           40 |         tanh |     0.096555 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          50              2            0.027172             0.072681        52     relu    0.13733    20     none    0.3453    15     none    0.22567

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.137327
Type: lstm Hidden size: 20 Activation: none Dropout: 0.345298
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 20 hidden units
     6   ''   Dropout             35% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 20.000000 MSE: 0.075000
SL: 7.537120e-03 HS: Saving to file: bayesopt/bayesopt_0.075.mat
|  218 | Accept |       0.075 |      6.3232 |    0.040542 |    0.043004 |           50 |            2 |     0.027172 |     0.072681 |           52 |         relu |      0.13733 |           20 |         none |       0.3453 |           15 |         none |      0.22567 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          22              2             0.51195            0.0088028        91     relu    0.31371    25     none    0.32201    11     tanh    0.35593

Type: lstm Hidden size: 91 Activation: relu Dropout: 0.313705
Type: lstm Hidden size: 25 Activation: none Dropout: 0.322006
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 91 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 25 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 91 ME: 25.000000 MSE: 0.056262
SL: 4.453421e-03 HS: Saving to file: bayesopt/bayesopt_0.056262.mat
|  219 | Accept |    0.056262 |      10.476 |    0.040542 |    0.043532 |           22 |            2 |      0.51195 |    0.0088028 |           91 |         relu |      0.31371 |           25 |         none |      0.32201 |           11 |         tanh |      0.35593 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          80              2            0.081747            0.0062615        79     relu    0.23777    84     none    0.17608     1     tanh    0.34204

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.237771
Type: lstm Hidden size: 84 Activation: none Dropout: 0.176083
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             24% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   Dropout             18% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 84.000000 MSE: 0.053110
SL: 4.001704e-03 HS: Saving to file: bayesopt/bayesopt_0.05311.mat
|  220 | Accept |     0.05311 |      8.8854 |    0.040542 |    0.043356 |           80 |            2 |     0.081747 |    0.0062615 |           79 |         relu |      0.23777 |           84 |         none |      0.17608 |            1 |         tanh |      0.34204 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          84              2             0.13671             0.010752        73     relu    0.39525    37     none    0.070448    84     none    0.37581

Type: lstm Hidden size: 73 Activation: relu Dropout: 0.395255
Type: lstm Hidden size: 37 Activation: none Dropout: 0.070448
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 73 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 37 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 73 ME: 37.000000 MSE: 0.059032
SL: 5.475203e-03 HS: Saving to file: bayesopt/bayesopt_0.059032.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  221 | Accept |    0.059032 |      6.8115 |    0.040542 |    0.043989 |           84 |            2 |      0.13671 |     0.010752 |           73 |         relu |      0.39525 |           37 |         none |     0.070448 |           84 |         none |      0.37581 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          66              2             0.04166            0.0075638        43     relu    0.15152    14     none    0.054694    52     none    0.25264

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.151522
Type: lstm Hidden size: 14 Activation: none Dropout: 0.054694
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 14.000000 MSE: 0.078660
SL: 1.084816e-02 HS: Saving to file: bayesopt/bayesopt_0.07866.mat
|  222 | Accept |     0.07866 |      5.5443 |    0.040542 |    0.043515 |           66 |            2 |      0.04166 |    0.0075638 |           43 |         relu |      0.15152 |           14 |         none |     0.054694 |           52 |         none |      0.25264 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          65              2            0.0060196           0.0054957        53     relu    0.045481    12     none    0.057071    67     tanh    0.13249

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.045481
Type: lstm Hidden size: 12 Activation: none Dropout: 0.057071
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 12.000000 MSE: 0.044640
SL: 2.945321e-03 HS: Saving to file: bayesopt/bayesopt_0.04464.mat
|  223 | Accept |     0.04464 |      5.9105 |    0.040542 |    0.043478 |           65 |            2 |    0.0060196 |    0.0054957 |           53 |         relu |     0.045481 |           12 |         none |     0.057071 |           67 |         tanh |      0.13249 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________

          36              2             7.5416             0.0082262        47     relu    0.046529     8     none    0.26628    50     tanh    0.027647

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.046529
Type: lstm Hidden size: 8 Activation: none Dropout: 0.266283
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             27% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 8.000000 MSE: 0.057772
SL: 5.304648e-03 HS: Saving to file: bayesopt/bayesopt_0.057772.mat
|  224 | Accept |    0.057772 |       6.741 |    0.040542 |    0.043726 |           36 |            2 |       7.5416 |    0.0082262 |           47 |         relu |     0.046529 |            8 |         none |      0.26628 |           50 |         tanh |     0.027647 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          37              2             0.13298            0.0057029        96     relu    0.27146    55     none    0.022893    61     none    0.015849

Type: lstm Hidden size: 96 Activation: relu Dropout: 0.271457
Type: lstm Hidden size: 55 Activation: none Dropout: 0.022893
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 96 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 55 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 55.000000 MSE: 0.044927
SL: 3.116489e-03 HS: Saving to file: bayesopt/bayesopt_0.044927.mat
|  225 | Accept |    0.044927 |      10.929 |    0.040542 |    0.043589 |           37 |            2 |      0.13298 |    0.0057029 |           96 |         relu |      0.27146 |           55 |         none |     0.022893 |           61 |         none |     0.015849 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ______

          52              2             0.68385            0.0054363        63     relu    0.12145    30     none    0.066115    40     tanh    0.4411

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.121452
Type: lstm Hidden size: 30 Activation: none Dropout: 0.066115
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 30 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 30.000000 MSE: 0.052867
SL: 4.381764e-03 HS: Saving to file: bayesopt/bayesopt_0.052867.mat
|  226 | Accept |    0.052867 |      7.0372 |    0.040542 |    0.043791 |           52 |            2 |      0.68385 |    0.0054363 |           63 |         relu |      0.12145 |           30 |         none |     0.066115 |           40 |         tanh |       0.4411 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          65              2             1.8239              0.011136        53     relu    0.14837    42     none    0.078682    52     tanh    0.17433

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.148367
Type: lstm Hidden size: 42 Activation: none Dropout: 0.078682
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   Dropout             8% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 42.000000 MSE: 0.049004
SL: 3.488969e-03 HS: Saving to file: bayesopt/bayesopt_0.049004.mat
|  227 | Accept |    0.049004 |      6.5893 |    0.040542 |     0.04361 |           65 |            2 |       1.8239 |     0.011136 |           53 |         relu |      0.14837 |           42 |         none |     0.078682 |           52 |         tanh |      0.17433 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          87              2            0.0098623            0.029238        20     relu    0.25005    31     none    0.37885    27     tanh    0.011866

Type: lstm Hidden size: 20 Activation: relu Dropout: 0.250048
Type: lstm Hidden size: 31 Activation: none Dropout: 0.378853
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 20 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             38% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 20 ME: 31.000000 MSE: 0.077855
SL: 9.113501e-03 HS: Saving to file: bayesopt/bayesopt_0.077855.mat
|  228 | Accept |    0.077855 |      4.6233 |    0.040542 |    0.040554 |           87 |            2 |    0.0098623 |     0.029238 |           20 |         relu |      0.25005 |           31 |         none |      0.37885 |           27 |         tanh |     0.011866 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          28              2             2.5335             0.0078731        78     relu    0.16115    66     none    0.079653    16     none    0.03085

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.161152
Type: lstm Hidden size: 66 Activation: none Dropout: 0.079653
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 66 hidden units
     6   ''   Dropout             8% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 66.000000 MSE: 0.052988
SL: 4.166230e-03 HS: Saving to file: bayesopt/bayesopt_0.052988.mat
|  229 | Accept |    0.052988 |      10.944 |    0.040542 |    0.043795 |           28 |            2 |       2.5335 |    0.0078731 |           78 |         relu |      0.16115 |           66 |         none |     0.079653 |           16 |         none |      0.03085 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______

          68              2            0.0048554            0.048805        44     relu    0.079932    10     none    0.05977    12     tanh    0.2017

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.079932
Type: lstm Hidden size: 10 Activation: none Dropout: 0.059770
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 10.000000 MSE: 0.084347
SL: 1.072839e-02 HS: Saving to file: bayesopt/bayesopt_0.084347.mat
|  230 | Accept |    0.084347 |      5.5181 |    0.040542 |    0.043421 |           68 |            2 |    0.0048554 |     0.048805 |           44 |         relu |     0.079932 |           10 |         none |      0.05977 |           12 |         tanh |       0.2017 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          53              2            0.0036622           0.0059638        17     relu    0.43852    68     none    0.46858     6     tanh    0.40058

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.438520
Type: lstm Hidden size: 68 Activation: none Dropout: 0.468578
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   Dropout             47% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 68.000000 MSE: 0.069104
SL: 7.722077e-03 HS: Saving to file: bayesopt/bayesopt_0.069104.mat
|  231 | Accept |    0.069104 |      6.8518 |    0.040542 |    0.044759 |           53 |            2 |    0.0036622 |    0.0059638 |           17 |         relu |      0.43852 |           68 |         none |      0.46858 |            6 |         tanh |      0.40058 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          92              2            0.0043937           0.0063077        34     relu    0.099013    60     none    0.27679     6     tanh    0.15078

Type: lstm Hidden size: 34 Activation: relu Dropout: 0.099013
Type: lstm Hidden size: 60 Activation: none Dropout: 0.276791
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 34 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 34 ME: 60.000000 MSE: 0.061369
SL: 5.439351e-03 HS: Saving to file: bayesopt/bayesopt_0.061369.mat
|  232 | Accept |    0.061369 |      6.4767 |    0.040542 |    0.044965 |           92 |            2 |    0.0043937 |    0.0063077 |           34 |         relu |     0.099013 |           60 |         none |      0.27679 |            6 |         tanh |      0.15078 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          30              2              2.25              0.0095168        77     relu    0.44492    15     none    0.40249     8     tanh    0.49634

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.444916
Type: lstm Hidden size: 15 Activation: none Dropout: 0.402493
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 15 hidden units
     6   ''   Dropout             40% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 15.000000 MSE: 0.055617
SL: 4.799386e-03 HS: Saving to file: bayesopt/bayesopt_0.055617.mat
|  233 | Accept |    0.055617 |      8.5553 |    0.040542 |    0.044815 |           30 |            2 |         2.25 |    0.0095168 |           77 |         relu |      0.44492 |           15 |         none |      0.40249 |            8 |         tanh |      0.49634 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              2             0.10228            0.0065896        62     relu    0.13225    19     none    0.17334    28     tanh    0.17908

Type: lstm Hidden size: 62 Activation: relu Dropout: 0.132250
Type: lstm Hidden size: 19 Activation: none Dropout: 0.173340
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 62 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   Dropout             17% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 62 ME: 19.000000 MSE: 0.069488
SL: 7.683982e-03 HS: Saving to file: bayesopt/bayesopt_0.069488.mat
|  234 | Accept |    0.069488 |      6.2954 |    0.040542 |    0.040578 |           90 |            2 |      0.10228 |    0.0065896 |           62 |         relu |      0.13225 |           19 |         none |      0.17334 |           28 |         tanh |      0.17908 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          37              2            0.046327            0.0044262        77     relu    0.44773    64     none    0.31081    80     none    0.037998

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.447728
Type: lstm Hidden size: 64 Activation: none Dropout: 0.310810
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   Dropout             31% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 64.000000 MSE: 0.071135
SL: 7.509221e-03 HS: Saving to file: bayesopt/bayesopt_0.071135.mat
|  235 | Accept |    0.071135 |      10.563 |    0.040542 |    0.042369 |           37 |            2 |     0.046327 |    0.0044262 |           77 |         relu |      0.44773 |           64 |         none |      0.31081 |           80 |         none |     0.037998 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          65              2             2.5425             0.0065478        79     relu    0.23002    33     none    0.087712    41     none    0.17066

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.230020
Type: lstm Hidden size: 33 Activation: none Dropout: 0.087712
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 33 hidden units
     6   ''   Dropout             9% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 33.000000 MSE: 0.043905
SL: 2.828998e-03 HS: Saving to file: bayesopt/bayesopt_0.043905.mat
|  236 | Accept |    0.043905 |      7.6157 |    0.040542 |    0.041179 |           65 |            2 |       2.5425 |    0.0065478 |           79 |         relu |      0.23002 |           33 |         none |     0.087712 |           41 |         none |      0.17066 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          87              2             0.30509            0.0047641        50     relu    0.25654    78     none    0.02335    80     tanh    0.37562

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.256535
Type: lstm Hidden size: 78 Activation: none Dropout: 0.023350
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 78 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 78.000000 MSE: 0.068610
SL: 6.798488e-03 HS: Saving to file: bayesopt/bayesopt_0.06861.mat
|  237 | Accept |     0.06861 |      7.6015 |    0.040542 |    0.043519 |           87 |            2 |      0.30509 |    0.0047641 |           50 |         relu |      0.25654 |           78 |         none |      0.02335 |           80 |         tanh |      0.37562 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          86              2            0.013911             0.01263         86     relu    0.2734    29     none    0.25602    92     none    0.47383

Type: lstm Hidden size: 86 Activation: relu Dropout: 0.273398
Type: lstm Hidden size: 29 Activation: none Dropout: 0.256018
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 86 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Dropout             26% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 86 ME: 29.000000 MSE: 0.062693
SL: 5.860910e-03 HS: Saving to file: bayesopt/bayesopt_0.062693.mat
|  238 | Accept |    0.062693 |      7.3927 |    0.040542 |     0.04348 |           86 |            2 |     0.013911 |      0.01263 |           86 |         relu |       0.2734 |           29 |         none |      0.25602 |           92 |         none |      0.47383 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          42              3             0.10024             0.013257        67     relu    0.12087    39     none    0.079875    86     none    0.15231

Type: lstm Hidden size: 67 Activation: relu Dropout: 0.120867
Type: lstm Hidden size: 39 Activation: none Dropout: 0.079875
Type: lstm Hidden size: 86 Activation: none Dropout: 0.152308
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 67 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             8% dropout
     7   ''   LSTM                LSTM with 86 hidden units
     8   ''   Dropout             15% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 67 ME: 39.000000 MSE: 86.000000
SL: 8.129895e-02 HS: 1.027498e-02 ME: Saving to file: bayesopt/bayesopt_0.081299.mat
|  239 | Accept |    0.081299 |      12.704 |    0.040542 |    0.044352 |           42 |            3 |      0.10024 |     0.013257 |           67 |         relu |      0.12087 |           39 |         none |     0.079875 |           86 |         none |      0.15231 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          87              2            0.029983            0.0058406        58     relu    0.13199    87     none    0.11389    39     tanh    0.054863

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.131994
Type: lstm Hidden size: 87 Activation: none Dropout: 0.113887
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 87 hidden units
     6   ''   Dropout             11% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 87.000000 MSE: 0.053837
SL: 4.648101e-03 HS: Saving to file: bayesopt/bayesopt_0.053837.mat
|  240 | Accept |    0.053837 |      8.5696 |    0.040542 |    0.044357 |           87 |            2 |     0.029983 |    0.0058406 |           58 |         relu |      0.13199 |           87 |         none |      0.11389 |           39 |         tanh |     0.054863 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          44              2             6.2715             0.0096613        45     relu    0.39327    77     none    0.33886     2     tanh    0.39611

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.393270
Type: lstm Hidden size: 77 Activation: none Dropout: 0.338862
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   Dropout             34% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 77.000000 MSE: 0.068517
SL: 7.059810e-03 HS: Saving to file: bayesopt/bayesopt_0.068517.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  241 | Accept |    0.068517 |      8.6149 |    0.040542 |    0.044346 |           44 |            2 |       6.2715 |    0.0096613 |           45 |         relu |      0.39327 |           77 |         none |      0.33886 |            2 |         tanh |      0.39611 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    ________

          32              2             0.25922            0.0064344        94     relu    0.14672    69     none    0.2743    54     none    0.084734

Type: lstm Hidden size: 94 Activation: relu Dropout: 0.146719
Type: lstm Hidden size: 69 Activation: none Dropout: 0.274301
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 94 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 69 hidden units
     6   ''   Dropout             27% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 94 ME: 69.000000 MSE: 0.062551
SL: 5.944755e-03 HS: Saving to file: bayesopt/bayesopt_0.062551.mat
|  242 | Accept |    0.062551 |      12.231 |    0.040542 |    0.045075 |           32 |            2 |      0.25922 |    0.0064344 |           94 |         relu |      0.14672 |           69 |         none |       0.2743 |           54 |         none |     0.084734 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          38              2             0.62885            0.0057188        72     relu    0.2814    98     none    0.23766    41     none    0.30895

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.281398
Type: lstm Hidden size: 98 Activation: none Dropout: 0.237657
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   Dropout             24% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 98.000000 MSE: 0.043559
SL: 2.704390e-03 HS: Saving to file: bayesopt/bayesopt_0.043559.mat
|  243 | Accept |    0.043559 |      12.236 |    0.040542 |     0.04487 |           38 |            2 |      0.62885 |    0.0057188 |           72 |         relu |       0.2814 |           98 |         none |      0.23766 |           41 |         none |      0.30895 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          71              2             6.5618             0.0050854        61     relu    0.31745    55     none    0.053654    80     none    0.19523

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.317446
Type: lstm Hidden size: 55 Activation: none Dropout: 0.053654
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 55 hidden units
     6   ''   Dropout             5% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 55.000000 MSE: 0.074171
SL: 8.634281e-03 HS: Saving to file: bayesopt/bayesopt_0.074171.mat
|  244 | Accept |    0.074171 |      7.8033 |    0.040542 |    0.045614 |           71 |            2 |       6.5618 |    0.0050854 |           61 |         relu |      0.31745 |           55 |         none |     0.053654 |           80 |         none |      0.19523 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          92              2             0.30368            0.0055094        58     relu    0.11749     5     none    0.47374    75     tanh    0.29021

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.117494
Type: lstm Hidden size: 5 Activation: none Dropout: 0.473743
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 5 hidden units
     6   ''   Dropout             47% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 5.000000 MSE: 0.105308
SL: 1.603808e-02 HS: Saving to file: bayesopt/bayesopt_0.10531.mat
|  245 | Accept |     0.10531 |      5.7464 |    0.040542 |    0.047525 |           92 |            2 |      0.30368 |    0.0055094 |           58 |         relu |      0.11749 |            5 |         none |      0.47374 |           75 |         tanh |      0.29021 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          61              2             0.70059             0.00692         51     relu    0.1055    48     none    0.18969    58     tanh    0.27782

Type: lstm Hidden size: 51 Activation: relu Dropout: 0.105501
Type: lstm Hidden size: 48 Activation: none Dropout: 0.189685
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 51 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 48 hidden units
     6   ''   Dropout             19% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 51 ME: 48.000000 MSE: 0.048356
SL: 3.545768e-03 HS: Saving to file: bayesopt/bayesopt_0.048356.mat
|  246 | Accept |    0.048356 |       6.582 |    0.040542 |    0.048574 |           61 |            2 |      0.70059 |      0.00692 |           51 |         relu |       0.1055 |           48 |         none |      0.18969 |           58 |         tanh |      0.27782 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          61              2            0.0019986            0.012681        57     relu    0.33364    29     none    0.17843    79     none    0.017008

Type: lstm Hidden size: 57 Activation: relu Dropout: 0.333644
Type: lstm Hidden size: 29 Activation: none Dropout: 0.178431
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 57 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   Dropout             18% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 57 ME: 29.000000 MSE: 0.115220
SL: 2.142689e-02 HS: Saving to file: bayesopt/bayesopt_0.11522.mat
|  247 | Accept |     0.11522 |       6.262 |    0.040542 |    0.040551 |           61 |            2 |    0.0019986 |     0.012681 |           57 |         relu |      0.33364 |           29 |         none |      0.17843 |           79 |         none |     0.017008 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          90              2             0.8842             0.0064664        65     relu    0.40438    70     none    0.36381    82     tanh    0.27419

Type: lstm Hidden size: 65 Activation: relu Dropout: 0.404384
Type: lstm Hidden size: 70 Activation: none Dropout: 0.363815
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 65 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             36% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 65 ME: 70.000000 MSE: 0.053065
SL: 4.393521e-03 HS: Saving to file: bayesopt/bayesopt_0.053065.mat
|  248 | Accept |    0.053065 |      8.2244 |    0.040542 |    0.040553 |           90 |            2 |       0.8842 |    0.0064664 |           65 |         relu |      0.40438 |           70 |         none |      0.36381 |           82 |         tanh |      0.27419 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________

          44              2            0.0033459            0.003925        66     relu    0.086407    34     none    0.40887    34     none    0.063526

Type: lstm Hidden size: 66 Activation: relu Dropout: 0.086407
Type: lstm Hidden size: 34 Activation: none Dropout: 0.408869
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 66 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 34 hidden units
     6   ''   Dropout             41% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 66 ME: 34.000000 MSE: 0.052439
SL: 4.232104e-03 HS: Saving to file: bayesopt/bayesopt_0.052439.mat
|  249 | Accept |    0.052439 |      7.9846 |    0.040542 |    0.040551 |           44 |            2 |    0.0033459 |     0.003925 |           66 |         relu |     0.086407 |           34 |         none |      0.40887 |           34 |         none |     0.063526 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          98              2            0.0075948            0.011472        33     relu    0.3143     9     none    0.057666    50     none    0.39948

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.314298
Type: lstm Hidden size: 9 Activation: none Dropout: 0.057666
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 9 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 9.000000 MSE: 0.073478
SL: 7.556258e-03 HS: Saving to file: bayesopt/bayesopt_0.073478.mat
|  250 | Accept |    0.073478 |      5.2086 |    0.040542 |    0.045927 |           98 |            2 |    0.0075948 |     0.011472 |           33 |         relu |       0.3143 |            9 |         none |     0.057666 |           50 |         none |      0.39948 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ________

          48              2             8.3389              0.003983        58     relu    0.010658    40     none    0.018268    63     tanh    0.072508

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.010658
Type: lstm Hidden size: 40 Activation: none Dropout: 0.018268
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 40 hidden units
     6   ''   Dropout             2% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 40.000000 MSE: 0.049128
SL: 3.878524e-03 HS: Saving to file: bayesopt/bayesopt_0.049128.mat
|  251 | Accept |    0.049128 |       6.875 |    0.040542 |    0.045906 |           48 |            2 |       8.3389 |     0.003983 |           58 |         relu |     0.010658 |           40 |         none |     0.018268 |           63 |         tanh |     0.072508 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ________

          42              2            0.013256             0.022862        39     relu    0.082666    40     none    0.010696    24     none    0.025156

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.082666
Type: lstm Hidden size: 40 Activation: none Dropout: 0.010696
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             8% dropout
     5   ''   LSTM                LSTM with 40 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 40.000000 MSE: 0.062881
SL: 5.570406e-03 HS: Saving to file: bayesopt/bayesopt_0.062881.mat
|  252 | Accept |    0.062881 |      6.8195 |    0.040542 |    0.045921 |           42 |            2 |     0.013256 |     0.022862 |           39 |         relu |     0.082666 |           40 |         none |     0.010696 |           24 |         none |     0.025156 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          44              2             0.33403            0.0099078        90     relu    0.40768    26     none    0.31524    45     none    0.43982

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.407677
Type: lstm Hidden size: 26 Activation: none Dropout: 0.315243
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 26 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 26.000000 MSE: 0.057470
SL: 4.738436e-03 HS: Saving to file: bayesopt/bayesopt_0.05747.mat
|  253 | Accept |     0.05747 |      9.4678 |    0.040542 |    0.045942 |           44 |            2 |      0.33403 |    0.0099078 |           90 |         relu |      0.40768 |           26 |         none |      0.31524 |           45 |         none |      0.43982 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          71              2             1.0553              0.032257        56     relu    0.095727    14     none    0.43979    86     tanh    0.37195

Type: lstm Hidden size: 56 Activation: relu Dropout: 0.095727
Type: lstm Hidden size: 14 Activation: none Dropout: 0.439791
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 56 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 14 hidden units
     6   ''   Dropout             44% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 56 ME: 14.000000 MSE: 0.183706
SL: 4.606973e-02 HS: Saving to file: bayesopt/bayesopt_0.18371.mat
|  254 | Accept |     0.18371 |      6.3955 |    0.040542 |     0.04786 |           71 |            2 |       1.0553 |     0.032257 |           56 |         relu |     0.095727 |           14 |         none |      0.43979 |           86 |         tanh |      0.37195 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          92              3             0.66648             0.012649        17     relu    0.18214    43     none    0.031785    41     none    0.35544

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.182140
Type: lstm Hidden size: 43 Activation: none Dropout: 0.031785
Type: lstm Hidden size: 41 Activation: none Dropout: 0.355435
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 43 hidden units
     6   ''   Dropout             3% dropout
     7   ''   LSTM                LSTM with 41 hidden units
     8   ''   Dropout             36% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 43.000000 MSE: 41.000000
SL: 9.043734e-02 HS: 1.253566e-02 ME: Saving to file: bayesopt/bayesopt_0.090437.mat
|  255 | Accept |    0.090437 |      7.6456 |    0.040542 |     0.04358 |           92 |            3 |      0.66648 |     0.012649 |           17 |         relu |      0.18214 |           43 |         none |     0.031785 |           41 |         none |      0.35544 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          85              3            0.0010551           0.0067501        64     relu    0.18261    84     none    0.28724    55     none    0.15777

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.182614
Type: lstm Hidden size: 84 Activation: none Dropout: 0.287240
Type: lstm Hidden size: 55 Activation: none Dropout: 0.157775
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   Dropout             29% dropout
     7   ''   LSTM                LSTM with 55 hidden units
     8   ''   Dropout             16% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 84.000000 MSE: 55.000000
SL: 1.046745e-01 HS: 1.664783e-02 ME: Saving to file: bayesopt/bayesopt_0.10467.mat
|  256 | Accept |     0.10467 |      11.943 |    0.040542 |    0.047979 |           85 |            3 |    0.0010551 |    0.0067501 |           64 |         relu |      0.18261 |           84 |         none |      0.28724 |           55 |         none |      0.15777 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ______    ___    ____    _______

          59              2             1.0322              0.021805        38     relu    0.036254    35     none    0.3129     9     tanh    0.46342

Type: lstm Hidden size: 38 Activation: relu Dropout: 0.036254
Type: lstm Hidden size: 35 Activation: none Dropout: 0.312905
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 38 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 35 hidden units
     6   ''   Dropout             31% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 38 ME: 35.000000 MSE: 0.058391
SL: 4.949473e-03 HS: Saving to file: bayesopt/bayesopt_0.058391.mat
|  257 | Accept |    0.058391 |       6.742 |    0.040542 |    0.047895 |           59 |            2 |       1.0322 |     0.021805 |           38 |         relu |     0.036254 |           35 |         none |       0.3129 |            9 |         tanh |      0.46342 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          97              2            0.019894             0.011249        68     relu    0.3787    65     none    0.30146     7     tanh    0.15632

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.378704
Type: lstm Hidden size: 65 Activation: none Dropout: 0.301465
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 65 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 65.000000 MSE: 0.058114
SL: 4.884504e-03 HS: Saving to file: bayesopt/bayesopt_0.058114.mat
|  258 | Accept |    0.058114 |      8.4878 |    0.040542 |    0.047897 |           97 |            2 |     0.019894 |     0.011249 |           68 |         relu |       0.3787 |           65 |         none |      0.30146 |            7 |         tanh |      0.15632 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          81              2            0.002778            0.0068529        88     relu    0.36711    25     none    0.32041    25     tanh    0.0085259

Type: lstm Hidden size: 88 Activation: relu Dropout: 0.367111
Type: lstm Hidden size: 25 Activation: none Dropout: 0.320415
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 88 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 25 hidden units
     6   ''   Dropout             32% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 88 ME: 25.000000 MSE: 0.049249
SL: 3.953329e-03 HS: Saving to file: bayesopt/bayesopt_0.049249.mat
|  259 | Accept |    0.049249 |      6.8974 |    0.040542 |    0.047893 |           81 |            2 |     0.002778 |    0.0068529 |           88 |         relu |      0.36711 |           25 |         none |      0.32041 |           25 |         tanh |    0.0085259 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          66              3             0.67287             0.022463        40     relu    0.14644     4     none    0.4457     8     tanh    0.15971

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.146439
Type: lstm Hidden size: 4 Activation: none Dropout: 0.445703
Type: lstm Hidden size: 8 Activation: tanh Dropout: 0.159714
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 4 hidden units
     6   ''   Dropout             45% dropout
     7   ''   LSTM                LSTM with 8 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             16% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 4.000000 MSE: 8.000000
SL: 1.858949e-01 HS: 4.631988e-02 ME: Saving to file: bayesopt/bayesopt_0.18589.mat
|  260 | Accept |     0.18589 |      6.9745 |    0.040542 |    0.040552 |           66 |            3 |      0.67287 |     0.022463 |           40 |         relu |      0.14644 |            4 |         none |       0.4457 |            8 |         tanh |      0.15971 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          88              2             0.08191            0.0050663        38     relu    0.31567    65     none    0.3053    83     tanh    0.32394

Type: lstm Hidden size: 38 Activation: relu Dropout: 0.315666
Type: lstm Hidden size: 65 Activation: none Dropout: 0.305301
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 38 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 65 hidden units
     6   ''   Dropout             31% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 38 ME: 65.000000 MSE: 0.076367
SL: 7.889187e-03 HS: Saving to file: bayesopt/bayesopt_0.076367.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  261 | Accept |    0.076367 |      6.5811 |    0.040542 |    0.040553 |           88 |            2 |      0.08191 |    0.0050663 |           38 |         relu |      0.31567 |           65 |         none |       0.3053 |           83 |         tanh |      0.32394 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    ______

          82              2             0.09442             0.011346        61     relu    0.4353    84     none    0.069148     6     none    0.3701

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.435300
Type: lstm Hidden size: 84 Activation: none Dropout: 0.069148
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 84.000000 MSE: 0.066881
SL: 6.441356e-03 HS: Saving to file: bayesopt/bayesopt_0.066881.mat
|  262 | Accept |    0.066881 |      8.1259 |    0.040542 |    0.040556 |           82 |            2 |      0.09442 |     0.011346 |           61 |         relu |       0.4353 |           84 |         none |     0.069148 |            6 |         none |       0.3701 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3      drop3  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _________

          94              2            0.0058995           0.0024535        61     relu    0.093005    31     none    0.32727    51     none    0.0083997

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.093005
Type: lstm Hidden size: 31 Activation: none Dropout: 0.327273
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             33% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 31.000000 MSE: 0.081068
SL: 1.000366e-02 HS: Saving to file: bayesopt/bayesopt_0.081068.mat
|  263 | Accept |    0.081068 |      6.6059 |    0.040542 |    0.040556 |           94 |            2 |    0.0058995 |    0.0024535 |           61 |         relu |     0.093005 |           31 |         none |      0.32727 |           51 |         none |    0.0083997 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    ________

          59              2              1.12              0.0079254        84     relu    0.1235    80     none    0.014988    40     none    0.069502

Type: lstm Hidden size: 84 Activation: relu Dropout: 0.123500
Type: lstm Hidden size: 80 Activation: none Dropout: 0.014988
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 84 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 80 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 84 ME: 80.000000 MSE: 0.049203
SL: 4.002926e-03 HS: Saving to file: bayesopt/bayesopt_0.049203.mat
|  264 | Accept |    0.049203 |      11.533 |    0.040542 |    0.040564 |           59 |            2 |         1.12 |    0.0079254 |           84 |         relu |       0.1235 |           80 |         none |     0.014988 |           40 |         none |     0.069502 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2            0.012071             0.017825        56     relu    0.12964     6     none    0.44847    11     tanh    0.39769

Type: lstm Hidden size: 56 Activation: relu Dropout: 0.129641
Type: lstm Hidden size: 6 Activation: none Dropout: 0.448466
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 56 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   Dropout             45% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 56 ME: 6.000000 MSE: 0.094275
SL: 1.280823e-02 HS: Saving to file: bayesopt/bayesopt_0.094275.mat
|  265 | Accept |    0.094275 |      6.1567 |    0.040542 |    0.044972 |           50 |            2 |     0.012071 |     0.017825 |           56 |         relu |      0.12964 |            6 |         none |      0.44847 |           11 |         tanh |      0.39769 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          16              2             0.98472            0.0059552        71     relu    0.26572    77     none    0.067352    45     none    0.39808

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.265721
Type: lstm Hidden size: 77 Activation: none Dropout: 0.067352
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 77.000000 MSE: 0.063449
SL: 5.698680e-03 HS: Saving to file: bayesopt/bayesopt_0.063449.mat
|  266 | Accept |    0.063449 |      13.536 |    0.040542 |    0.047403 |           16 |            2 |      0.98472 |    0.0059552 |           71 |         relu |      0.26572 |           77 |         none |     0.067352 |           45 |         none |      0.39808 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______

          88              2             5.6404              0.008514        81     relu    0.029206    39     none    0.064084    88     none    0.21511

Type: lstm Hidden size: 81 Activation: relu Dropout: 0.029206
Type: lstm Hidden size: 39 Activation: none Dropout: 0.064084
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 81 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             6% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 81 ME: 39.000000 MSE: 0.055380
SL: 4.642012e-03 HS: Saving to file: bayesopt/bayesopt_0.05538.mat
|  267 | Accept |     0.05538 |      7.5053 |    0.040542 |    0.040565 |           88 |            2 |       5.6404 |     0.008514 |           81 |         relu |     0.029206 |           39 |         none |     0.064084 |           88 |         none |      0.21511 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          91              2             1.6512              0.006267        90     relu    0.17579    53     none    0.21092    84     none    0.28323

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.175790
Type: lstm Hidden size: 53 Activation: none Dropout: 0.210915
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 53 hidden units
     6   ''   Dropout             21% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 53.000000 MSE: 0.043689
SL: 2.845145e-03 HS: Saving to file: bayesopt/bayesopt_0.043689.mat
|  268 | Accept |    0.043689 |      8.8569 |    0.040542 |    0.047425 |           91 |            2 |       1.6512 |     0.006267 |           90 |         relu |      0.17579 |           53 |         none |      0.21092 |           84 |         none |      0.28323 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________

          64              3            0.0026318           0.0079301        40     relu    0.36336    20     none    0.070619     5     tanh    0.088957

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.363358
Type: lstm Hidden size: 20 Activation: none Dropout: 0.070619
Type: lstm Hidden size: 5 Activation: tanh Dropout: 0.088957
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 20 hidden units
     6   ''   Dropout             7% dropout
     7   ''   LSTM                LSTM with 5 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             9% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 20.000000 MSE: 5.000000
SL: 8.816049e-02 HS: 1.049478e-02 ME: Saving to file: bayesopt/bayesopt_0.08816.mat
|  269 | Accept |     0.08816 |      7.2403 |    0.040542 |    0.041349 |           64 |            3 |    0.0026318 |    0.0079301 |           40 |         relu |      0.36336 |           20 |         none |     0.070619 |            5 |         tanh |     0.088957 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          64              2             0.14085            0.0082583        24     relu    0.32953    70     none    0.16053    24     tanh    0.085109

Type: lstm Hidden size: 24 Activation: relu Dropout: 0.329531
Type: lstm Hidden size: 70 Activation: none Dropout: 0.160534
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 24 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 24 ME: 70.000000 MSE: 0.068191
SL: 6.662822e-03 HS: Saving to file: bayesopt/bayesopt_0.068191.mat
|  270 | Accept |    0.068191 |      6.5726 |    0.040542 |    0.047595 |           64 |            2 |      0.14085 |    0.0082583 |           24 |         relu |      0.32953 |           70 |         none |      0.16053 |           24 |         tanh |     0.085109 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          23              3             6.6529             0.0080304        43     relu    0.36925    60     none    0.14563    78     none    0.39094

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.369247
Type: lstm Hidden size: 60 Activation: none Dropout: 0.145626
Type: lstm Hidden size: 78 Activation: none Dropout: 0.390944
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   Dropout             15% dropout
     7   ''   LSTM                LSTM with 78 hidden units
     8   ''   Dropout             39% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 60.000000 MSE: 78.000000
SL: 8.348677e-02 HS: 9.879893e-03 ME: Saving to file: bayesopt/bayesopt_0.083487.mat
|  271 | Accept |    0.083487 |      14.942 |    0.040542 |     0.04055 |           23 |            3 |       6.6529 |    0.0080304 |           43 |         relu |      0.36925 |           60 |         none |      0.14563 |           78 |         none |      0.39094 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          94              2            0.044465            0.0075555        33     relu    0.39879    77     none    0.46736    70     tanh    0.38649

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.398787
Type: lstm Hidden size: 77 Activation: none Dropout: 0.467356
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   Dropout             47% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 77.000000 MSE: 0.057751
SL: 4.700547e-03 HS: Saving to file: bayesopt/bayesopt_0.057751.mat
|  272 | Accept |    0.057751 |      7.2229 |    0.040542 |    0.040551 |           94 |            2 |     0.044465 |    0.0075555 |           33 |         relu |      0.39879 |           77 |         none |      0.46736 |           70 |         tanh |      0.38649 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2             0.38747            0.0060588        74     relu    0.40067    62     none    0.41897    47     tanh    0.43715

Type: lstm Hidden size: 74 Activation: relu Dropout: 0.400671
Type: lstm Hidden size: 62 Activation: none Dropout: 0.418971
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 74 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 62 hidden units
     6   ''   Dropout             42% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 74 ME: 62.000000 MSE: 0.059563
SL: 5.119992e-03 HS: Saving to file: bayesopt/bayesopt_0.059563.mat
|  273 | Accept |    0.059563 |       8.733 |    0.040542 |    0.040549 |           50 |            2 |      0.38747 |    0.0060588 |           74 |         relu |      0.40067 |           62 |         none |      0.41897 |           47 |         tanh |      0.43715 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          25              2             0.40805            0.0065237        58     relu    0.11048    94     none    0.15613    69     none    0.47262

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.110476
Type: lstm Hidden size: 94 Activation: none Dropout: 0.156129
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 94.000000 MSE: 0.101146
SL: 1.548516e-02 HS: Saving to file: bayesopt/bayesopt_0.10115.mat
|  274 | Accept |     0.10115 |      11.831 |    0.040542 |    0.044996 |           25 |            2 |      0.40805 |    0.0065237 |           58 |         relu |      0.11048 |           94 |         none |      0.15613 |           69 |         none |      0.47262 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          93              2            0.054232            0.0063961        94     relu    0.17584    26     none    0.29521    63     none    0.28507

Type: lstm Hidden size: 94 Activation: relu Dropout: 0.175838
Type: lstm Hidden size: 26 Activation: none Dropout: 0.295206
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 94 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 26 hidden units
     6   ''   Dropout             30% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 94 ME: 26.000000 MSE: 0.058820
SL: 5.196685e-03 HS: Saving to file: bayesopt/bayesopt_0.05882.mat
|  275 | Accept |     0.05882 |      7.9164 |    0.040542 |    0.040551 |           93 |            2 |     0.054232 |    0.0063961 |           94 |         relu |      0.17584 |           26 |         none |      0.29521 |           63 |         none |      0.28507 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          34              2              8.151             0.0049638        89     relu    0.1617    21     none    0.092676    77     none    0.40767

Type: lstm Hidden size: 89 Activation: relu Dropout: 0.161698
Type: lstm Hidden size: 21 Activation: none Dropout: 0.092676
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 89 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             9% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 89 ME: 21.000000 MSE: 0.066693
SL: 5.935969e-03 HS: Saving to file: bayesopt/bayesopt_0.066693.mat
|  276 | Accept |    0.066693 |      9.3865 |    0.040542 |    0.044493 |           34 |            2 |        8.151 |    0.0049638 |           89 |         relu |       0.1617 |           21 |         none |     0.092676 |           77 |         none |      0.40767 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          58              2            0.024322             0.021035        22     relu    0.1261    20     none    0.02833    74     tanh    0.20397

Type: lstm Hidden size: 22 Activation: relu Dropout: 0.126095
Type: lstm Hidden size: 20 Activation: none Dropout: 0.028330
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 22 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 20 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 22 ME: 20.000000 MSE: 0.071493
SL: 7.632402e-03 HS: Saving to file: bayesopt/bayesopt_0.071493.mat
|  277 | Accept |    0.071493 |      5.6558 |    0.040542 |    0.045448 |           58 |            2 |     0.024322 |     0.021035 |           22 |         relu |       0.1261 |           20 |         none |      0.02833 |           74 |         tanh |      0.20397 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          92              3            0.0088327           0.0092138        44     relu    0.31875    21     none    0.16135    33     tanh    0.11286

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.318745
Type: lstm Hidden size: 21 Activation: none Dropout: 0.161345
Type: lstm Hidden size: 33 Activation: tanh Dropout: 0.112858
  11?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             16% dropout
     7   ''   LSTM                LSTM with 33 hidden units
     8   ''   Tanh                Hyperbolic tangent
     9   ''   Dropout             11% dropout
    10   ''   Fully Connected     1 fully connected layer
    11   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 21.000000 MSE: 33.000000
SL: 7.247946e-02 HS: 7.561812e-03 ME: Saving to file: bayesopt/bayesopt_0.072479.mat
|  278 | Accept |    0.072479 |      7.7743 |    0.040542 |    0.045569 |           92 |            3 |    0.0088327 |    0.0092138 |           44 |         relu |      0.31875 |           21 |         none |      0.16135 |           33 |         tanh |      0.11286 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          90              2            0.047412            0.0070153        76     relu    0.24706    52     none    0.068118    40     none    0.40484

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.247056
Type: lstm Hidden size: 52 Activation: none Dropout: 0.068118
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 52 hidden units
     6   ''   Dropout             7% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 52.000000 MSE: 0.054390
SL: 4.358307e-03 HS: Saving to file: bayesopt/bayesopt_0.05439.mat
|  279 | Accept |     0.05439 |       7.899 |    0.040542 |     0.04055 |           90 |            2 |     0.047412 |    0.0070153 |           76 |         relu |      0.24706 |           52 |         none |     0.068118 |           40 |         none |      0.40484 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          91              2            0.0031778           0.0051628        91     relu    0.4889    33     none    0.026053    84     none    0.20655

Type: lstm Hidden size: 91 Activation: relu Dropout: 0.488897
Type: lstm Hidden size: 33 Activation: none Dropout: 0.026053
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 91 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 33 hidden units
     6   ''   Dropout             3% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 91 ME: 33.000000 MSE: 0.068615
SL: 6.764615e-03 HS: Saving to file: bayesopt/bayesopt_0.068615.mat
|  280 | Accept |    0.068615 |       8.084 |    0.040542 |    0.040549 |           91 |            2 |    0.0031778 |    0.0051628 |           91 |         relu |       0.4889 |           33 |         none |     0.026053 |           84 |         none |      0.20655 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ________

          51              2            0.099348             0.018664        60     relu    0.3175    49     none    0.38701     5     tanh    0.063809

Type: lstm Hidden size: 60 Activation: relu Dropout: 0.317503
Type: lstm Hidden size: 49 Activation: none Dropout: 0.387011
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 60 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 49 hidden units
     6   ''   Dropout             39% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 60 ME: 49.000000 MSE: 0.053362
SL: 5.168145e-03 HS: Saving to file: bayesopt/bayesopt_0.053362.mat
|==========================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |
|==========================================================================================================================================================================================================================================================================|
|  281 | Accept |    0.053362 |      7.6722 |    0.040542 |    0.040549 |           51 |            2 |     0.099348 |     0.018664 |           60 |         relu |       0.3175 |           49 |         none |      0.38701 |            5 |         tanh |     0.063809 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          59              2            0.0039115            0.034039        48     relu    0.46049    54     none    0.09993    62     tanh    0.47963

Type: lstm Hidden size: 48 Activation: relu Dropout: 0.460493
Type: lstm Hidden size: 54 Activation: none Dropout: 0.099930
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 48 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             46% dropout
     5   ''   LSTM                LSTM with 54 hidden units
     6   ''   Dropout             10% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 48 ME: 54.000000 MSE: 0.062254
SL: 5.727935e-03 HS: Saving to file: bayesopt/bayesopt_0.062254.mat
|  282 | Accept |    0.062254 |      8.0863 |    0.040542 |    0.040549 |           59 |            2 |    0.0039115 |     0.034039 |           48 |         relu |      0.46049 |           54 |         none |      0.09993 |           62 |         tanh |      0.47963 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          47              2            0.0057879           0.0083183        43     relu    0.34866    31     none    0.10566    61     tanh    0.045988

Type: lstm Hidden size: 43 Activation: relu Dropout: 0.348663
Type: lstm Hidden size: 31 Activation: none Dropout: 0.105660
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 43 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             35% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   Dropout             11% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 43 ME: 31.000000 MSE: 0.077095
SL: 8.106406e-03 HS: Saving to file: bayesopt/bayesopt_0.077095.mat
|  283 | Accept |    0.077095 |       6.898 |    0.040542 |    0.040552 |           47 |            2 |    0.0057879 |    0.0083183 |           43 |         relu |      0.34866 |           31 |         none |      0.10566 |           61 |         tanh |     0.045988 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          61              2              3.587              0.027939        35     relu    0.32725    43     none    0.12964    52     tanh    0.36407

Type: lstm Hidden size: 35 Activation: relu Dropout: 0.327251
Type: lstm Hidden size: 43 Activation: none Dropout: 0.129641
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 35 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             33% dropout
     5   ''   LSTM                LSTM with 43 hidden units
     6   ''   Dropout             13% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 35 ME: 43.000000 MSE: 0.066152
SL: 6.661639e-03 HS: Saving to file: bayesopt/bayesopt_0.066152.mat
|  284 | Accept |    0.066152 |      5.6662 |    0.040542 |     0.04055 |           61 |            2 |        3.587 |     0.027939 |           35 |         relu |      0.32725 |           43 |         none |      0.12964 |           52 |         tanh |      0.36407 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          75              2             0.25689            0.0090917        24     relu    0.16742    72     none    0.19822     3     tanh    0.31246

Type: lstm Hidden size: 24 Activation: relu Dropout: 0.167421
Type: lstm Hidden size: 72 Activation: none Dropout: 0.198225
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 24 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 72 hidden units
     6   ''   Dropout             20% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 24 ME: 72.000000 MSE: 0.057770
SL: 5.334693e-03 HS: Saving to file: bayesopt/bayesopt_0.05777.mat
|  285 | Accept |     0.05777 |      7.4607 |    0.040542 |    0.040553 |           75 |            2 |      0.25689 |    0.0090917 |           24 |         relu |      0.16742 |           72 |         none |      0.19822 |            3 |         tanh |      0.31246 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          95              2             0.06274            0.0040138        48     relu    0.29127    16     none    0.27815    89     none    0.38328

Type: lstm Hidden size: 48 Activation: relu Dropout: 0.291267
Type: lstm Hidden size: 16 Activation: none Dropout: 0.278148
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 48 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             29% dropout
     5   ''   LSTM                LSTM with 16 hidden units
     6   ''   Dropout             28% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 48 ME: 16.000000 MSE: 0.067857
SL: 7.151812e-03 HS: Saving to file: bayesopt/bayesopt_0.067857.mat
|  286 | Accept |    0.067857 |      5.7731 |    0.040542 |    0.040555 |           95 |            2 |      0.06274 |    0.0040138 |           48 |         relu |      0.29127 |           16 |         none |      0.27815 |           89 |         none |      0.38328 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          52              3             1.2513              0.012937        44     relu    0.15854    57     none    0.30026    81     none    0.012195

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.158543
Type: lstm Hidden size: 57 Activation: none Dropout: 0.300257
Type: lstm Hidden size: 81 Activation: none Dropout: 0.012195
  10?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             16% dropout
     5   ''   LSTM                LSTM with 57 hidden units
     6   ''   Dropout             30% dropout
     7   ''   LSTM                LSTM with 81 hidden units
     8   ''   Dropout             1% dropout
     9   ''   Fully Connected     1 fully connected layer
    10   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 57.000000 MSE: 81.000000
SL: 6.234795e-02 HS: 5.747906e-03 ME: Saving to file: bayesopt/bayesopt_0.062348.mat
|  287 | Accept |    0.062348 |      12.691 |    0.040542 |    0.040559 |           52 |            3 |       1.2513 |     0.012937 |           44 |         relu |      0.15854 |           57 |         none |      0.30026 |           81 |         none |     0.012195 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          38              2             4.6367              0.002592        90     relu    0.2818    69     none    0.31425    25     none    0.23204

Type: lstm Hidden size: 90 Activation: relu Dropout: 0.281800
Type: lstm Hidden size: 69 Activation: none Dropout: 0.314251
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 90 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 69 hidden units
     6   ''   Dropout             31% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 90 ME: 69.000000 MSE: 0.065434
SL: 5.916949e-03 HS: Saving to file: bayesopt/bayesopt_0.065434.mat
|  288 | Accept |    0.065434 |       11.44 |    0.040542 |    0.040558 |           38 |            2 |       4.6367 |     0.002592 |           90 |         relu |       0.2818 |           69 |         none |      0.31425 |           25 |         none |      0.23204 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          99              2            0.001006            0.0062357        63     relu    0.43981    64     none    0.15591    99     none    0.48547

Type: lstm Hidden size: 63 Activation: relu Dropout: 0.439811
Type: lstm Hidden size: 64 Activation: none Dropout: 0.155910
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 63 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   Dropout             16% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 63 ME: 64.000000 MSE: 0.068010
SL: 6.828020e-03 HS: Saving to file: bayesopt/bayesopt_0.06801.mat
|  289 | Accept |     0.06801 |      8.0363 |    0.040542 |    0.040555 |           99 |            2 |     0.001006 |    0.0062357 |           63 |         relu |      0.43981 |           64 |         none |      0.15591 |           99 |         none |      0.48547 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          76              2            0.0013618           0.0044088        100    relu    0.29322    48     none    0.24176    21     tanh    0.40855

Type: lstm Hidden size: 100 Activation: relu Dropout: 0.293217
Type: lstm Hidden size: 48 Activation: none Dropout: 0.241761
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 100 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             29% dropout
     5   ''   LSTM                LSTM with 48 hidden units
     6   ''   Dropout             24% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 100 ME: 48.000000 MSE: 0.225529
SL: 9.837403e-02 HS: Saving to file: bayesopt/bayesopt_0.22553.mat
|  290 | Accept |     0.22553 |      10.891 |    0.040542 |    0.042141 |           76 |            2 |    0.0013618 |    0.0044088 |          100 |         relu |      0.29322 |           48 |         none |      0.24176 |           21 |         tanh |      0.40855 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          39              2             9.6566             0.0037473        93     relu    0.48947    40     none    0.40399    78     tanh    0.066609

Type: lstm Hidden size: 93 Activation: relu Dropout: 0.489472
Type: lstm Hidden size: 40 Activation: none Dropout: 0.403988
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 93 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 40 hidden units
     6   ''   Dropout             40% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 93 ME: 40.000000 MSE: 0.065367
SL: 6.163992e-03 HS: Saving to file: bayesopt/bayesopt_0.065367.mat
|  291 | Accept |    0.065367 |      10.396 |    0.040542 |    0.042117 |           39 |            2 |       9.6566 |    0.0037473 |           93 |         relu |      0.48947 |           40 |         none |      0.40399 |           78 |         tanh |     0.066609 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _____

          83              2             8.6589              0.050481        13     relu    0.22559    36     none    0.04292    90     tanh    0.263

Type: lstm Hidden size: 13 Activation: relu Dropout: 0.225585
Type: lstm Hidden size: 36 Activation: none Dropout: 0.042920
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 13 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 36 hidden units
     6   ''   Dropout             4% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 13 ME: 36.000000 MSE: 0.070585
SL: 6.913431e-03 HS: Saving to file: bayesopt/bayesopt_0.070585.mat
|  292 | Accept |    0.070585 |      4.7064 |    0.040542 |     0.04217 |           83 |            2 |       8.6589 |     0.050481 |           13 |         relu |      0.22559 |           36 |         none |      0.04292 |           90 |         tanh |        0.263 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          91              2            0.0040575            0.040434         3     relu    0.11974    43     none    0.33479    39     tanh    0.42909

Type: lstm Hidden size: 3 Activation: relu Dropout: 0.119739
Type: lstm Hidden size: 43 Activation: none Dropout: 0.334791
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 3 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 43 hidden units
     6   ''   Dropout             33% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 3 ME: 43.000000 MSE: 0.207910
SL: 9.011595e-02 HS: Saving to file: bayesopt/bayesopt_0.20791.mat
|  293 | Accept |     0.20791 |      4.8503 |    0.040542 |    0.042517 |           91 |            2 |    0.0040575 |     0.040434 |            3 |         relu |      0.11974 |           43 |         none |      0.33479 |           39 |         tanh |      0.42909 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          56              2            0.0057596            0.009733        76     relu    0.46905     7     none    0.35952    33     tanh    0.22144

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.469046
Type: lstm Hidden size: 7 Activation: none Dropout: 0.359519
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             47% dropout
     5   ''   LSTM                LSTM with 7 hidden units
     6   ''   Dropout             36% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 7.000000 MSE: 0.064751
SL: 6.276140e-03 HS: Saving to file: bayesopt/bayesopt_0.064751.mat
|  294 | Accept |    0.064751 |      7.2939 |    0.040542 |    0.040546 |           56 |            2 |    0.0057596 |     0.009733 |           76 |         relu |      0.46905 |            7 |         none |      0.35952 |           33 |         tanh |      0.22144 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          99              2             5.6009             0.0067352        78     relu    0.11337     5     none    0.24574    66     tanh    0.2107

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.113369
Type: lstm Hidden size: 5 Activation: none Dropout: 0.245743
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 5 hidden units
     6   ''   Dropout             25% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 5.000000 MSE: 0.058771
SL: 5.392752e-03 HS: Saving to file: bayesopt/bayesopt_0.058771.mat
|  295 | Accept |    0.058771 |      6.7992 |    0.040542 |    0.040546 |           99 |            2 |       5.6009 |    0.0067352 |           78 |         relu |      0.11337 |            5 |         none |      0.24574 |           66 |         tanh |       0.2107 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          79              2              1.428             0.0029867        76     relu    0.45153    39     none    0.013089    64     tanh    0.47329

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.451530
Type: lstm Hidden size: 39 Activation: none Dropout: 0.013089
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 39 hidden units
     6   ''   Dropout             1% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 39.000000 MSE: 0.060142
SL: 5.565115e-03 HS: Saving to file: bayesopt/bayesopt_0.060142.mat
|  296 | Accept |    0.060142 |      8.7707 |    0.040542 |    0.040545 |           79 |            2 |        1.428 |    0.0029867 |           76 |         relu |      0.45153 |           39 |         none |     0.013089 |           64 |         tanh |      0.47329 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______

          93              2             6.6065             0.0072516        79     relu    0.042727    21     none    0.34551    21     tanh    0.47294

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.042727
Type: lstm Hidden size: 21 Activation: none Dropout: 0.345515
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   Dropout             35% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 21.000000 MSE: 0.043297
SL: 2.721550e-03 HS: Saving to file: bayesopt/bayesopt_0.043297.mat
|  297 | Accept |    0.043297 |      6.9812 |    0.040542 |    0.040546 |           93 |            2 |       6.6065 |    0.0072516 |           79 |         relu |     0.042727 |           21 |         none |      0.34551 |           21 |         tanh |      0.47294 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    _______

          86              2            0.034577            0.0084901        59     relu    0.0041038    94     none    0.23071    10     tanh    0.42191

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.004104
Type: lstm Hidden size: 94 Activation: none Dropout: 0.230711
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             0% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   Dropout             23% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 94.000000 MSE: 0.068030
SL: 6.665829e-03 HS: Saving to file: bayesopt/bayesopt_0.06803.mat
|  298 | Accept |     0.06803 |      8.9584 |    0.040542 |    0.043178 |           86 |            2 |     0.034577 |    0.0084901 |           59 |         relu |    0.0041038 |           94 |         none |      0.23071 |           10 |         tanh |      0.42191 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2            0.0010955           0.0027203        77     relu    0.34055    11     none    0.13209    44     tanh    0.24728

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.340548
Type: lstm Hidden size: 11 Activation: none Dropout: 0.132092
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 11 hidden units
     6   ''   Dropout             13% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 11.000000 MSE: 0.057098
SL: 4.891677e-03 HS: Saving to file: bayesopt/bayesopt_0.057098.mat
|  299 | Accept |    0.057098 |      7.3481 |    0.040542 |    0.043286 |           50 |            2 |    0.0010955 |    0.0027203 |           77 |         relu |      0.34055 |           11 |         none |      0.13209 |           44 |         tanh |      0.24728 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          85              2            0.0065492           0.0072934        89     relu    0.23032     8     none    0.29116    52     tanh    0.42726

Type: lstm Hidden size: 89 Activation: relu Dropout: 0.230319
Type: lstm Hidden size: 8 Activation: none Dropout: 0.291165
  8?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 89 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 8 hidden units
     6   ''   Dropout             29% dropout
     7   ''   Fully Connected     1 fully connected layer
     8   ''   Regression Output   mean-squared-error
SL: 1 HS: 89 ME: 8.000000 MSE: 0.057407
SL: 4.791986e-03 HS: Saving to file: bayesopt/bayesopt_0.057407.mat
|  300 | Accept |    0.057407 |      6.5968 |    0.040542 |    0.043355 |           85 |            2 |    0.0065492 |    0.0072934 |           89 |         relu |      0.23032 |            8 |         none |      0.29116 |           52 |         tanh |      0.42726 |

__________________________________________________________
Optimization completed.
MaxObjectiveEvaluations of 300 reached.
Total function evaluations: 300
Total elapsed time: 3943.6207 seconds
Total objective function evaluation time: 2580.1138

Best observed feasible point:
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          63              2            0.010733             0.026647        33     relu    0.27291    36     none    0.17883     7     tanh    0.46316

Observed objective function value = 0.040542
Estimated objective function value = 0.043355
Function evaluation time = 5.766

Best estimated feasible point (according to models):
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          63              2            0.010733             0.026647        33     relu    0.27291    36     none    0.17883     7     tanh    0.46316

Estimated objective function value = 0.043355
Estimated function evaluation time = 5.9648

Loading file: bayesopt/bayesopt_0.040542.mat
  SeriesNetwork with properties:

         Layers: [8?1 nnet.cnn.layer.Layer]
     InputNames: {'sequenceinput'}
    OutputNames: {'regressionoutput'}

  8?1 Layer array with layers:

     1   'sequenceinput'      Sequence Input      Sequence input with 1 dimensions
     2   'lstm_1'             LSTM                LSTM with 33 hidden units
     3   'relu'               ReLU                ReLU
     4   'dropout_1'          Dropout             27% dropout
     5   'lstm_2'             LSTM                LSTM with 36 hidden units
     6   'dropout_2'          Dropout             18% dropout
     7   'fc'                 Fully Connected     1 fully connected layer
     8   'regressionoutput'   Regression Output   mean-squared-error with response 'Response'
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          63              2            0.010733             0.026647        33     relu    0.27291    36     none    0.17883     7     tanh    0.46316

SL: 63 HS: 33 ME: 0.040542 MSE: 0.002418
Best NN configuration
Mean error: 0.040542
Max error:  0.113508
MSE:        0.002418
RMSE:       0.049171
>> 