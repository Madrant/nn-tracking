  24?1 optimizableVariable array with properties:

    Name
    Range
    Type
    Transform
    Optimize

    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          8               2             0.07045             0.16769         44     relu    0.43794    92     relu    0.1523     3     relu    0.28462     7     none    0.077138    54     tanh    0.12808

hs_array:     44    92

ht_array length: 1lstm
    44    92

    "lstm"    "lstm"

    "relu"    "relu"

    0.4379    0.1523

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.437939
Type: lstm Hidden size: 92 Activation: relu Dropout: 0.152303
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 92 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             15% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 92.000000 MSE: 0.182193
SL: 4.528311e-02 HS: Saving to file: bayesopt/bayesopt_0.18219.mat
|====================================================================================================================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |          hs4 |         act4 |        drop4 |          hs5 |         act5 |        drop5 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |              |              |              |              |              |              |
|====================================================================================================================================================================================================================================================================================================================================================================|
|    1 | Best   |     0.18219 |      43.227 |     0.18219 |     0.18219 |            8 |            2 |      0.07045 |      0.16769 |           44 |         relu |      0.43794 |           92 |         relu |       0.1523 |            3 |         relu |      0.28462 |            7 |         none |     0.077138 |           54 |         tanh |      0.12808 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    ________

          92              3            0.016388             0.53915          6     none    0.033642    10     none    0.48308    66     none    0.053155    43     tanh    0.13973    74     none    0.022367

hs_array:      6    10    66

ht_array length: 1lstm
     6    10    66

    "lstm"    "lstm"    "lstm"

    "none"    "none"    "none"

    0.0336    0.4831    0.0532

Type: lstm Hidden size: 6 Activation: none Dropout: 0.033642
Type: lstm Hidden size: 10 Activation: none Dropout: 0.483083
Type: lstm Hidden size: 66 Activation: none Dropout: 0.053155
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 6 hidden units
     3   ''   Dropout             3% dropout
     4   ''   LSTM                LSTM with 10 hidden units
     5   ''   Dropout             48% dropout
     6   ''   LSTM                LSTM with 66 hidden units
     7   ''   Dropout             5% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 6 ME: 10.000000 MSE: 66.000000
SL: 1.866859e-01 HS: 4.667800e-02 ME: Saving to file: bayesopt/bayesopt_0.18669.mat
|    2 | Accept |     0.18669 |      11.375 |     0.18219 |     0.18265 |           92 |            3 |     0.016388 |      0.53915 |            6 |         none |     0.033642 |           10 |         none |      0.48308 |           66 |         none |     0.053155 |           43 |         tanh |      0.13973 |           74 |         none |     0.022367 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5      drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          9               3             2.6614              0.012925        82     relu    0.062704    42     relu    0.057219    48     relu    0.40094    76     tanh    0.49521    38     relu    0.0053368

hs_array:     82    42    48

ht_array length: 1lstm
    82    42    48

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0627    0.0572    0.4009

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.062704
Type: lstm Hidden size: 42 Activation: relu Dropout: 0.057219
Type: lstm Hidden size: 48 Activation: relu Dropout: 0.400938
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             6% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             6% dropout
     8   ''   LSTM                LSTM with 48 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             40% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 42.000000 MSE: 48.000000
SL: 8.041350e-02 HS: 9.352286e-03 ME: Saving to file: bayesopt/bayesopt_0.080414.mat
|    3 | Best   |    0.080414 |       40.28 |    0.080414 |    0.084641 |            9 |            3 |       2.6614 |     0.012925 |           82 |         relu |     0.062704 |           42 |         relu |     0.057219 |           48 |         relu |      0.40094 |           76 |         tanh |      0.49521 |           38 |         relu |    0.0053368 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          20              5              1.86              0.00012125        2     relu    0.10672    99     tanh    0.097471    61     none    0.26189    46     tanh    0.048322    11     relu    0.22656

hs_array:      2    99    61    46    11

ht_array length: 1lstm
     2    99    61    46    11

    "lstm"    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "tanh"    "none"    "tanh"    "relu"

    0.1067    0.0975    0.2619    0.0483    0.2266

Type: lstm Hidden size: 2 Activation: relu Dropout: 0.106716
Type: lstm Hidden size: 99 Activation: tanh Dropout: 0.097471
Type: lstm Hidden size: 61 Activation: none Dropout: 0.261892
Type: lstm Hidden size: 46 Activation: tanh Dropout: 0.048322
Type: lstm Hidden size: 11 Activation: relu Dropout: 0.226564
  17?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 2 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 99 hidden units
     6   ''   Tanh                Hyperbolic tangent
     7   ''   Dropout             10% dropout
     8   ''   LSTM                LSTM with 61 hidden units
     9   ''   Dropout             26% dropout
    10   ''   LSTM                LSTM with 46 hidden units
    11   ''   Tanh                Hyperbolic tangent
    12   ''   Dropout             5% dropout
    13   ''   LSTM                LSTM with 11 hidden units
    14   ''   ReLU                ReLU
    15   ''   Dropout             23% dropout
    16   ''   Fully Connected     1 fully connected layer
    17   ''   Regression Output   mean-squared-error
SL: 1 HS: 2 ME: 99.000000 MSE: 61.000000
SL: 46 HS: 11 ME: 0.189052 MSE: 0.048047
Saving to file: bayesopt/bayesopt_0.18905.mat
|    4 | Accept |     0.18905 |        39.1 |    0.080414 |    0.085887 |           20 |            5 |         1.86 |   0.00012125 |            2 |         relu |      0.10672 |           99 |         tanh |     0.097471 |           61 |         none |      0.26189 |           46 |         tanh |     0.048322 |           11 |         relu |      0.22656 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              3             4.9708              0.064833        71     relu    0.097069    23     relu    0.18598    47     relu    0.36559    25     tanh    0.37636     2     relu    0.15392

hs_array:     71    23    47

ht_array length: 1lstm
    71    23    47

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0971    0.1860    0.3656

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.097069
Type: lstm Hidden size: 23 Activation: relu Dropout: 0.185982
Type: lstm Hidden size: 47 Activation: relu Dropout: 0.365592
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 23 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             19% dropout
     8   ''   LSTM                LSTM with 47 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             37% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 23.000000 MSE: 47.000000
SL: 1.157454e-01 HS: 1.875246e-02 ME: Saving to file: bayesopt/bayesopt_0.11575.mat
|    5 | Accept |     0.11575 |      17.124 |    0.080414 |    0.082841 |           50 |            3 |       4.9708 |     0.064833 |           71 |         relu |     0.097069 |           23 |         relu |      0.18598 |           47 |         relu |      0.36559 |           25 |         tanh |      0.37636 |            2 |         relu |      0.15392 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5      drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _________

          94              3             0.58499             0.05001         97     relu    0.067242    47     relu    0.34484    40     relu    0.45044    97     tanh    0.26036     1     relu    0.0060967

hs_array:     97    47    40

ht_array length: 1lstm
    97    47    40

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0672    0.3448    0.4504

Type: lstm Hidden size: 97 Activation: relu Dropout: 0.067242
Type: lstm Hidden size: 47 Activation: relu Dropout: 0.344841
Type: lstm Hidden size: 40 Activation: relu Dropout: 0.450443
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 97 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 47 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             34% dropout
     8   ''   LSTM                LSTM with 40 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             45% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 97 ME: 47.000000 MSE: 40.000000
SL: 1.837924e-01 HS: 4.687072e-02 ME: Saving to file: bayesopt/bayesopt_0.18379.mat
|    6 | Accept |     0.18379 |      17.405 |    0.080414 |    0.080435 |           94 |            3 |      0.58499 |      0.05001 |           97 |         relu |     0.067242 |           47 |         relu |      0.34484 |           40 |         relu |      0.45044 |           97 |         tanh |      0.26036 |            1 |         relu |    0.0060967 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______

          12              4            0.076458             0.036606        53     relu    0.17824    74     relu    0.18639    48     relu    0.1103    12     tanh    0.30798    44     relu    0.15275

hs_array:     53    74    48    12

ht_array length: 1lstm
    53    74    48    12

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.1782    0.1864    0.1103    0.3080

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.178238
Type: lstm Hidden size: 74 Activation: relu Dropout: 0.186390
Type: lstm Hidden size: 48 Activation: relu Dropout: 0.110296
Type: lstm Hidden size: 12 Activation: tanh Dropout: 0.307978
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 74 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             19% dropout
     8   ''   LSTM                LSTM with 48 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             11% dropout
    11   ''   LSTM                LSTM with 12 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             31% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 74.000000 MSE: 48.000000
SL: 12 HS: 1.824103e-01 ME: 0.045324 MSE: Saving to file: bayesopt/bayesopt_0.18241.mat
|    7 | Accept |     0.18241 |      44.286 |    0.080414 |    0.080428 |           12 |            4 |     0.076458 |     0.036606 |           53 |         relu |      0.17824 |           74 |         relu |      0.18639 |           48 |         relu |       0.1103 |           12 |         tanh |      0.30798 |           44 |         relu |      0.15275 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _____

          62              3             4.5763               0.1481         65     relu    0.49378    67     relu    0.49411    48     relu    0.49405    50     tanh    0.25554    89     relu    0.215

hs_array:     65    67    48

ht_array length: 1lstm
    65    67    48

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.4938    0.4941    0.4940

Type: lstm Hidden size: 65 Activation: relu Dropout: 0.493778
Type: lstm Hidden size: 67 Activation: relu Dropout: 0.494114
Type: lstm Hidden size: 48 Activation: relu Dropout: 0.494050
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 65 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 67 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             49% dropout
     8   ''   LSTM                LSTM with 48 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             49% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 65 ME: 67.000000 MSE: 48.000000
SL: 1.835159e-01 HS: 4.555882e-02 ME: Saving to file: bayesopt/bayesopt_0.18352.mat
|    8 | Accept |     0.18352 |      17.462 |    0.080414 |    0.080428 |           62 |            3 |       4.5763 |       0.1481 |           65 |         relu |      0.49378 |           67 |         relu |      0.49411 |           48 |         relu |      0.49405 |           50 |         tanh |      0.25554 |           89 |         relu |        0.215 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          29              3             0.10335             0.67409         42     relu    0.16975    19     relu    0.10622    51     relu    0.45282    93     tanh    0.39438    13     relu    0.45281

hs_array:     42    19    51

ht_array length: 1lstm
    42    19    51

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1697    0.1062    0.4528

Type: lstm Hidden size: 42 Activation: relu Dropout: 0.169748
Type: lstm Hidden size: 19 Activation: relu Dropout: 0.106218
Type: lstm Hidden size: 51 Activation: relu Dropout: 0.452819
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 42 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             17% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             11% dropout
     8   ''   LSTM                LSTM with 51 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             45% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 42 ME: 19.000000 MSE: 51.000000
SL: 3.025748e-01 HS: 1.309696e-01 ME: Saving to file: bayesopt/bayesopt_0.30257.mat
|    9 | Accept |     0.30257 |      18.691 |    0.080414 |    0.080431 |           29 |            3 |      0.10335 |      0.67409 |           42 |         relu |      0.16975 |           19 |         relu |      0.10622 |           51 |         relu |      0.45282 |           93 |         tanh |      0.39438 |           13 |         relu |      0.45281 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          72              3             1.9672              0.12854         61     relu    0.15047    64     relu    0.051298    53     relu    0.23213    21     tanh    0.27903     3     relu    0.15408

hs_array:     61    64    53

ht_array length: 1lstm
    61    64    53

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1505    0.0513    0.2321

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.150472
Type: lstm Hidden size: 64 Activation: relu Dropout: 0.051298
Type: lstm Hidden size: 53 Activation: relu Dropout: 0.232132
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             5% dropout
     8   ''   LSTM                LSTM with 53 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             23% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 64.000000 MSE: 53.000000
SL: 1.819778e-01 HS: 4.522072e-02 ME: Saving to file: bayesopt/bayesopt_0.18198.mat
|   10 | Accept |     0.18198 |      19.622 |    0.080414 |    0.080429 |           72 |            3 |       1.9672 |      0.12854 |           61 |         relu |      0.15047 |           64 |         relu |     0.051298 |           53 |         relu |      0.23213 |           21 |         tanh |      0.27903 |            3 |         relu |      0.15408 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          44              3             3.3363             0.0012496        55     relu    0.10108     6     relu    0.26249    75     relu    0.45844    73     tanh    0.11504    53     relu    0.49489

hs_array:     55     6    75

ht_array length: 1lstm
    55     6    75

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1011    0.2625    0.4584

Type: lstm Hidden size: 55 Activation: relu Dropout: 0.101075
Type: lstm Hidden size: 6 Activation: relu Dropout: 0.262494
Type: lstm Hidden size: 75 Activation: relu Dropout: 0.458439
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 55 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             26% dropout
     8   ''   LSTM                LSTM with 75 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             46% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 55 ME: 6.000000 MSE: 75.000000
SL: 1.847400e-01 HS: 4.630290e-02 ME: Saving to file: bayesopt/bayesopt_0.18474.mat
|   11 | Accept |     0.18474 |      16.663 |    0.080414 |    0.080428 |           44 |            3 |       3.3363 |    0.0012496 |           55 |         relu |      0.10108 |            6 |         relu |      0.26249 |           75 |         relu |      0.45844 |           73 |         tanh |      0.11504 |           53 |         relu |      0.49489 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          9               3              5.337              0.082998        100    relu    0.10041    10     relu    0.29064    50     relu    0.47491    19     tanh    0.25836    13     relu    0.16477

hs_array:    100    10    50

ht_array length: 1lstm
   100    10    50

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1004    0.2906    0.4749

Type: lstm Hidden size: 100 Activation: relu Dropout: 0.100412
Type: lstm Hidden size: 10 Activation: relu Dropout: 0.290636
Type: lstm Hidden size: 50 Activation: relu Dropout: 0.474905
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 100 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             29% dropout
     8   ''   LSTM                LSTM with 50 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             47% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 100 ME: 10.000000 MSE: 50.000000
SL: 1.884919e-01 HS: 4.834470e-02 ME: Saving to file: bayesopt/bayesopt_0.18849.mat
|   12 | Accept |     0.18849 |       41.47 |    0.080414 |    0.080421 |            9 |            3 |        5.337 |     0.082998 |          100 |         relu |      0.10041 |           10 |         relu |      0.29064 |           50 |         relu |      0.47491 |           19 |         tanh |      0.25836 |           13 |         relu |      0.16477 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          12              4            0.018931            0.0013107        78     relu    0.015726    86     relu    0.49972    28     relu    0.10455    14     tanh    0.14816    73     relu    0.42408

hs_array:     78    86    28    14

ht_array length: 1lstm
    78    86    28    14

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.0157    0.4997    0.1046    0.1482

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.015726
Type: lstm Hidden size: 86 Activation: relu Dropout: 0.499723
Type: lstm Hidden size: 28 Activation: relu Dropout: 0.104551
Type: lstm Hidden size: 14 Activation: tanh Dropout: 0.148163
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             50% dropout
     8   ''   LSTM                LSTM with 28 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             10% dropout
    11   ''   LSTM                LSTM with 14 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             15% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 86.000000 MSE: 28.000000
SL: 14 HS: 1.818932e-01 ME: 0.045153 MSE: Saving to file: bayesopt/bayesopt_0.18189.mat
|   13 | Accept |     0.18189 |      44.623 |    0.080414 |    0.080421 |           12 |            4 |     0.018931 |    0.0013107 |           78 |         relu |     0.015726 |           86 |         relu |      0.49972 |           28 |         relu |      0.10455 |           14 |         tanh |      0.14816 |           73 |         relu |      0.42408 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          4               4            0.037019            0.00098897       79     relu    0.2863    20     relu    0.46859    22     relu    0.37661    94     tanh    0.48142     5     relu    0.44584

hs_array:     79    20    22    94

ht_array length: 1lstm
    79    20    22    94

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.2863    0.4686    0.3766    0.4814

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.286302
Type: lstm Hidden size: 20 Activation: relu Dropout: 0.468587
Type: lstm Hidden size: 22 Activation: relu Dropout: 0.376607
Type: lstm Hidden size: 94 Activation: tanh Dropout: 0.481419
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             29% dropout
     5   ''   LSTM                LSTM with 20 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             47% dropout
     8   ''   LSTM                LSTM with 22 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             38% dropout
    11   ''   LSTM                LSTM with 94 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             48% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 20.000000 MSE: 22.000000
SL: 94 HS: 1.891433e-01 ME: 0.049951 MSE: Saving to file: bayesopt/bayesopt_0.18914.mat
|   14 | Accept |     0.18914 |      97.844 |    0.080414 |    0.080426 |            4 |            4 |     0.037019 |   0.00098897 |           79 |         relu |       0.2863 |           20 |         relu |      0.46859 |           22 |         relu |      0.37661 |           94 |         tanh |      0.48142 |            5 |         relu |      0.44584 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          33              3             1.1063             0.00060284       79     relu    0.30254    16     relu    0.17325    29     relu    0.10363    55     tanh    0.44981    69     relu    0.24873

hs_array:     79    16    29

ht_array length: 1lstm
    79    16    29

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.3025    0.1733    0.1036

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.302543
Type: lstm Hidden size: 16 Activation: relu Dropout: 0.173252
Type: lstm Hidden size: 29 Activation: relu Dropout: 0.103631
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 16 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             17% dropout
     8   ''   LSTM                LSTM with 29 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             10% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 16.000000 MSE: 29.000000
SL: 1.852809e-01 HS: 4.641079e-02 ME: Saving to file: bayesopt/bayesopt_0.18528.mat
|   15 | Accept |     0.18528 |      18.753 |    0.080414 |    0.080427 |           33 |            3 |       1.1063 |   0.00060284 |           79 |         relu |      0.30254 |           16 |         relu |      0.17325 |           29 |         relu |      0.10363 |           55 |         tanh |      0.44981 |           69 |         relu |      0.24873 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          88              5             7.4083             0.0003927        80     relu    0.28483    88     relu    0.4294    27     relu    0.39534    38     tanh    0.15711    94     relu    0.39539

hs_array:     80    88    27    38    94

ht_array length: 1lstm
    80    88    27    38    94

    "lstm"    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"    "relu"

    0.2848    0.4294    0.3953    0.1571    0.3954

Type: lstm Hidden size: 80 Activation: relu Dropout: 0.284832
Type: lstm Hidden size: 88 Activation: relu Dropout: 0.429403
Type: lstm Hidden size: 27 Activation: relu Dropout: 0.395340
Type: lstm Hidden size: 38 Activation: tanh Dropout: 0.157111
Type: lstm Hidden size: 94 Activation: relu Dropout: 0.395394
  18?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 80 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 88 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             43% dropout
     8   ''   LSTM                LSTM with 27 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             40% dropout
    11   ''   LSTM                LSTM with 38 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             16% dropout
    14   ''   LSTM                LSTM with 94 hidden units
    15   ''   ReLU                ReLU
    16   ''   Dropout             40% dropout
    17   ''   Fully Connected     1 fully connected layer
    18   ''   Regression Output   mean-squared-error
SL: 1 HS: 80 ME: 88.000000 MSE: 27.000000
SL: 38 HS: 94 ME: 0.199457 MSE: 0.052493
Saving to file: bayesopt/bayesopt_0.19946.mat
|   16 | Accept |     0.19946 |      32.899 |    0.080414 |    0.080425 |           88 |            5 |       7.4083 |    0.0003927 |           80 |         relu |      0.28483 |           88 |         relu |       0.4294 |           27 |         relu |      0.39534 |           38 |         tanh |      0.15711 |           94 |         relu |      0.39539 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ______

          94              3             3.4213             0.00018874       77     relu    0.37195    42     relu    0.33059    55     relu    0.10349    86     tanh    0.37777    73     relu    0.2783

hs_array:     77    42    55

ht_array length: 1lstm
    77    42    55

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.3719    0.3306    0.1035

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.371947
Type: lstm Hidden size: 42 Activation: relu Dropout: 0.330586
Type: lstm Hidden size: 55 Activation: relu Dropout: 0.103492
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             33% dropout
     8   ''   LSTM                LSTM with 55 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             10% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 42.000000 MSE: 55.000000
SL: 2.053062e-01 HS: 5.661639e-02 ME: Saving to file: bayesopt/bayesopt_0.20531.mat
|   17 | Accept |     0.20531 |       15.43 |    0.080414 |    0.080426 |           94 |            3 |       3.4213 |   0.00018874 |           77 |         relu |      0.37195 |           42 |         relu |      0.33059 |           55 |         relu |      0.10349 |           86 |         tanh |      0.37777 |           73 |         relu |       0.2783 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          43              3             1.5801             0.0010032        75     relu    0.15408    41     relu    0.38316    69     relu    0.49143    71     tanh    0.036484    71     relu    0.35159

hs_array:     75    41    69

ht_array length: 1lstm
    75    41    69

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1541    0.3832    0.4914

Type: lstm Hidden size: 75 Activation: relu Dropout: 0.154085
Type: lstm Hidden size: 41 Activation: relu Dropout: 0.383159
Type: lstm Hidden size: 69 Activation: relu Dropout: 0.491428
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 75 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 41 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             38% dropout
     8   ''   LSTM                LSTM with 69 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             49% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 75 ME: 41.000000 MSE: 69.000000
SL: 1.831604e-01 HS: 4.551458e-02 ME: Saving to file: bayesopt/bayesopt_0.18316.mat
|   18 | Accept |     0.18316 |      20.042 |    0.080414 |    0.080425 |           43 |            3 |       1.5801 |    0.0010032 |           75 |         relu |      0.15408 |           41 |         relu |      0.38316 |           69 |         relu |      0.49143 |           71 |         tanh |     0.036484 |           71 |         relu |      0.35159 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3     hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______    ___    ____    ______    ___    ____    _______

          99              3             3.7021              0.22444         80     relu    0.015779    56     relu    0.15703    59     relu    0.3906    57     tanh    0.3978    86     relu    0.31166

hs_array:     80    56    59

ht_array length: 1lstm
    80    56    59

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0158    0.1570    0.3906

Type: lstm Hidden size: 80 Activation: relu Dropout: 0.015779
Type: lstm Hidden size: 56 Activation: relu Dropout: 0.157027
Type: lstm Hidden size: 59 Activation: relu Dropout: 0.390601
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 80 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 56 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             16% dropout
     8   ''   LSTM                LSTM with 59 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             39% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 80 ME: 56.000000 MSE: 59.000000
SL: 1.892702e-01 HS: 5.116011e-02 ME: Saving to file: bayesopt/bayesopt_0.18927.mat
|   19 | Accept |     0.18927 |      17.855 |    0.080414 |    0.080425 |           99 |            3 |       3.7021 |      0.22444 |           80 |         relu |     0.015779 |           56 |         relu |      0.15703 |           59 |         relu |       0.3906 |           57 |         tanh |       0.3978 |           86 |         relu |      0.31166 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    ________

          62              2             2.7199              0.01463         12     relu    0.013881    68     relu    0.10011    60     relu    0.37964    12     tanh    0.002034    75     relu    0.061928

hs_array:     12    68

ht_array length: 1lstm
    12    68

    "lstm"    "lstm"

    "relu"    "relu"

    0.0139    0.1001

Type: lstm Hidden size: 12 Activation: relu Dropout: 0.013881
Type: lstm Hidden size: 68 Activation: relu Dropout: 0.100110
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 12 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             10% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 12 ME: 68.000000 MSE: 0.054656
SL: 4.330916e-03 HS: Saving to file: bayesopt/bayesopt_0.054656.mat
|   20 | Best   |    0.054656 |      9.9985 |    0.054656 |    0.054678 |           62 |            2 |       2.7199 |      0.01463 |           12 |         relu |     0.013881 |           68 |         relu |      0.10011 |           60 |         relu |      0.37964 |           12 |         tanh |     0.002034 |           75 |         relu |     0.061928 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          76              3            0.061004             0.011641        92     relu    0.4838    95     relu    0.40206    97     relu    0.37136    28     tanh    0.10538    61     relu    0.27604

hs_array:     92    95    97

ht_array length: 1lstm
    92    95    97

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.4838    0.4021    0.3714

Type: lstm Hidden size: 92 Activation: relu Dropout: 0.483801
Type: lstm Hidden size: 95 Activation: relu Dropout: 0.402063
Type: lstm Hidden size: 97 Activation: relu Dropout: 0.371361
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 92 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             40% dropout
     8   ''   LSTM                LSTM with 97 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             37% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 92 ME: 95.000000 MSE: 97.000000
SL: 7.846990e-02 HS: 1.127917e-02 ME: Saving to file: bayesopt/bayesopt_0.07847.mat
|====================================================================================================================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |          hs4 |         act4 |        drop4 |          hs5 |         act5 |        drop5 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |              |              |              |              |              |              |
|====================================================================================================================================================================================================================================================================================================================================================================|
|   21 | Accept |     0.07847 |      28.698 |    0.054656 |    0.054675 |           76 |            3 |     0.061004 |     0.011641 |           92 |         relu |       0.4838 |           95 |         relu |      0.40206 |           97 |         relu |      0.37136 |           28 |         tanh |      0.10538 |           61 |         relu |      0.27604 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          32              3              1.267             0.00065702        2     relu    0.20936     7     relu    0.48056    86     relu    0.37954    89     tanh    0.41143    54     relu    0.13096

hs_array:      2     7    86

ht_array length: 1lstm
     2     7    86

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.2094    0.4806    0.3795

Type: lstm Hidden size: 2 Activation: relu Dropout: 0.209361
Type: lstm Hidden size: 7 Activation: relu Dropout: 0.480562
Type: lstm Hidden size: 86 Activation: relu Dropout: 0.379541
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 2 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 7 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             48% dropout
     8   ''   LSTM                LSTM with 86 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             38% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 2 ME: 7.000000 MSE: 86.000000
SL: 1.856895e-01 HS: 4.704899e-02 ME: Saving to file: bayesopt/bayesopt_0.18569.mat
|   22 | Accept |     0.18569 |      18.661 |    0.054656 |    0.054693 |           32 |            3 |        1.267 |   0.00065702 |            2 |         relu |      0.20936 |            7 |         relu |      0.48056 |           86 |         relu |      0.37954 |           89 |         tanh |      0.41143 |           54 |         relu |      0.13096 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          58              3             0.35348             0.011537         7     relu    0.17841    32     relu    0.41477    38     relu    0.32857    81     tanh    0.094186    24     relu    0.30947

hs_array:      7    32    38

ht_array length: 1lstm
     7    32    38

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1784    0.4148    0.3286

Type: lstm Hidden size: 7 Activation: relu Dropout: 0.178410
Type: lstm Hidden size: 32 Activation: relu Dropout: 0.414775
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.328567
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 7 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 32 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             41% dropout
     8   ''   LSTM                LSTM with 38 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             33% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 7 ME: 32.000000 MSE: 38.000000
SL: 1.863490e-01 HS: 4.695636e-02 ME: Saving to file: bayesopt/bayesopt_0.18635.mat
|   23 | Accept |     0.18635 |      12.444 |    0.054656 |    0.054695 |           58 |            3 |      0.35348 |     0.011537 |            7 |         relu |      0.17841 |           32 |         relu |      0.41477 |           38 |         relu |      0.32857 |           81 |         tanh |     0.094186 |           24 |         relu |      0.30947 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5      drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _________

          83              4             4.3275              0.022339        64     relu    0.38043    12     relu    0.36298    94     relu    0.38687     5     tanh    0.35426    58     relu    0.0039758

hs_array:     64    12    94     5

ht_array length: 1lstm
    64    12    94     5

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.3804    0.3630    0.3869    0.3543

Type: lstm Hidden size: 64 Activation: relu Dropout: 0.380431
Type: lstm Hidden size: 12 Activation: relu Dropout: 0.362983
Type: lstm Hidden size: 94 Activation: relu Dropout: 0.386868
Type: lstm Hidden size: 5 Activation: tanh Dropout: 0.354258
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 64 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 12 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             36% dropout
     8   ''   LSTM                LSTM with 94 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             39% dropout
    11   ''   LSTM                LSTM with 5 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             35% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 64 ME: 12.000000 MSE: 94.000000
SL: 5 HS: 1.862848e-01 ME: 0.046548 MSE: Saving to file: bayesopt/bayesopt_0.18628.mat
|   24 | Accept |     0.18628 |      19.333 |    0.054656 |    0.054672 |           83 |            4 |       4.3275 |     0.022339 |           64 |         relu |      0.38043 |           12 |         relu |      0.36298 |           94 |         relu |      0.38687 |            5 |         tanh |      0.35426 |           58 |         relu |    0.0039758 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________

          22              3             6.4919              0.010416        77     relu    0.038479    42     relu    0.13997    47     relu    0.36882    39     tanh    0.19539    23     relu    0.005269

hs_array:     77    42    47

ht_array length: 1lstm
    77    42    47

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0385    0.1400    0.3688

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.038479
Type: lstm Hidden size: 42 Activation: relu Dropout: 0.139966
Type: lstm Hidden size: 47 Activation: relu Dropout: 0.368821
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             14% dropout
     8   ''   LSTM                LSTM with 47 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             37% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 42.000000 MSE: 47.000000
SL: 6.430605e-02 HS: 6.145413e-03 ME: Saving to file: bayesopt/bayesopt_0.064306.mat
|   25 | Accept |    0.064306 |      22.532 |    0.054656 |    0.054673 |           22 |            3 |       6.4919 |     0.010416 |           77 |         relu |     0.038479 |           42 |         relu |      0.13997 |           47 |         relu |      0.36882 |           39 |         tanh |      0.19539 |           23 |         relu |     0.005269 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          94              5            0.036251             0.022805        71     relu    0.43139    59     relu    0.35814    96     relu    0.36903    32     tanh    0.04857    34     relu    0.29075

hs_array:     71    59    96    32    34

ht_array length: 1lstm
    71    59    96    32    34

    "lstm"    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"    "relu"

    0.4314    0.3581    0.3690    0.0486    0.2908

Type: lstm Hidden size: 71 Activation: relu Dropout: 0.431393
Type: lstm Hidden size: 59 Activation: relu Dropout: 0.358144
Type: lstm Hidden size: 96 Activation: relu Dropout: 0.369029
Type: lstm Hidden size: 32 Activation: tanh Dropout: 0.048570
Type: lstm Hidden size: 34 Activation: relu Dropout: 0.290752
  18?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 71 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             43% dropout
     5   ''   LSTM                LSTM with 59 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             36% dropout
     8   ''   LSTM                LSTM with 96 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             37% dropout
    11   ''   LSTM                LSTM with 32 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             5% dropout
    14   ''   LSTM                LSTM with 34 hidden units
    15   ''   ReLU                ReLU
    16   ''   Dropout             29% dropout
    17   ''   Fully Connected     1 fully connected layer
    18   ''   Regression Output   mean-squared-error
SL: 1 HS: 71 ME: 59.000000 MSE: 96.000000
SL: 32 HS: 34 ME: 0.183030 MSE: 0.045463
Saving to file: bayesopt/bayesopt_0.18303.mat
|   26 | Accept |     0.18303 |      26.405 |    0.054656 |    0.054669 |           94 |            5 |     0.036251 |     0.022805 |           71 |         relu |      0.43139 |           59 |         relu |      0.35814 |           96 |         relu |      0.36903 |           32 |         tanh |      0.04857 |           34 |         relu |      0.29075 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5      drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _________

          43              3             1.1897             0.0063429        33     relu    0.033397     6     relu    0.059623    15     relu    0.37546    67     tanh    0.41728    13     relu    0.0057683

hs_array:     33     6    15

ht_array length: 1lstm
    33     6    15

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0334    0.0596    0.3755

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.033397
Type: lstm Hidden size: 6 Activation: relu Dropout: 0.059623
Type: lstm Hidden size: 15 Activation: relu Dropout: 0.375461
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             6% dropout
     8   ''   LSTM                LSTM with 15 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             38% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 6.000000 MSE: 15.000000
SL: 1.854025e-01 HS: 4.621834e-02 ME: Saving to file: bayesopt/bayesopt_0.1854.mat
|   27 | Accept |      0.1854 |      11.878 |    0.054656 |    0.054664 |           43 |            3 |       1.1897 |    0.0063429 |           33 |         relu |     0.033397 |            6 |         relu |     0.059623 |           15 |         relu |      0.37546 |           67 |         tanh |      0.41728 |           13 |         relu |    0.0057683 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          40              2              2.089             0.0066895        89     relu    0.15428    54     relu    0.19631    85     relu    0.38047    19     tanh    0.12866    91     relu    0.38482

hs_array:     89    54

ht_array length: 1lstm
    89    54

    "lstm"    "lstm"

    "relu"    "relu"

    0.1543    0.1963

Type: lstm Hidden size: 89 Activation: relu Dropout: 0.154275
Type: lstm Hidden size: 54 Activation: relu Dropout: 0.196307
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 89 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 54 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             20% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 89 ME: 54.000000 MSE: 0.048844
SL: 3.345718e-03 HS: Saving to file: bayesopt/bayesopt_0.048844.mat
|   28 | Best   |    0.048844 |      15.892 |    0.048844 |    0.048855 |           40 |            2 |        2.089 |    0.0066895 |           89 |         relu |      0.15428 |           54 |         relu |      0.19631 |           85 |         relu |      0.38047 |           19 |         tanh |      0.12866 |           91 |         relu |      0.38482 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          8               2             9.5269             0.0078805        82     relu    0.24969    40     relu    0.090388     5     relu    0.23218    43     tanh    0.15657    95     relu    0.40285

hs_array:     82    40

ht_array length: 1lstm
    82    40

    "lstm"    "lstm"

    "relu"    "relu"

    0.2497    0.0904

Type: lstm Hidden size: 82 Activation: relu Dropout: 0.249687
Type: lstm Hidden size: 40 Activation: relu Dropout: 0.090388
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 82 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 40 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             9% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 82 ME: 40.000000 MSE: 0.061793
SL: 6.104649e-03 HS: Saving to file: bayesopt/bayesopt_0.061793.mat
|   29 | Accept |    0.061793 |      31.278 |    0.048844 |    0.048861 |            8 |            2 |       9.5269 |    0.0078805 |           82 |         relu |      0.24969 |           40 |         relu |     0.090388 |            5 |         relu |      0.23218 |           43 |         tanh |      0.15657 |           95 |         relu |      0.40285 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3    hs4    act4     drop4      hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _____    ___    ____    ________    ___    ____    ________

          37              4              2.233             0.0071345        40     relu    0.34914    62     relu    0.079564    19     relu    0.256    38     tanh    0.077593    55     relu    0.049448

hs_array:     40    62    19    38

ht_array length: 1lstm
    40    62    19    38

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.3491    0.0796    0.2560    0.0776

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.349139
Type: lstm Hidden size: 62 Activation: relu Dropout: 0.079564
Type: lstm Hidden size: 19 Activation: relu Dropout: 0.256002
Type: lstm Hidden size: 38 Activation: tanh Dropout: 0.077593
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             35% dropout
     5   ''   LSTM                LSTM with 62 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             8% dropout
     8   ''   LSTM                LSTM with 19 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             26% dropout
    11   ''   LSTM                LSTM with 38 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             8% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 62.000000 MSE: 19.000000
SL: 38 HS: 7.824610e-02 ME: 0.008872 MSE: Saving to file: bayesopt/bayesopt_0.078246.mat
|   30 | Accept |    0.078246 |      20.644 |    0.048844 |     0.04887 |           37 |            4 |        2.233 |    0.0071345 |           40 |         relu |      0.34914 |           62 |         relu |     0.079564 |           19 |         relu |        0.256 |           38 |         tanh |     0.077593 |           55 |         relu |     0.049448 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          28              2              1.527             0.0022459        21     relu    0.12599    91     relu    0.28217    15     relu    0.095704    48     tanh    0.29007    44     relu    0.11804

hs_array:     21    91

ht_array length: 1lstm
    21    91

    "lstm"    "lstm"

    "relu"    "relu"

    0.1260    0.2822

Type: lstm Hidden size: 21 Activation: relu Dropout: 0.125987
Type: lstm Hidden size: 91 Activation: relu Dropout: 0.282167
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 21 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             28% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 21 ME: 91.000000 MSE: 0.053115
SL: 4.142153e-03 HS: Saving to file: bayesopt/bayesopt_0.053115.mat
|   31 | Accept |    0.053115 |      15.689 |    0.048844 |    0.048864 |           28 |            2 |        1.527 |    0.0022459 |           21 |         relu |      0.12599 |           91 |         relu |      0.28217 |           15 |         relu |     0.095704 |           48 |         tanh |      0.29007 |           44 |         relu |      0.11804 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          91              3             0.65361            0.0030222        12     relu    0.012892    74     relu    0.05911     6     relu    0.040349    83     tanh    0.11607    55     relu    0.19366

hs_array:     12    74     6

ht_array length: 1lstm
    12    74     6

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0129    0.0591    0.0403

Type: lstm Hidden size: 12 Activation: relu Dropout: 0.012892
Type: lstm Hidden size: 74 Activation: relu Dropout: 0.059110
Type: lstm Hidden size: 6 Activation: relu Dropout: 0.040349
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 12 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 74 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             6% dropout
     8   ''   LSTM                LSTM with 6 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             4% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 12 ME: 74.000000 MSE: 6.000000
SL: 7.963735e-02 HS: 8.966255e-03 ME: Saving to file: bayesopt/bayesopt_0.079637.mat
|   32 | Accept |    0.079637 |      13.981 |    0.048844 |    0.048867 |           91 |            3 |      0.65361 |    0.0030222 |           12 |         relu |     0.012892 |           74 |         relu |      0.05911 |            6 |         relu |     0.040349 |           83 |         tanh |      0.11607 |           55 |         relu |      0.19366 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    ______    ___    ____    _______

          72              3             1.9395             0.0063168        58     relu    0.20815    90     relu    0.087406    69     relu    0.43398    54     tanh    0.2588    32     relu    0.35512

hs_array:     58    90    69

ht_array length: 1lstm
    58    90    69

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.2082    0.0874    0.4340

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.208152
Type: lstm Hidden size: 90 Activation: relu Dropout: 0.087406
Type: lstm Hidden size: 69 Activation: relu Dropout: 0.433979
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             21% dropout
     5   ''   LSTM                LSTM with 90 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             9% dropout
     8   ''   LSTM                LSTM with 69 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             43% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 90.000000 MSE: 69.000000
SL: 4.605566e-02 HS: 3.138157e-03 ME: Saving to file: bayesopt/bayesopt_0.046056.mat
|   33 | Best   |    0.046056 |      23.003 |    0.046056 |     0.04606 |           72 |            3 |       1.9395 |    0.0063168 |           58 |         relu |      0.20815 |           90 |         relu |     0.087406 |           69 |         relu |      0.43398 |           54 |         tanh |       0.2588 |           32 |         relu |      0.35512 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          45              2             3.8698             0.0051137        45     relu    0.19117    81     relu    0.12646    74     relu    0.24605    29     tanh    0.46845     3     relu    0.45826

hs_array:     45    81

ht_array length: 1lstm
    45    81

    "lstm"    "lstm"

    "relu"    "relu"

    0.1912    0.1265

Type: lstm Hidden size: 45 Activation: relu Dropout: 0.191169
Type: lstm Hidden size: 81 Activation: relu Dropout: 0.126456
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 45 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             13% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 45 ME: 81.000000 MSE: 0.038720
SL: 2.266817e-03 HS: Saving to file: bayesopt/bayesopt_0.03872.mat
|   34 | Best   |     0.03872 |      14.561 |     0.03872 |    0.038687 |           45 |            2 |       3.8698 |    0.0051137 |           45 |         relu |      0.19117 |           81 |         relu |      0.12646 |           74 |         relu |      0.24605 |           29 |         tanh |      0.46845 |            3 |         relu |      0.45826 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______

          38              2             0.35326            0.0044804        55     relu    0.085218    94     relu    0.26831    35     relu    0.2889    67     tanh    0.38125    89     relu    0.38004

hs_array:     55    94

ht_array length: 1lstm
    55    94

    "lstm"    "lstm"

    "relu"    "relu"

    0.0852    0.2683

Type: lstm Hidden size: 55 Activation: relu Dropout: 0.085218
Type: lstm Hidden size: 94 Activation: relu Dropout: 0.268307
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 55 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             9% dropout
     5   ''   LSTM                LSTM with 94 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             27% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 55 ME: 94.000000 MSE: 0.059217
SL: 5.063791e-03 HS: Saving to file: bayesopt/bayesopt_0.059217.mat
|   35 | Accept |    0.059217 |      16.436 |     0.03872 |    0.038712 |           38 |            2 |      0.35326 |    0.0044804 |           55 |         relu |     0.085218 |           94 |         relu |      0.26831 |           35 |         relu |       0.2889 |           67 |         tanh |      0.38125 |           89 |         relu |      0.38004 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          52              5             0.21663            0.0055735        20     relu    0.36389    96     relu    0.086606    30     relu    0.46565    22     tanh    0.38342    39     relu    0.18694

hs_array:     20    96    30    22    39

ht_array length: 1lstm
    20    96    30    22    39

    "lstm"    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"    "relu"

    0.3639    0.0866    0.4656    0.3834    0.1869

Type: lstm Hidden size: 20 Activation: relu Dropout: 0.363888
Type: lstm Hidden size: 96 Activation: relu Dropout: 0.086606
Type: lstm Hidden size: 30 Activation: relu Dropout: 0.465649
Type: lstm Hidden size: 22 Activation: tanh Dropout: 0.383421
Type: lstm Hidden size: 39 Activation: relu Dropout: 0.186939
  18?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 20 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 96 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             9% dropout
     8   ''   LSTM                LSTM with 30 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             47% dropout
    11   ''   LSTM                LSTM with 22 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             38% dropout
    14   ''   LSTM                LSTM with 39 hidden units
    15   ''   ReLU                ReLU
    16   ''   Dropout             19% dropout
    17   ''   Fully Connected     1 fully connected layer
    18   ''   Regression Output   mean-squared-error
SL: 1 HS: 20 ME: 96.000000 MSE: 30.000000
SL: 22 HS: 39 ME: 0.073184 MSE: 0.008529
Saving to file: bayesopt/bayesopt_0.073184.mat
|   36 | Accept |    0.073184 |      27.081 |     0.03872 |     0.03869 |           52 |            5 |      0.21663 |    0.0055735 |           20 |         relu |      0.36389 |           96 |         relu |     0.086606 |           30 |         relu |      0.46565 |           22 |         tanh |      0.38342 |           39 |         relu |      0.18694 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    ________

          7               4             1.2289             0.0068443        96     relu    0.25562    100    relu    0.11143    61     relu    0.018135    17     tanh    0.48099    37     relu    0.013108

hs_array:     96   100    61    17

ht_array length: 1lstm
    96   100    61    17

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.2556    0.1114    0.0181    0.4810

Type: lstm Hidden size: 96 Activation: relu Dropout: 0.255622
Type: lstm Hidden size: 100 Activation: relu Dropout: 0.111434
Type: lstm Hidden size: 61 Activation: relu Dropout: 0.018135
Type: lstm Hidden size: 17 Activation: tanh Dropout: 0.480990
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 96 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 100 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             11% dropout
     8   ''   LSTM                LSTM with 61 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             2% dropout
    11   ''   LSTM                LSTM with 17 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             48% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 96 ME: 100.000000 MSE: 61.000000
SL: 17 HS: 1.843168e-01 ME: 0.046178 MSE: Saving to file: bayesopt/bayesopt_0.18432.mat
|   37 | Accept |     0.18432 |      77.924 |     0.03872 |    0.038714 |            7 |            4 |       1.2289 |    0.0068443 |           96 |         relu |      0.25562 |          100 |         relu |      0.11143 |           61 |         relu |     0.018135 |           17 |         tanh |      0.48099 |           37 |         relu |     0.013108 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1      drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          6               2             0.3087              0.003443        73     relu    0.0086037    83     relu    0.14738    28     relu    0.48494     1     tanh    0.43555    92     relu    0.20483

hs_array:     73    83

ht_array length: 1lstm
    73    83

    "lstm"    "lstm"

    "relu"    "relu"

    0.0086    0.1474

Type: lstm Hidden size: 73 Activation: relu Dropout: 0.008604
Type: lstm Hidden size: 83 Activation: relu Dropout: 0.147377
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 73 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 83 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             15% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 73 ME: 83.000000 MSE: 0.069843
SL: 7.530652e-03 HS: Saving to file: bayesopt/bayesopt_0.069843.mat
|   38 | Accept |    0.069843 |      43.726 |     0.03872 |    0.038714 |            6 |            2 |       0.3087 |     0.003443 |           73 |         relu |    0.0086037 |           83 |         relu |      0.14738 |           28 |         relu |      0.48494 |            1 |         tanh |      0.43555 |           92 |         relu |      0.20483 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________

          60              3             1.4688             0.0060391        61     relu    0.13968    78     relu    0.18452    66     relu    0.37873    59     tanh    0.21197     3     relu    0.081281

hs_array:     61    78    66

ht_array length: 1lstm
    61    78    66

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1397    0.1845    0.3787

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.139678
Type: lstm Hidden size: 78 Activation: relu Dropout: 0.184519
Type: lstm Hidden size: 66 Activation: relu Dropout: 0.378732
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 78 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             18% dropout
     8   ''   LSTM                LSTM with 66 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             38% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 78.000000 MSE: 66.000000
SL: 1.870549e-01 HS: 4.747146e-02 ME: Saving to file: bayesopt/bayesopt_0.18705.mat
|   39 | Accept |     0.18705 |       22.54 |     0.03872 |    0.038736 |           60 |            3 |       1.4688 |    0.0060391 |           61 |         relu |      0.13968 |           78 |         relu |      0.18452 |           66 |         relu |      0.37873 |           59 |         tanh |      0.21197 |            3 |         relu |     0.081281 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          56              3             1.9931              0.015637        29     relu    0.22015    77     relu    0.48094     9     relu    0.38692     6     tanh    0.48023    42     relu    0.49121

hs_array:     29    77     9

ht_array length: 1lstm
    29    77     9

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.2202    0.4809    0.3869

Type: lstm Hidden size: 29 Activation: relu Dropout: 0.220151
Type: lstm Hidden size: 77 Activation: relu Dropout: 0.480940
Type: lstm Hidden size: 9 Activation: relu Dropout: 0.386916
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 29 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             22% dropout
     5   ''   LSTM                LSTM with 77 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             48% dropout
     8   ''   LSTM                LSTM with 9 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             39% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 29 ME: 77.000000 MSE: 9.000000
SL: 7.988541e-02 HS: 1.062511e-02 ME: Saving to file: bayesopt/bayesopt_0.079885.mat
|   40 | Accept |    0.079885 |      14.686 |     0.03872 |    0.038738 |           56 |            3 |       1.9931 |     0.015637 |           29 |         relu |      0.22015 |           77 |         relu |      0.48094 |            9 |         relu |      0.38692 |            6 |         tanh |      0.48023 |           42 |         relu |      0.49121 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          80              2             1.6266             0.0019366         3     relu    0.36821    95     relu    0.22838    99     relu    0.36587    96     tanh    0.22901    40     relu    0.26595

hs_array:      3    95

ht_array length: 1lstm
     3    95

    "lstm"    "lstm"

    "relu"    "relu"

    0.3682    0.2284

Type: lstm Hidden size: 3 Activation: relu Dropout: 0.368208
Type: lstm Hidden size: 95 Activation: relu Dropout: 0.228380
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 3 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             37% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             23% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 3 ME: 95.000000 MSE: 0.080213
SL: 8.693763e-03 HS: Saving to file: bayesopt/bayesopt_0.080213.mat
|====================================================================================================================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |          hs4 |         act4 |        drop4 |          hs5 |         act5 |        drop5 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |              |              |              |              |              |              |
|====================================================================================================================================================================================================================================================================================================================================================================|
|   41 | Accept |    0.080213 |      11.956 |     0.03872 |    0.038739 |           80 |            2 |       1.6266 |    0.0019366 |            3 |         relu |      0.36821 |           95 |         relu |      0.22838 |           99 |         relu |      0.36587 |           96 |         tanh |      0.22901 |           40 |         relu |      0.26595 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ______

          73              4             2.6645             0.0051916        40     relu    0.047816    76     relu    0.39379    50     relu    0.49582    15     tanh    0.21407    90     relu    0.4446

hs_array:     40    76    50    15

ht_array length: 1lstm
    40    76    50    15

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.0478    0.3938    0.4958    0.2141

Type: lstm Hidden size: 40 Activation: relu Dropout: 0.047816
Type: lstm Hidden size: 76 Activation: relu Dropout: 0.393790
Type: lstm Hidden size: 50 Activation: relu Dropout: 0.495823
Type: lstm Hidden size: 15 Activation: tanh Dropout: 0.214075
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 40 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             5% dropout
     5   ''   LSTM                LSTM with 76 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             39% dropout
     8   ''   LSTM                LSTM with 50 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             50% dropout
    11   ''   LSTM                LSTM with 15 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             21% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 40 ME: 76.000000 MSE: 50.000000
SL: 15 HS: 6.991052e-02 ME: 0.007540 MSE: Saving to file: bayesopt/bayesopt_0.069911.mat
|   42 | Accept |    0.069911 |      22.124 |     0.03872 |     0.03874 |           73 |            4 |       2.6645 |    0.0051916 |           40 |         relu |     0.047816 |           76 |         relu |      0.39379 |           50 |         relu |      0.49582 |           15 |         tanh |      0.21407 |           90 |         relu |       0.4446 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _____

          61              3             4.7351              0.012241        47     relu    0.067261    56     relu    0.37701    28     relu    0.20778     8     tanh    0.37863    95     relu    0.308

hs_array:     47    56    28

ht_array length: 1lstm
    47    56    28

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0673    0.3770    0.2078

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.067261
Type: lstm Hidden size: 56 Activation: relu Dropout: 0.377013
Type: lstm Hidden size: 28 Activation: relu Dropout: 0.207779
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             7% dropout
     5   ''   LSTM                LSTM with 56 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             38% dropout
     8   ''   LSTM                LSTM with 28 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             21% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 56.000000 MSE: 28.000000
SL: 7.364239e-02 HS: 8.506261e-03 ME: Saving to file: bayesopt/bayesopt_0.073642.mat
|   43 | Accept |    0.073642 |      14.119 |     0.03872 |    0.038736 |           61 |            3 |       4.7351 |     0.012241 |           47 |         relu |     0.067261 |           56 |         relu |      0.37701 |           28 |         relu |      0.20778 |            8 |         tanh |      0.37863 |           95 |         relu |        0.308 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          53              2            0.078567            0.0088778         5     relu    0.25739    92     relu    0.18769    75     relu    0.24739    54     tanh    0.15112    29     relu    0.45609

hs_array:      5    92

ht_array length: 1lstm
     5    92

    "lstm"    "lstm"

    "relu"    "relu"

    0.2574    0.1877

Type: lstm Hidden size: 5 Activation: relu Dropout: 0.257386
Type: lstm Hidden size: 92 Activation: relu Dropout: 0.187691
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 5 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 92 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             19% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 5 ME: 92.000000 MSE: 0.054680
SL: 4.547168e-03 HS: Saving to file: bayesopt/bayesopt_0.05468.mat
|   44 | Accept |     0.05468 |      11.543 |     0.03872 |    0.038735 |           53 |            2 |     0.078567 |    0.0088778 |            5 |         relu |      0.25739 |           92 |         relu |      0.18769 |           75 |         relu |      0.24739 |           54 |         tanh |      0.15112 |           29 |         relu |      0.45609 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    ________    ___    ____    _______

          89              2            0.069485             0.011475        48     relu    0.25208    86     relu    0.072071    86     relu    0.39491    62     tanh    0.034904    79     relu    0.43415

hs_array:     48    86

ht_array length: 1lstm
    48    86

    "lstm"    "lstm"

    "relu"    "relu"

    0.2521    0.0721

Type: lstm Hidden size: 48 Activation: relu Dropout: 0.252083
Type: lstm Hidden size: 86 Activation: relu Dropout: 0.072071
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 48 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             7% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 48 ME: 86.000000 MSE: 0.050898
SL: 4.075446e-03 HS: Saving to file: bayesopt/bayesopt_0.050898.mat
|   45 | Accept |    0.050898 |      14.437 |     0.03872 |    0.038735 |           89 |            2 |     0.069485 |     0.011475 |           48 |         relu |      0.25208 |           86 |         relu |     0.072071 |           86 |         relu |      0.39491 |           62 |         tanh |     0.034904 |           79 |         relu |      0.43415 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          71              2             6.0072              0.010724        19     relu    0.23682    88     relu    0.080511    27     relu    0.37368    82     tanh    0.21939    78     relu    0.3425

hs_array:     19    88

ht_array length: 1lstm
    19    88

    "lstm"    "lstm"

    "relu"    "relu"

    0.2368    0.0805

Type: lstm Hidden size: 19 Activation: relu Dropout: 0.236817
Type: lstm Hidden size: 88 Activation: relu Dropout: 0.080511
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 19 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             24% dropout
     5   ''   LSTM                LSTM with 88 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             8% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 19 ME: 88.000000 MSE: 0.050163
SL: 3.691509e-03 HS: Saving to file: bayesopt/bayesopt_0.050163.mat
|   46 | Accept |    0.050163 |      13.144 |     0.03872 |    0.038735 |           71 |            2 |       6.0072 |     0.010724 |           19 |         relu |      0.23682 |           88 |         relu |     0.080511 |           27 |         relu |      0.37368 |           82 |         tanh |      0.21939 |           78 |         relu |       0.3425 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          46              3             1.8124             0.0038016        44     relu    0.17555    88     relu    0.18032    78     relu    0.020474    30     tanh    0.45843    51     relu    0.28075

hs_array:     44    88    78

ht_array length: 1lstm
    44    88    78

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1755    0.1803    0.0205

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.175547
Type: lstm Hidden size: 88 Activation: relu Dropout: 0.180325
Type: lstm Hidden size: 78 Activation: relu Dropout: 0.020474
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             18% dropout
     5   ''   LSTM                LSTM with 88 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             18% dropout
     8   ''   LSTM                LSTM with 78 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             2% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 88.000000 MSE: 78.000000
SL: 4.552924e-02 HS: 3.068684e-03 ME: Saving to file: bayesopt/bayesopt_0.045529.mat
|   47 | Accept |    0.045529 |      22.016 |     0.03872 |    0.038741 |           46 |            3 |       1.8124 |    0.0038016 |           44 |         relu |      0.17555 |           88 |         relu |      0.18032 |           78 |         relu |     0.020474 |           30 |         tanh |      0.45843 |           51 |         relu |      0.28075 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          97              2            0.020451            0.0096817        52     relu    0.009252    66     relu    0.25894    24     relu    0.30172    60     tanh    0.062282    90     relu    0.26441

hs_array:     52    66

ht_array length: 1lstm
    52    66

    "lstm"    "lstm"

    "relu"    "relu"

    0.0093    0.2589

Type: lstm Hidden size: 52 Activation: relu Dropout: 0.009252
Type: lstm Hidden size: 66 Activation: relu Dropout: 0.258940
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 52 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 66 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             26% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 52 ME: 66.000000 MSE: 0.060233
SL: 5.611382e-03 HS: Saving to file: bayesopt/bayesopt_0.060233.mat
|   48 | Accept |    0.060233 |      10.274 |     0.03872 |    0.038741 |           97 |            2 |     0.020451 |    0.0096817 |           52 |         relu |     0.009252 |           66 |         relu |      0.25894 |           24 |         relu |      0.30172 |           60 |         tanh |     0.062282 |           90 |         relu |      0.26441 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2      hs3    act3    drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    ________    ___    ____    ______    ___    ____    ________    ___    ____    _______

          17              2            0.048749             0.011805        36     relu    0.040862    54     relu    0.052506    92     relu    0.1938    89     tanh    0.090076    87     relu    0.04972

hs_array:     36    54

ht_array length: 1lstm
    36    54

    "lstm"    "lstm"

    "relu"    "relu"

    0.0409    0.0525

Type: lstm Hidden size: 36 Activation: relu Dropout: 0.040862
Type: lstm Hidden size: 54 Activation: relu Dropout: 0.052506
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 36 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 54 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             5% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 36 ME: 54.000000 MSE: 0.089082
SL: 1.161509e-02 HS: Saving to file: bayesopt/bayesopt_0.089082.mat
|   49 | Accept |    0.089082 |      17.705 |     0.03872 |     0.03874 |           17 |            2 |     0.048749 |     0.011805 |           36 |         relu |     0.040862 |           54 |         relu |     0.052506 |           92 |         relu |       0.1938 |           89 |         tanh |     0.090076 |           87 |         relu |      0.04972 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          52              3             1.6076             0.0061541        39     relu    0.15013    87     relu    0.31101    90     relu    0.023678    97     tanh    0.29584    27     relu    0.49992

hs_array:     39    87    90

ht_array length: 1lstm
    39    87    90

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1501    0.3110    0.0237

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.150129
Type: lstm Hidden size: 87 Activation: relu Dropout: 0.311013
Type: lstm Hidden size: 90 Activation: relu Dropout: 0.023678
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 87 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             31% dropout
     8   ''   LSTM                LSTM with 90 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             2% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 87.000000 MSE: 90.000000
SL: 4.968710e-02 HS: 3.746880e-03 ME: Saving to file: bayesopt/bayesopt_0.049687.mat
|   50 | Accept |    0.049687 |      22.623 |     0.03872 |    0.038742 |           52 |            3 |       1.6076 |    0.0061541 |           39 |         relu |      0.15013 |           87 |         relu |      0.31101 |           90 |         relu |     0.023678 |           97 |         tanh |      0.29584 |           27 |         relu |      0.49992 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3      hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ________    ___    ____    ______    ___    ____    _______

          53              3             2.7762             0.0086328        33     relu    0.49287    99     relu    0.081453    70     relu    0.091571     4     tanh    0.4495    87     relu    0.15196

hs_array:     33    99    70

ht_array length: 1lstm
    33    99    70

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.4929    0.0815    0.0916

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.492872
Type: lstm Hidden size: 99 Activation: relu Dropout: 0.081453
Type: lstm Hidden size: 70 Activation: relu Dropout: 0.091571
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 99 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             8% dropout
     8   ''   LSTM                LSTM with 70 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             9% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 99.000000 MSE: 70.000000
SL: 5.651433e-02 HS: 5.267292e-03 ME: Saving to file: bayesopt/bayesopt_0.056514.mat
|   51 | Accept |    0.056514 |      19.375 |     0.03872 |    0.038743 |           53 |            3 |       2.7762 |    0.0086328 |           33 |         relu |      0.49287 |           99 |         relu |     0.081453 |           70 |         relu |     0.091571 |            4 |         tanh |       0.4495 |           87 |         relu |      0.15196 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          86              4             1.2465             0.0058896        20     relu    0.42354    83     relu    0.037086    30     relu    0.45671    39     tanh    0.36387    70     relu    0.044012

hs_array:     20    83    30    39

ht_array length: 1lstm
    20    83    30    39

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.4235    0.0371    0.4567    0.3639

Type: lstm Hidden size: 20 Activation: relu Dropout: 0.423538
Type: lstm Hidden size: 83 Activation: relu Dropout: 0.037086
Type: lstm Hidden size: 30 Activation: relu Dropout: 0.456714
Type: lstm Hidden size: 39 Activation: tanh Dropout: 0.363866
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 20 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 83 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             4% dropout
     8   ''   LSTM                LSTM with 30 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             46% dropout
    11   ''   LSTM                LSTM with 39 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             36% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 20 ME: 83.000000 MSE: 30.000000
SL: 39 HS: 7.576135e-02 ME: 0.008066 MSE: Saving to file: bayesopt/bayesopt_0.075761.mat
|   52 | Accept |    0.075761 |       18.81 |     0.03872 |    0.038742 |           86 |            4 |       1.2465 |    0.0058896 |           20 |         relu |      0.42354 |           83 |         relu |     0.037086 |           30 |         relu |      0.45671 |           39 |         tanh |      0.36387 |           70 |         relu |     0.044012 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3    drop3     hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    ______    ___    ____    ______    ___    ____    _______

          65              2             0.77812            0.0055401        12     relu    0.2315    95     relu    0.14624    73     relu    0.1516    28     tanh    0.4789    37     relu    0.33551

hs_array:     12    95

ht_array length: 1lstm
    12    95

    "lstm"    "lstm"

    "relu"    "relu"

    0.2315    0.1462

Type: lstm Hidden size: 12 Activation: relu Dropout: 0.231504
Type: lstm Hidden size: 95 Activation: relu Dropout: 0.146240
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 12 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             23% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             15% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 12 ME: 95.000000 MSE: 0.048238
SL: 3.472578e-03 HS: Saving to file: bayesopt/bayesopt_0.048238.mat
|   53 | Accept |    0.048238 |      11.082 |     0.03872 |    0.038746 |           65 |            2 |      0.77812 |    0.0055401 |           12 |         relu |       0.2315 |           95 |         relu |      0.14624 |           73 |         relu |       0.1516 |           28 |         tanh |       0.4789 |           37 |         relu |      0.33551 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          26              3             6.4261             0.0053046        88     relu    0.10614    38     relu    0.22272    65     relu    0.054459    90     tanh    0.29661    92     relu    0.10233

hs_array:     88    38    65

ht_array length: 1lstm
    88    38    65

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.1061    0.2227    0.0545

Type: lstm Hidden size: 88 Activation: relu Dropout: 0.106136
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.222717
Type: lstm Hidden size: 65 Activation: relu Dropout: 0.054459
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 88 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             11% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             22% dropout
     8   ''   LSTM                LSTM with 65 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             5% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 88 ME: 38.000000 MSE: 65.000000
SL: 5.597104e-02 HS: 4.432118e-03 ME: Saving to file: bayesopt/bayesopt_0.055971.mat
|   54 | Accept |    0.055971 |      23.513 |     0.03872 |    0.038747 |           26 |            3 |       6.4261 |    0.0053046 |           88 |         relu |      0.10614 |           38 |         relu |      0.22272 |           65 |         relu |     0.054459 |           90 |         tanh |      0.29661 |           92 |         relu |      0.10233 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          64              2             4.5088             0.0036667        30     relu    0.36059    80     relu    0.3364    29     relu    0.18113    10     tanh    0.10705    69     relu    0.46845

hs_array:     30    80

ht_array length: 1lstm
    30    80

    "lstm"    "lstm"

    "relu"    "relu"

    0.3606    0.3364

Type: lstm Hidden size: 30 Activation: relu Dropout: 0.360593
Type: lstm Hidden size: 80 Activation: relu Dropout: 0.336402
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 30 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             36% dropout
     5   ''   LSTM                LSTM with 80 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             34% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 30 ME: 80.000000 MSE: 0.051880
SL: 3.994438e-03 HS: Saving to file: bayesopt/bayesopt_0.05188.mat
|   55 | Accept |     0.05188 |      10.744 |     0.03872 |    0.038744 |           64 |            2 |       4.5088 |    0.0036667 |           30 |         relu |      0.36059 |           80 |         relu |       0.3364 |           29 |         relu |      0.18113 |           10 |         tanh |      0.10705 |           69 |         relu |      0.46845 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          84              4             0.1483             0.0084675        53     relu    0.34977    84     relu    0.075166    81     relu    0.47987    14     tanh    0.31508    29     relu    0.4578

hs_array:     53    84    81    14

ht_array length: 1lstm
    53    84    81    14

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.3498    0.0752    0.4799    0.3151

Type: lstm Hidden size: 53 Activation: relu Dropout: 0.349771
Type: lstm Hidden size: 84 Activation: relu Dropout: 0.075166
Type: lstm Hidden size: 81 Activation: relu Dropout: 0.479870
Type: lstm Hidden size: 14 Activation: tanh Dropout: 0.315077
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 53 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             35% dropout
     5   ''   LSTM                LSTM with 84 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             8% dropout
     8   ''   LSTM                LSTM with 81 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             48% dropout
    11   ''   LSTM                LSTM with 14 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             32% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 53 ME: 84.000000 MSE: 81.000000
SL: 14 HS: 1.860698e-01 ME: 0.046937 MSE: Saving to file: bayesopt/bayesopt_0.18607.mat
|   56 | Accept |     0.18607 |      23.912 |     0.03872 |    0.039735 |           84 |            4 |       0.1483 |    0.0084675 |           53 |         relu |      0.34977 |           84 |         relu |     0.075166 |           81 |         relu |      0.47987 |           14 |         tanh |      0.31508 |           29 |         relu |       0.4578 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________

          80              3             1.6172             0.0090988        79     relu    0.3895    90     relu    0.11255    35     relu    0.42584    68     tanh    0.20878    62     relu    0.039317

hs_array:     79    90    35

ht_array length: 1lstm
    79    90    35

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.3895    0.1125    0.4258

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.389496
Type: lstm Hidden size: 90 Activation: relu Dropout: 0.112550
Type: lstm Hidden size: 35 Activation: relu Dropout: 0.425845
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 90 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             11% dropout
     8   ''   LSTM                LSTM with 35 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             43% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 90.000000 MSE: 35.000000
SL: 6.703141e-02 HS: 6.915226e-03 ME: Saving to file: bayesopt/bayesopt_0.067031.mat
|   57 | Accept |    0.067031 |      19.392 |     0.03872 |    0.038738 |           80 |            3 |       1.6172 |    0.0090988 |           79 |         relu |       0.3895 |           90 |         relu |      0.11255 |           35 |         relu |      0.42584 |           68 |         tanh |      0.20878 |           62 |         relu |     0.039317 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          98              2             6.5845             0.0054187        76     relu    0.32156    38     relu    0.39146    74     relu    0.12632    27     tanh    0.26858    17     relu    0.05297

hs_array:     76    38

ht_array length: 1lstm
    76    38

    "lstm"    "lstm"

    "relu"    "relu"

    0.3216    0.3915

Type: lstm Hidden size: 76 Activation: relu Dropout: 0.321559
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.391464
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 76 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             39% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 76 ME: 38.000000 MSE: 0.059325
SL: 5.347905e-03 HS: Saving to file: bayesopt/bayesopt_0.059325.mat
|   58 | Accept |    0.059325 |      10.618 |     0.03872 |    0.038741 |           98 |            2 |       6.5845 |    0.0054187 |           76 |         relu |      0.32156 |           38 |         relu |      0.39146 |           74 |         relu |      0.12632 |           27 |         tanh |      0.26858 |           17 |         relu |      0.05297 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          94              2             0.01913             0.010172        72     relu    0.29516    98     relu    0.086784     7     relu    0.22802    16     tanh    0.15397    23     relu    0.20966

hs_array:     72    98

ht_array length: 1lstm
    72    98

    "lstm"    "lstm"

    "relu"    "relu"

    0.2952    0.0868

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.295159
Type: lstm Hidden size: 98 Activation: relu Dropout: 0.086784
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             9% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 98.000000 MSE: 0.056764
SL: 5.485293e-03 HS: Saving to file: bayesopt/bayesopt_0.056764.mat
|   59 | Accept |    0.056764 |      14.369 |     0.03872 |    0.038743 |           94 |            2 |      0.01913 |     0.010172 |           72 |         relu |      0.29516 |           98 |         relu |     0.086784 |            7 |         relu |      0.22802 |           16 |         tanh |      0.15397 |           23 |         relu |      0.20966 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          40              2            0.012209            0.0058135        26     relu    0.12787    42     relu    0.29067    38     relu    0.47716    60     tanh    0.15946    10     relu    0.21089

hs_array:     26    42

ht_array length: 1lstm
    26    42

    "lstm"    "lstm"

    "relu"    "relu"

    0.1279    0.2907

Type: lstm Hidden size: 26 Activation: relu Dropout: 0.127875
Type: lstm Hidden size: 42 Activation: relu Dropout: 0.290674
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 26 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             13% dropout
     5   ''   LSTM                LSTM with 42 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             29% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 26 ME: 42.000000 MSE: 0.074982
SL: 8.409307e-03 HS: Saving to file: bayesopt/bayesopt_0.074982.mat
|   60 | Accept |    0.074982 |       10.56 |     0.03872 |    0.038746 |           40 |            2 |     0.012209 |    0.0058135 |           26 |         relu |      0.12787 |           42 |         relu |      0.29067 |           38 |         relu |      0.47716 |           60 |         tanh |      0.15946 |           10 |         relu |      0.21089 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          19              2            0.026345            0.0074402        51     relu    0.010652    65     relu    0.40763    80     relu    0.081572    48     tanh    0.48735    33     relu    0.19559

hs_array:     51    65

ht_array length: 1lstm
    51    65

    "lstm"    "lstm"

    "relu"    "relu"

    0.0107    0.4076

Type: lstm Hidden size: 51 Activation: relu Dropout: 0.010652
Type: lstm Hidden size: 65 Activation: relu Dropout: 0.407628
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 51 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 65 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             41% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 51 ME: 65.000000 MSE: 0.058754
SL: 5.104286e-03 HS: Saving to file: bayesopt/bayesopt_0.058754.mat
|====================================================================================================================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |          hs4 |         act4 |        drop4 |          hs5 |         act5 |        drop5 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |              |              |              |              |              |              |
|====================================================================================================================================================================================================================================================================================================================================================================|
|   61 | Accept |    0.058754 |      17.696 |     0.03872 |    0.038734 |           19 |            2 |     0.026345 |    0.0074402 |           51 |         relu |     0.010652 |           65 |         relu |      0.40763 |           80 |         relu |     0.081572 |           48 |         tanh |      0.48735 |           33 |         relu |      0.19559 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1    hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _____    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______

          24              2             2.0603              0.017956        17     relu    0.451    19     relu    0.21197    72     relu    0.2038    72     tanh    0.36078    18     relu    0.47745

hs_array:     17    19

ht_array length: 1lstm
    17    19

    "lstm"    "lstm"

    "relu"    "relu"

    0.4510    0.2120

Type: lstm Hidden size: 17 Activation: relu Dropout: 0.451001
Type: lstm Hidden size: 19 Activation: relu Dropout: 0.211975
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 17 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             45% dropout
     5   ''   LSTM                LSTM with 19 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             21% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 17 ME: 19.000000 MSE: 0.066977
SL: 6.195809e-03 HS: Saving to file: bayesopt/bayesopt_0.066977.mat
|   62 | Accept |    0.066977 |      11.371 |     0.03872 |    0.038733 |           24 |            2 |       2.0603 |     0.017956 |           17 |         relu |        0.451 |           19 |         relu |      0.21197 |           72 |         relu |       0.2038 |           72 |         tanh |      0.36078 |           18 |         relu |      0.47745 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    ________    ___    ____    _______

          86              2             5.1002              0.012123        58     relu    0.19446    81     relu    0.36988    48     relu    0.037607    16     tanh    0.013743    34     relu    0.46077

hs_array:     58    81

ht_array length: 1lstm
    58    81

    "lstm"    "lstm"

    "relu"    "relu"

    0.1945    0.3699

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.194461
Type: lstm Hidden size: 81 Activation: relu Dropout: 0.369877
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 81 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             37% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 81.000000 MSE: 0.042338
SL: 2.829012e-03 HS: Saving to file: bayesopt/bayesopt_0.042338.mat
|   63 | Accept |    0.042338 |      13.161 |     0.03872 |    0.038733 |           86 |            2 |       5.1002 |     0.012123 |           58 |         relu |      0.19446 |           81 |         relu |      0.36988 |           48 |         relu |     0.037607 |           16 |         tanh |     0.013743 |           34 |         relu |      0.46077 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ______

          78              2             2.9449             0.0011803        58     relu    0.10447    41     relu    0.012351    46     relu    0.11286    45     tanh    0.23843    49     relu    0.3462

hs_array:     58    41

ht_array length: 1lstm
    58    41

    "lstm"    "lstm"

    "relu"    "relu"

    0.1045    0.0124

Type: lstm Hidden size: 58 Activation: relu Dropout: 0.104470
Type: lstm Hidden size: 41 Activation: relu Dropout: 0.012351
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 58 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             10% dropout
     5   ''   LSTM                LSTM with 41 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             1% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 58 ME: 41.000000 MSE: 0.051021
SL: 4.193032e-03 HS: Saving to file: bayesopt/bayesopt_0.051021.mat
|   64 | Accept |    0.051021 |      9.7624 |     0.03872 |    0.038736 |           78 |            2 |       2.9449 |    0.0011803 |           58 |         relu |      0.10447 |           41 |         relu |     0.012351 |           46 |         relu |      0.11286 |           45 |         tanh |      0.23843 |           49 |         relu |       0.3462 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    ________

          97              2            0.063863             0.001249        77     relu    0.43673    29     relu    0.33366    27     relu    0.055999    80     tanh    0.47812    53     relu    0.073224

hs_array:     77    29

ht_array length: 1lstm
    77    29

    "lstm"    "lstm"

    "relu"    "relu"

    0.4367    0.3337

Type: lstm Hidden size: 77 Activation: relu Dropout: 0.436732
Type: lstm Hidden size: 29 Activation: relu Dropout: 0.333662
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 77 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 29 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             33% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 77 ME: 29.000000 MSE: 0.074036
SL: 8.027136e-03 HS: Saving to file: bayesopt/bayesopt_0.074036.mat
|   65 | Accept |    0.074036 |      10.171 |     0.03872 |    0.038733 |           97 |            2 |     0.063863 |     0.001249 |           77 |         relu |      0.43673 |           29 |         relu |      0.33366 |           27 |         relu |     0.055999 |           80 |         tanh |      0.47812 |           53 |         relu |     0.073224 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          74              2             0.42772            0.0096077        21     relu    0.47655    55     relu    0.28495    40     relu    0.36963    11     tanh    0.029428     4     relu    0.36581

hs_array:     21    55

ht_array length: 1lstm
    21    55

    "lstm"    "lstm"

    "relu"    "relu"

    0.4765    0.2850

Type: lstm Hidden size: 21 Activation: relu Dropout: 0.476548
Type: lstm Hidden size: 55 Activation: relu Dropout: 0.284952
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 21 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             48% dropout
     5   ''   LSTM                LSTM with 55 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             28% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 21 ME: 55.000000 MSE: 0.053858
SL: 4.458204e-03 HS: Saving to file: bayesopt/bayesopt_0.053858.mat
|   66 | Accept |    0.053858 |      8.5282 |     0.03872 |    0.038734 |           74 |            2 |      0.42772 |    0.0096077 |           21 |         relu |      0.47655 |           55 |         relu |      0.28495 |           40 |         relu |      0.36963 |           11 |         tanh |     0.029428 |            4 |         relu |      0.36581 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______    ___    ____    ________    ___    ____    _______

          79              3             1.7733             0.0077689        41     relu    0.38233    92     relu    0.11298    46     relu    0.2838    64     tanh    0.037229    83     relu    0.29534

hs_array:     41    92    46

ht_array length: 1lstm
    41    92    46

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.3823    0.1130    0.2838

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.382327
Type: lstm Hidden size: 92 Activation: relu Dropout: 0.112976
Type: lstm Hidden size: 46 Activation: relu Dropout: 0.283801
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 92 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             11% dropout
     8   ''   LSTM                LSTM with 46 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             28% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 92.000000 MSE: 46.000000
SL: 6.317503e-02 HS: 5.696003e-03 ME: Saving to file: bayesopt/bayesopt_0.063175.mat
|   67 | Accept |    0.063175 |      17.554 |     0.03872 |    0.038736 |           79 |            3 |       1.7733 |    0.0077689 |           41 |         relu |      0.38233 |           92 |         relu |      0.11298 |           46 |         relu |       0.2838 |           64 |         tanh |     0.037229 |           83 |         relu |      0.29534 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______

         100              2             3.5441             0.0018046        93     relu    0.011073    10     relu    0.30837    83     relu    0.2081    94     tanh    0.26344    72     relu    0.32725

hs_array:     93    10

ht_array length: 1lstm
    93    10

    "lstm"    "lstm"

    "relu"    "relu"

    0.0111    0.3084

Type: lstm Hidden size: 93 Activation: relu Dropout: 0.011073
Type: lstm Hidden size: 10 Activation: relu Dropout: 0.308374
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 93 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             1% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             31% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 93 ME: 10.000000 MSE: 0.074342
SL: 8.448030e-03 HS: Saving to file: bayesopt/bayesopt_0.074342.mat
|   68 | Accept |    0.074342 |      10.409 |     0.03872 |    0.038736 |          100 |            2 |       3.5441 |    0.0018046 |           93 |         relu |     0.011073 |           10 |         relu |      0.30837 |           83 |         relu |       0.2081 |           94 |         tanh |      0.26344 |           72 |         relu |      0.32725 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    ______    ___    ____    _______    ___    ____    _______

          83              2             0.41083             0.024594        59     relu    0.15429    68     relu    0.064813    27     relu    0.4561    80     tanh    0.10042    61     relu    0.35589

hs_array:     59    68

ht_array length: 1lstm
    59    68

    "lstm"    "lstm"

    "relu"    "relu"

    0.1543    0.0648

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.154291
Type: lstm Hidden size: 68 Activation: relu Dropout: 0.064813
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             15% dropout
     5   ''   LSTM                LSTM with 68 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             6% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 68.000000 MSE: 0.052308
SL: 3.986695e-03 HS: Saving to file: bayesopt/bayesopt_0.052308.mat
|   69 | Accept |    0.052308 |      11.863 |     0.03872 |    0.038736 |           83 |            2 |      0.41083 |     0.024594 |           59 |         relu |      0.15429 |           68 |         relu |     0.064813 |           27 |         relu |       0.4561 |           80 |         tanh |      0.10042 |           61 |         relu |      0.35589 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          89              2             3.1831              0.002779         3     relu    0.41537    27     relu    0.27792    100    relu    0.29901    54     tanh    0.24528    30     relu    0.43759

hs_array:      3    27

ht_array length: 1lstm
     3    27

    "lstm"    "lstm"

    "relu"    "relu"

    0.4154    0.2779

Type: lstm Hidden size: 3 Activation: relu Dropout: 0.415369
Type: lstm Hidden size: 27 Activation: relu Dropout: 0.277924
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 3 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 27 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             28% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 3 ME: 27.000000 MSE: 0.182320
SL: 4.515367e-02 HS: Saving to file: bayesopt/bayesopt_0.18232.mat
|   70 | Accept |     0.18232 |        7.07 |     0.03872 |    0.038729 |           89 |            2 |       3.1831 |     0.002779 |            3 |         relu |      0.41537 |           27 |         relu |      0.27792 |          100 |         relu |      0.29901 |           54 |         tanh |      0.24528 |           30 |         relu |      0.43759 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          71              2            0.029157             0.033291        54     relu    0.32419    86     relu    0.4408    19     relu    0.32978    97     tanh    0.034827    23     relu    0.39253

hs_array:     54    86

ht_array length: 1lstm
    54    86

    "lstm"    "lstm"

    "relu"    "relu"

    0.3242    0.4408

Type: lstm Hidden size: 54 Activation: relu Dropout: 0.324192
Type: lstm Hidden size: 86 Activation: relu Dropout: 0.440804
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 54 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             44% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 54 ME: 86.000000 MSE: 0.056969
SL: 5.012448e-03 HS: Saving to file: bayesopt/bayesopt_0.056969.mat
|   71 | Accept |    0.056969 |      14.141 |     0.03872 |    0.038729 |           71 |            2 |     0.029157 |     0.033291 |           54 |         relu |      0.32419 |           86 |         relu |       0.4408 |           19 |         relu |      0.32978 |           97 |         tanh |     0.034827 |           23 |         relu |      0.39253 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3      hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    ________    ___    ____    ______    ___    ____    _______

          74              2             3.9938              0.025678         7     relu    0.34139    89     relu    0.4426    36     relu    0.059532    16     tanh    0.1136    72     relu    0.20921

hs_array:      7    89

ht_array length: 1lstm
     7    89

    "lstm"    "lstm"

    "relu"    "relu"

    0.3414    0.4426

Type: lstm Hidden size: 7 Activation: relu Dropout: 0.341387
Type: lstm Hidden size: 89 Activation: relu Dropout: 0.442597
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 7 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 89 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             44% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 7 ME: 89.000000 MSE: 0.082373
SL: 9.920224e-03 HS: Saving to file: bayesopt/bayesopt_0.082373.mat
|   72 | Accept |    0.082373 |      9.7171 |     0.03872 |    0.038736 |           74 |            2 |       3.9938 |     0.025678 |            7 |         relu |      0.34139 |           89 |         relu |       0.4426 |           36 |         relu |     0.059532 |           16 |         tanh |       0.1136 |           72 |         relu |      0.20921 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          91              2             3.2946             0.0023583        68     relu    0.24699    67     relu    0.40979    13     relu    0.48531    50     tanh    0.30094    30     relu    0.47117

hs_array:     68    67

ht_array length: 1lstm
    68    67

    "lstm"    "lstm"

    "relu"    "relu"

    0.2470    0.4098

Type: lstm Hidden size: 68 Activation: relu Dropout: 0.246990
Type: lstm Hidden size: 67 Activation: relu Dropout: 0.409788
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 68 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             25% dropout
     5   ''   LSTM                LSTM with 67 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             41% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 68 ME: 67.000000 MSE: 0.061912
SL: 5.425807e-03 HS: Saving to file: bayesopt/bayesopt_0.061912.mat
|   73 | Accept |    0.061912 |      13.397 |     0.03872 |     0.03873 |           91 |            2 |       3.2946 |    0.0023583 |           68 |         relu |      0.24699 |           67 |         relu |      0.40979 |           13 |         relu |      0.48531 |           50 |         tanh |      0.30094 |           30 |         relu |      0.47117 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3      drop3       hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    __________    ___    ____    _______    ___    ____    _______

          91              2             0.60231             0.035758        33     relu    0.35246    38     relu    0.06907    44     relu    0.00039658     8     tanh    0.20874    19     relu    0.44392

hs_array:     33    38

ht_array length: 1lstm
    33    38

    "lstm"    "lstm"

    "relu"    "relu"

    0.3525    0.0691

Type: lstm Hidden size: 33 Activation: relu Dropout: 0.352462
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.069070
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 33 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             35% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             7% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 33 ME: 38.000000 MSE: 0.052378
SL: 4.162671e-03 HS: Saving to file: bayesopt/bayesopt_0.052378.mat
|   74 | Accept |    0.052378 |      8.9443 |     0.03872 |    0.038729 |           91 |            2 |      0.60231 |     0.035758 |           33 |         relu |      0.35246 |           38 |         relu |      0.06907 |           44 |         relu |   0.00039658 |            8 |         tanh |      0.20874 |           19 |         relu |      0.44392 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          41              2            0.015156            0.00030239       50     relu    0.34452    60     relu    0.3686    92     relu    0.28184    49     tanh    0.18012    42     relu    0.35455

hs_array:     50    60

ht_array length: 1lstm
    50    60

    "lstm"    "lstm"

    "relu"    "relu"

    0.3445    0.3686

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.344516
Type: lstm Hidden size: 60 Activation: relu Dropout: 0.368598
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             34% dropout
     5   ''   LSTM                LSTM with 60 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             37% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 60.000000 MSE: 0.077767
SL: 1.079193e-02 HS: Saving to file: bayesopt/bayesopt_0.077767.mat
|   75 | Accept |    0.077767 |      11.706 |     0.03872 |     0.03873 |           41 |            2 |     0.015156 |   0.00030239 |           50 |         relu |      0.34452 |           60 |         relu |       0.3686 |           92 |         relu |      0.28184 |           49 |         tanh |      0.18012 |           42 |         relu |      0.35455 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          89              2             0.50939            0.0012029        61     relu    0.025101    80     relu    0.43069    53     relu    0.33252    12     tanh    0.048138    59     relu    0.22348

hs_array:     61    80

ht_array length: 1lstm
    61    80

    "lstm"    "lstm"

    "relu"    "relu"

    0.0251    0.4307

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.025101
Type: lstm Hidden size: 80 Activation: relu Dropout: 0.430686
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             3% dropout
     5   ''   LSTM                LSTM with 80 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             43% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 80.000000 MSE: 0.063002
SL: 5.702029e-03 HS: Saving to file: bayesopt/bayesopt_0.063002.mat
|   76 | Accept |    0.063002 |      13.466 |     0.03872 |    0.038729 |           89 |            2 |      0.50939 |    0.0012029 |           61 |         relu |     0.025101 |           80 |         relu |      0.43069 |           53 |         relu |      0.33252 |           12 |         tanh |     0.048138 |           59 |         relu |      0.22348 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________

          62              3            0.050702            0.0093402        67     relu    0.021041    82     relu    0.21096    34     relu    0.21987    52     tanh    0.18279    34     relu    0.029206

hs_array:     67    82    34

ht_array length: 1lstm
    67    82    34

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.0210    0.2110    0.2199

Type: lstm Hidden size: 67 Activation: relu Dropout: 0.021041
Type: lstm Hidden size: 82 Activation: relu Dropout: 0.210959
Type: lstm Hidden size: 34 Activation: relu Dropout: 0.219866
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 67 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 82 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             21% dropout
     8   ''   LSTM                LSTM with 34 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             22% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 67 ME: 82.000000 MSE: 34.000000
SL: 6.040446e-02 HS: 5.425947e-03 ME: Saving to file: bayesopt/bayesopt_0.060404.mat
|   77 | Accept |    0.060404 |      19.482 |     0.03872 |     0.03873 |           62 |            3 |     0.050702 |    0.0093402 |           67 |         relu |     0.021041 |           82 |         relu |      0.21096 |           34 |         relu |      0.21987 |           52 |         tanh |      0.18279 |           34 |         relu |     0.029206 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          60              2             1.3393              0.025925        13     relu    0.26397    69     relu    0.1607    62     relu    0.14668    23     tanh    0.38213    23     relu    0.24373

hs_array:     13    69

ht_array length: 1lstm
    13    69

    "lstm"    "lstm"

    "relu"    "relu"

    0.2640    0.1607

Type: lstm Hidden size: 13 Activation: relu Dropout: 0.263973
Type: lstm Hidden size: 69 Activation: relu Dropout: 0.160698
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 13 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             26% dropout
     5   ''   LSTM                LSTM with 69 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             16% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 13 ME: 69.000000 MSE: 0.061816
SL: 5.673354e-03 HS: Saving to file: bayesopt/bayesopt_0.061816.mat
|   78 | Accept |    0.061816 |      10.427 |     0.03872 |     0.03873 |           60 |            2 |       1.3393 |     0.025925 |           13 |         relu |      0.26397 |           69 |         relu |       0.1607 |           62 |         relu |      0.14668 |           23 |         tanh |      0.38213 |           23 |         relu |      0.24373 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4      hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______

          62              2             0.12434             0.021011        37     relu    0.040201    50     relu    0.16834    22     relu    0.41301    74     tanh    0.028976    33     relu    0.42982

hs_array:     37    50

ht_array length: 1lstm
    37    50

    "lstm"    "lstm"

    "relu"    "relu"

    0.0402    0.1683

Type: lstm Hidden size: 37 Activation: relu Dropout: 0.040201
Type: lstm Hidden size: 50 Activation: relu Dropout: 0.168342
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 37 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             4% dropout
     5   ''   LSTM                LSTM with 50 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             17% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 37 ME: 50.000000 MSE: 0.056471
SL: 4.595920e-03 HS: Saving to file: bayesopt/bayesopt_0.056471.mat
|   79 | Accept |    0.056471 |      11.039 |     0.03872 |     0.03873 |           62 |            2 |      0.12434 |     0.021011 |           37 |         relu |     0.040201 |           50 |         relu |      0.16834 |           22 |         relu |      0.41301 |           74 |         tanh |     0.028976 |           33 |         relu |      0.42982 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1      hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ________

          89              2             0.17177             0.010594        61     relu    0.024636    16     relu    0.19404    94     relu    0.05354    35     tanh    0.48997     3     relu    0.080234

hs_array:     61    16

ht_array length: 1lstm
    61    16

    "lstm"    "lstm"

    "relu"    "relu"

    0.0246    0.1940

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.024636
Type: lstm Hidden size: 16 Activation: relu Dropout: 0.194041
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             2% dropout
     5   ''   LSTM                LSTM with 16 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             19% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 16.000000 MSE: 0.061582
SL: 5.414129e-03 HS: Saving to file: bayesopt/bayesopt_0.061582.mat
|   80 | Accept |    0.061582 |      9.9514 |     0.03872 |     0.03873 |           89 |            2 |      0.17177 |     0.010594 |           61 |         relu |     0.024636 |           16 |         relu |      0.19404 |           94 |         relu |      0.05354 |           35 |         tanh |      0.48997 |            3 |         relu |     0.080234 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          73              2             0.27951             0.010806        44     relu    0.19661    86     relu    0.38579    22     relu    0.13581    65     tanh    0.10533    22     relu    0.35801

hs_array:     44    86

ht_array length: 1lstm
    44    86

    "lstm"    "lstm"

    "relu"    "relu"

    0.1966    0.3858

Type: lstm Hidden size: 44 Activation: relu Dropout: 0.196614
Type: lstm Hidden size: 86 Activation: relu Dropout: 0.385785
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 44 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             20% dropout
     5   ''   LSTM                LSTM with 86 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             39% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 44 ME: 86.000000 MSE: 0.051462
SL: 4.073633e-03 HS: Saving to file: bayesopt/bayesopt_0.051462.mat
|====================================================================================================================================================================================================================================================================================================================================================================|
| Iter | Eval   | Objective   | Objective   | BestSoFar   | BestSoFar   | sequenceLeng-|    numLayers | gradientThre-| initialLearn-|          hs1 |         act1 |        drop1 |          hs2 |         act2 |        drop2 |          hs3 |         act3 |        drop3 |          hs4 |         act4 |        drop4 |          hs5 |         act5 |        drop5 |
|      | result |             | runtime     | (observed)  | (estim.)    | th           |              | shold        | Rate         |              |              |              |              |              |              |              |              |              |              |              |              |              |              |              |
|====================================================================================================================================================================================================================================================================================================================================================================|
|   81 | Accept |    0.051462 |      13.264 |     0.03872 |    0.038735 |           73 |            2 |      0.27951 |     0.010806 |           44 |         relu |      0.19661 |           86 |         relu |      0.38579 |           22 |         relu |      0.13581 |           65 |         tanh |      0.10533 |           22 |         relu |      0.35801 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ______

          26              2             0.81858             0.01365         70     relu    0.41841    71     relu    0.27196    39     relu    0.28027    81     tanh    0.33041    79     relu    0.2146

hs_array:     70    71

ht_array length: 1lstm
    70    71

    "lstm"    "lstm"

    "relu"    "relu"

    0.4184    0.2720

Type: lstm Hidden size: 70 Activation: relu Dropout: 0.418412
Type: lstm Hidden size: 71 Activation: relu Dropout: 0.271963
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 70 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             42% dropout
     5   ''   LSTM                LSTM with 71 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             27% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 70 ME: 71.000000 MSE: 0.051671
SL: 3.911553e-03 HS: Saving to file: bayesopt/bayesopt_0.051671.mat
|   82 | Accept |    0.051671 |      17.361 |     0.03872 |    0.038735 |           26 |            2 |      0.81858 |      0.01365 |           70 |         relu |      0.41841 |           71 |         relu |      0.27196 |           39 |         relu |      0.28027 |           81 |         tanh |      0.33041 |           79 |         relu |       0.2146 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______

          84              2            0.010636             0.009264        83     relu    0.13575    61     relu    0.41323    53     relu    0.063343    35     tanh    0.14801    67     relu    0.29599

hs_array:     83    61

ht_array length: 1lstm
    83    61

    "lstm"    "lstm"

    "relu"    "relu"

    0.1357    0.4132

Type: lstm Hidden size: 83 Activation: relu Dropout: 0.135746
Type: lstm Hidden size: 61 Activation: relu Dropout: 0.413226
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 83 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 61 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             41% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 83 ME: 61.000000 MSE: 0.049624
SL: 3.838887e-03 HS: Saving to file: bayesopt/bayesopt_0.049624.mat
|   83 | Accept |    0.049624 |      13.454 |     0.03872 |    0.038735 |           84 |            2 |     0.010636 |     0.009264 |           83 |         relu |      0.13575 |           61 |         relu |      0.41323 |           53 |         relu |     0.063343 |           35 |         tanh |      0.14801 |           67 |         relu |      0.29599 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3    hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _____    ___    ____    _______    ___    ____    _______

          90              2             8.7155              0.02219         78     relu    0.49403    27     relu    0.41846    80     relu    0.458    83     tanh    0.37721    24     relu    0.17233

hs_array:     78    27

ht_array length: 1lstm
    78    27

    "lstm"    "lstm"

    "relu"    "relu"

    0.4940    0.4185

Type: lstm Hidden size: 78 Activation: relu Dropout: 0.494033
Type: lstm Hidden size: 27 Activation: relu Dropout: 0.418460
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 78 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 27 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             42% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 78 ME: 27.000000 MSE: 0.070549
SL: 7.136338e-03 HS: Saving to file: bayesopt/bayesopt_0.070549.mat
|   84 | Accept |    0.070549 |       11.49 |     0.03872 |    0.038735 |           90 |            2 |       8.7155 |      0.02219 |           78 |         relu |      0.49403 |           27 |         relu |      0.41846 |           80 |         relu |        0.458 |           83 |         tanh |      0.37721 |           24 |         relu |      0.17233 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5    drop5
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _____

          59              2             0.21516            0.0060066        67     relu    0.13573    70     relu    0.40787    76     relu    0.2111    84     tanh    0.13933    73     relu    0.319

hs_array:     67    70

ht_array length: 1lstm
    67    70

    "lstm"    "lstm"

    "relu"    "relu"

    0.1357    0.4079

Type: lstm Hidden size: 67 Activation: relu Dropout: 0.135725
Type: lstm Hidden size: 70 Activation: relu Dropout: 0.407866
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 67 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             14% dropout
     5   ''   LSTM                LSTM with 70 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             41% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 67 ME: 70.000000 MSE: 0.061489
SL: 5.463644e-03 HS: Saving to file: bayesopt/bayesopt_0.061489.mat
|   85 | Accept |    0.061489 |      14.157 |     0.03872 |    0.038735 |           59 |            2 |      0.21516 |    0.0060066 |           67 |         relu |      0.13573 |           70 |         relu |      0.40787 |           76 |         relu |       0.2111 |           84 |         tanh |      0.13933 |           73 |         relu |        0.319 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          97              2             1.9276              0.040173        41     relu    0.37613     6     relu    0.23604     5     relu    0.42365    92     tanh    0.41187     2     relu    0.23942

hs_array:     41     6

ht_array length: 1lstm
    41     6

    "lstm"    "lstm"

    "relu"    "relu"

    0.3761    0.2360

Type: lstm Hidden size: 41 Activation: relu Dropout: 0.376126
Type: lstm Hidden size: 6 Activation: relu Dropout: 0.236041
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 41 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             38% dropout
     5   ''   LSTM                LSTM with 6 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             24% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 41 ME: 6.000000 MSE: 0.056869
SL: 4.823533e-03 HS: Saving to file: bayesopt/bayesopt_0.056869.mat
|   86 | Accept |    0.056869 |      7.1725 |     0.03872 |    0.038736 |           97 |            2 |       1.9276 |     0.040173 |           41 |         relu |      0.37613 |            6 |         relu |      0.23604 |            5 |         relu |      0.42365 |           92 |         tanh |      0.41187 |            2 |         relu |      0.23942 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ______    ___    ____    _______

          94              2             0.53776            0.0014571        39     relu    0.32473    21     relu    0.18035    33     relu    0.14581    17     tanh    0.3252    72     relu    0.12187

hs_array:     39    21

ht_array length: 1lstm
    39    21

    "lstm"    "lstm"

    "relu"    "relu"

    0.3247    0.1803

Type: lstm Hidden size: 39 Activation: relu Dropout: 0.324733
Type: lstm Hidden size: 21 Activation: relu Dropout: 0.180347
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 39 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             32% dropout
     5   ''   LSTM                LSTM with 21 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             18% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 39 ME: 21.000000 MSE: 0.071024
SL: 7.901349e-03 HS: Saving to file: bayesopt/bayesopt_0.071024.mat
|   87 | Accept |    0.071024 |      7.0145 |     0.03872 |    0.038736 |           94 |            2 |      0.53776 |    0.0014571 |           39 |         relu |      0.32473 |           21 |         relu |      0.18035 |           33 |         relu |      0.14581 |           17 |         tanh |       0.3252 |           72 |         relu |      0.12187 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5    drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ______

          89              2            0.016072             0.17844         16     relu    0.30443    17     relu    0.46647     3     relu    0.23296    50     tanh    0.26939    20     relu    0.1415

hs_array:     16    17

ht_array length: 1lstm
    16    17

    "lstm"    "lstm"

    "relu"    "relu"

    0.3044    0.4665

Type: lstm Hidden size: 16 Activation: relu Dropout: 0.304433
Type: lstm Hidden size: 17 Activation: relu Dropout: 0.466473
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 16 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             30% dropout
     5   ''   LSTM                LSTM with 17 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             47% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 16 ME: 17.000000 MSE: 0.123502
SL: 2.123018e-02 HS: Saving to file: bayesopt/bayesopt_0.1235.mat
|   88 | Accept |      0.1235 |      7.4396 |     0.03872 |    0.038736 |           89 |            2 |     0.016072 |      0.17844 |           16 |         relu |      0.30443 |           17 |         relu |      0.46647 |            3 |         relu |      0.23296 |           50 |         tanh |      0.26939 |           20 |         relu |       0.1415 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          74              2             0.18012            0.00047033       61     relu    0.27162    17     relu    0.11717    75     relu    0.48273    36     tanh    0.26262    86     relu    0.26423

hs_array:     61    17

ht_array length: 1lstm
    61    17

    "lstm"    "lstm"

    "relu"    "relu"

    0.2716    0.1172

Type: lstm Hidden size: 61 Activation: relu Dropout: 0.271625
Type: lstm Hidden size: 17 Activation: relu Dropout: 0.117171
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 61 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             27% dropout
     5   ''   LSTM                LSTM with 17 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             12% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 61 ME: 17.000000 MSE: 0.104047
SL: 1.672310e-02 HS: Saving to file: bayesopt/bayesopt_0.10405.mat
|   89 | Accept |     0.10405 |      8.5885 |     0.03872 |    0.038736 |           74 |            2 |      0.18012 |   0.00047033 |           61 |         relu |      0.27162 |           17 |         relu |      0.11717 |           75 |         relu |      0.48273 |           36 |         tanh |      0.26262 |           86 |         relu |      0.26423 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1     hs2    act2     drop2     hs3    act3      drop3      hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    ______    ___    ____    _______    ___    ____    _________    ___    ____    _______    ___    ____    _______

          40              2             0.33534            0.00080142       55     relu    0.4039    95     relu    0.24461    83     relu    0.0067084    71     tanh    0.38505     9     relu    0.16509

hs_array:     55    95

ht_array length: 1lstm
    55    95

    "lstm"    "lstm"

    "relu"    "relu"

    0.4039    0.2446

Type: lstm Hidden size: 55 Activation: relu Dropout: 0.403901
Type: lstm Hidden size: 95 Activation: relu Dropout: 0.244607
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 55 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             40% dropout
     5   ''   LSTM                LSTM with 95 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             24% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 55 ME: 95.000000 MSE: 0.051320
SL: 4.279861e-03 HS: Saving to file: bayesopt/bayesopt_0.05132.mat
|   90 | Accept |     0.05132 |      16.107 |     0.03872 |    0.038733 |           40 |            2 |      0.33534 |   0.00080142 |           55 |         relu |       0.4039 |           95 |         relu |      0.24461 |           83 |         relu |    0.0067084 |           71 |         tanh |      0.38505 |            9 |         relu |      0.16509 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5  
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    ________

          69              4             0.2891             0.0018868         2     relu    0.31194    13     relu    0.043568    39     relu    0.31111    30     tanh    0.36429    75     relu    0.066515

hs_array:      2    13    39    30

ht_array length: 1lstm
     2    13    39    30

    "lstm"    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"    "tanh"

    0.3119    0.0436    0.3111    0.3643

Type: lstm Hidden size: 2 Activation: relu Dropout: 0.311944
Type: lstm Hidden size: 13 Activation: relu Dropout: 0.043568
Type: lstm Hidden size: 39 Activation: relu Dropout: 0.311114
Type: lstm Hidden size: 30 Activation: tanh Dropout: 0.364286
  15?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 2 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 13 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             4% dropout
     8   ''   LSTM                LSTM with 39 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             31% dropout
    11   ''   LSTM                LSTM with 30 hidden units
    12   ''   Tanh                Hyperbolic tangent
    13   ''   Dropout             36% dropout
    14   ''   Fully Connected     1 fully connected layer
    15   ''   Regression Output   mean-squared-error
SL: 1 HS: 2 ME: 13.000000 MSE: 39.000000
SL: 30 HS: 1.835967e-01 ME: 0.045932 MSE: Saving to file: bayesopt/bayesopt_0.1836.mat
|   91 | Accept |      0.1836 |      15.492 |     0.03872 |    0.038733 |           69 |            4 |       0.2891 |    0.0018868 |            2 |         relu |      0.31194 |           13 |         relu |     0.043568 |           39 |         relu |      0.31111 |           30 |         tanh |      0.36429 |           75 |         relu |     0.066515 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          50              2             3.5466             0.0067718         2     relu    0.43537    98     relu    0.2578    57     relu    0.43807    45     tanh    0.36283    55     relu    0.37985

hs_array:      2    98

ht_array length: 1lstm
     2    98

    "lstm"    "lstm"

    "relu"    "relu"

    0.4354    0.2578

Type: lstm Hidden size: 2 Activation: relu Dropout: 0.435366
Type: lstm Hidden size: 98 Activation: relu Dropout: 0.257797
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 2 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             26% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 2 ME: 98.000000 MSE: 0.064533
SL: 5.892640e-03 HS: Saving to file: bayesopt/bayesopt_0.064533.mat
|   92 | Accept |    0.064533 |      13.064 |     0.03872 |    0.038733 |           50 |            2 |       3.5466 |    0.0067718 |            2 |         relu |      0.43537 |           98 |         relu |       0.2578 |           57 |         relu |      0.43807 |           45 |         tanh |      0.36283 |           55 |         relu |      0.37985 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          55              2             6.3321             0.0081211        55     relu    0.38969    98     relu    0.44742    16     relu    0.23878    68     tanh    0.38878    47     relu    0.44369

hs_array:     55    98

ht_array length: 1lstm
    55    98

    "lstm"    "lstm"

    "relu"    "relu"

    0.3897    0.4474

Type: lstm Hidden size: 55 Activation: relu Dropout: 0.389691
Type: lstm Hidden size: 98 Activation: relu Dropout: 0.447420
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 55 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             39% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             45% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 55 ME: 98.000000 MSE: 0.050529
SL: 4.076391e-03 HS: Saving to file: bayesopt/bayesopt_0.050529.mat
|   93 | Accept |    0.050529 |      14.165 |     0.03872 |    0.038734 |           55 |            2 |       6.3321 |    0.0081211 |           55 |         relu |      0.38969 |           98 |         relu |      0.44742 |           16 |         relu |      0.23878 |           68 |         tanh |      0.38878 |           47 |         relu |      0.44369 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          55              2            0.051492             0.001097        50     relu    0.19241    91     relu    0.013363    22     relu    0.26097    38     tanh    0.11534    34     relu    0.23277

hs_array:     50    91

ht_array length: 1lstm
    50    91

    "lstm"    "lstm"

    "relu"    "relu"

    0.1924    0.0134

Type: lstm Hidden size: 50 Activation: relu Dropout: 0.192407
Type: lstm Hidden size: 91 Activation: relu Dropout: 0.013363
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 50 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             19% dropout
     5   ''   LSTM                LSTM with 91 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             1% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 50 ME: 91.000000 MSE: 0.052577
SL: 4.295966e-03 HS: Saving to file: bayesopt/bayesopt_0.052577.mat
|   94 | Accept |    0.052577 |      13.284 |     0.03872 |    0.038733 |           55 |            2 |     0.051492 |     0.001097 |           50 |         relu |      0.19241 |           91 |         relu |     0.013363 |           22 |         relu |      0.26097 |           38 |         tanh |      0.11534 |           34 |         relu |      0.23277 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1    drop1    hs2    act2    drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _____    ___    ____    ______    ___    ____    ______    ___    ____    _______    ___    ____    _______

          46              2             0.30481            0.0006707        59     relu    0.276    38     relu    0.2698     1     relu    0.1444    28     tanh    0.25442     6     relu    0.36507

hs_array:     59    38

ht_array length: 1lstm
    59    38

    "lstm"    "lstm"

    "relu"    "relu"

    0.2760    0.2698

Type: lstm Hidden size: 59 Activation: relu Dropout: 0.275995
Type: lstm Hidden size: 38 Activation: relu Dropout: 0.269800
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 59 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             28% dropout
     5   ''   LSTM                LSTM with 38 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             27% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 59 ME: 38.000000 MSE: 0.076076
SL: 9.173583e-03 HS: Saving to file: bayesopt/bayesopt_0.076076.mat
|   95 | Accept |    0.076076 |        10.9 |     0.03872 |    0.038733 |           46 |            2 |      0.30481 |    0.0006707 |           59 |         relu |        0.276 |           38 |         relu |       0.2698 |            1 |         relu |       0.1444 |           28 |         tanh |      0.25442 |            6 |         relu |      0.36507 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4    drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    ______    ___    ____    _______

          49              2             1.6238              0.020543        97     relu    0.12293    64     relu    0.25816     2     relu    0.27929     2     tanh    0.1069    57     relu    0.46533

hs_array:     97    64

ht_array length: 1lstm
    97    64

    "lstm"    "lstm"

    "relu"    "relu"

    0.1229    0.2582

Type: lstm Hidden size: 97 Activation: relu Dropout: 0.122933
Type: lstm Hidden size: 64 Activation: relu Dropout: 0.258163
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 97 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             12% dropout
     5   ''   LSTM                LSTM with 64 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             26% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 97 ME: 64.000000 MSE: 0.054508
SL: 4.295338e-03 HS: Saving to file: bayesopt/bayesopt_0.054508.mat
|   96 | Accept |    0.054508 |      16.045 |     0.03872 |    0.038733 |           49 |            2 |       1.6238 |     0.020543 |           97 |         relu |      0.12293 |           64 |         relu |      0.25816 |            2 |         relu |      0.27929 |            2 |         tanh |       0.1069 |           57 |         relu |      0.46533 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3    drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______

          51              3             9.1209             0.0064794        21     relu    0.31453    98     relu    0.44284     5     relu    0.4622    24     tanh    0.25553    46     relu    0.09292

hs_array:     21    98     5

ht_array length: 1lstm
    21    98     5

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.3145    0.4428    0.4622

Type: lstm Hidden size: 21 Activation: relu Dropout: 0.314530
Type: lstm Hidden size: 98 Activation: relu Dropout: 0.442835
Type: lstm Hidden size: 5 Activation: relu Dropout: 0.462204
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 21 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             31% dropout
     5   ''   LSTM                LSTM with 98 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             44% dropout
     8   ''   LSTM                LSTM with 5 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             46% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 21 ME: 98.000000 MSE: 5.000000
SL: 1.813799e-01 HS: 4.514771e-02 ME: Saving to file: bayesopt/bayesopt_0.18138.mat
|   97 | Accept |     0.18138 |      16.525 |     0.03872 |    0.040329 |           51 |            3 |       9.1209 |    0.0064794 |           21 |         relu |      0.31453 |           98 |         relu |      0.44284 |            5 |         relu |       0.4622 |           24 |         tanh |      0.25553 |           46 |         relu |      0.09292 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          51              3             2.7484              0.014248        79     relu    0.48839    31     relu    0.063038    71     relu    0.13328    41     tanh    0.11655    55     relu    0.30714

hs_array:     79    31    71

ht_array length: 1lstm
    79    31    71

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.4884    0.0630    0.1333

Type: lstm Hidden size: 79 Activation: relu Dropout: 0.488394
Type: lstm Hidden size: 31 Activation: relu Dropout: 0.063038
Type: lstm Hidden size: 71 Activation: relu Dropout: 0.133284
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 79 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             49% dropout
     5   ''   LSTM                LSTM with 31 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             6% dropout
     8   ''   LSTM                LSTM with 71 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             13% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 79 ME: 31.000000 MSE: 71.000000
SL: 7.258109e-02 HS: 7.642961e-03 ME: Saving to file: bayesopt/bayesopt_0.072581.mat
|   98 | Accept |    0.072581 |      19.257 |     0.03872 |    0.039256 |           51 |            3 |       2.7484 |     0.014248 |           79 |         relu |      0.48839 |           31 |         relu |     0.063038 |           71 |         relu |      0.13328 |           41 |         tanh |      0.11655 |           55 |         relu |      0.30714 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2    drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          54              2              4.52               0.25516         72     relu    0.43862    10     relu    0.1014    81     relu    0.19278     9     tanh    0.32746    34     relu    0.22027

hs_array:     72    10

ht_array length: 1lstm
    72    10

    "lstm"    "lstm"

    "relu"    "relu"

    0.4386    0.1014

Type: lstm Hidden size: 72 Activation: relu Dropout: 0.438619
Type: lstm Hidden size: 10 Activation: relu Dropout: 0.101401
  9?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 72 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             44% dropout
     5   ''   LSTM                LSTM with 10 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             10% dropout
     8   ''   Fully Connected     1 fully connected layer
     9   ''   Regression Output   mean-squared-error
SL: 1 HS: 72 ME: 10.000000 MSE: 0.184872
SL: 4.611062e-02 HS: Saving to file: bayesopt/bayesopt_0.18487.mat
|   99 | Accept |     0.18487 |      10.129 |     0.03872 |    0.039321 |           54 |            2 |         4.52 |      0.25516 |           72 |         relu |      0.43862 |           10 |         relu |       0.1014 |           81 |         relu |      0.19278 |            9 |         tanh |      0.32746 |           34 |         relu |      0.22027 |
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2      hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    ________    ___    ____    _______    ___    ____    _______    ___    ____    _______

          85              3              1.374             0.0071816        47     relu    0.40723    73     relu    0.013744    49     relu    0.14311    27     tanh    0.10397    76     relu    0.29241

hs_array:     47    73    49

ht_array length: 1lstm
    47    73    49

    "lstm"    "lstm"    "lstm"

    "relu"    "relu"    "relu"

    0.4072    0.0137    0.1431

Type: lstm Hidden size: 47 Activation: relu Dropout: 0.407231
Type: lstm Hidden size: 73 Activation: relu Dropout: 0.013744
Type: lstm Hidden size: 49 Activation: relu Dropout: 0.143107
  12?1 Layer array with layers:

     1   ''   Sequence Input      Sequence input with 1 dimensions
     2   ''   LSTM                LSTM with 47 hidden units
     3   ''   ReLU                ReLU
     4   ''   Dropout             41% dropout
     5   ''   LSTM                LSTM with 73 hidden units
     6   ''   ReLU                ReLU
     7   ''   Dropout             1% dropout
     8   ''   LSTM                LSTM with 49 hidden units
     9   ''   ReLU                ReLU
    10   ''   Dropout             14% dropout
    11   ''   Fully Connected     1 fully connected layer
    12   ''   Regression Output   mean-squared-error
SL: 1 HS: 47 ME: 73.000000 MSE: 49.000000
SL: 5.614643e-02 HS: 4.469283e-03 ME: Saving to file: bayesopt/bayesopt_0.056146.mat
|  100 | Accept |    0.056146 |      17.589 |     0.03872 |    0.038932 |           85 |            3 |        1.374 |    0.0071816 |           47 |         relu |      0.40723 |           73 |         relu |     0.013744 |           49 |         relu |      0.14311 |           27 |         tanh |      0.10397 |           76 |         relu |      0.29241 |

__________________________________________________________
Optimization completed.
MaxObjectiveEvaluations of 100 reached.
Total function evaluations: 100
Total elapsed time: 2225.171 seconds
Total objective function evaluation time: 1886.2128

Best observed feasible point:
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          45              2             3.8698             0.0051137        45     relu    0.19117    81     relu    0.12646    74     relu    0.24605    29     tanh    0.46845     3     relu    0.45826

Observed objective function value = 0.03872
Estimated objective function value = 0.038932
Function evaluation time = 14.5607

Best estimated feasible point (according to models):
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          45              2             3.8698             0.0051137        45     relu    0.19117    81     relu    0.12646    74     relu    0.24605    29     tanh    0.46845     3     relu    0.45826

Estimated objective function value = 0.038932
Estimated function evaluation time = 14.0414

Loading file: bayesopt/bayesopt_0.03872.mat
  SeriesNetwork with properties:

         Layers: [9?1 nnet.cnn.layer.Layer]
     InputNames: {'sequenceinput'}
    OutputNames: {'regressionoutput'}

  9?1 Layer array with layers:

     1   'sequenceinput'      Sequence Input      Sequence input with 1 dimensions
     2   'lstm_1'             LSTM                LSTM with 45 hidden units
     3   'relu_1'             ReLU                ReLU
     4   'dropout_1'          Dropout             19% dropout
     5   'lstm_2'             LSTM                LSTM with 81 hidden units
     6   'relu_2'             ReLU                ReLU
     7   'dropout_2'          Dropout             13% dropout
     8   'fc'                 Fully Connected     1 fully connected layer
     9   'regressionoutput'   Regression Output   mean-squared-error with response 'Response'
    sequenceLength    numLayers    gradientThreshold    initialLearnRate    hs1    act1     drop1     hs2    act2     drop2     hs3    act3     drop3     hs4    act4     drop4     hs5    act5     drop5 
    ______________    _________    _________________    ________________    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______    ___    ____    _______

          45              2             3.8698             0.0051137        45     relu    0.19117    81     relu    0.12646    74     relu    0.24605    29     tanh    0.46845     3     relu    0.45826

SL: 45 HS: 45 ME: 0.038720 MSE: 0.002267
Best NN configuration
Mean error: 0.038720
Max error:  0.110049
MSE:        0.002267
RMSE:       0.047611
>> 